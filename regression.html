<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapitre 6 La régression linéaire | Modèle linéaire général et modèle linéaire généralisé</title>
  <meta name="description" content="Chapitre 6 La régression linéaire | Modèle linéaire général et modèle linéaire généralisé" />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapitre 6 La régression linéaire | Modèle linéaire général et modèle linéaire généralisé" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapitre 6 La régression linéaire | Modèle linéaire général et modèle linéaire généralisé" />
  
  
  

<meta name="author" content="Cathy Maugis-Rabusseau (INSA Toulouse / IMT)" />


<meta name="date" content="2021-10-28" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="singulier.html"/>
<link rel="next" href="ANOVA.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<link href="libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet" />
<script src="libs/pagedtable-1.1/js/pagedtable.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">UF Elements de modélisation statistique</a></li>
<li>      <img src="image/LogoInsaToulouse.jpg" height="20px" align="right"/>      </li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Préface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#modélisation-dune-réponse-quantitative"><i class="fa fa-check"></i><b>1.1</b> Modélisation d’une réponse quantitative</a><ul>
<li class="chapter" data-level="1.1.1" data-path="intro.html"><a href="intro.html#jeu-de-données-illustratif"><i class="fa fa-check"></i><b>1.1.1</b> Jeu de données illustratif</a></li>
<li class="chapter" data-level="1.1.2" data-path="intro.html"><a href="intro.html#régression-linéaire"><i class="fa fa-check"></i><b>1.1.2</b> Régression linéaire</a></li>
<li class="chapter" data-level="1.1.3" data-path="intro.html"><a href="intro.html#analyse-de-la-variance-anova"><i class="fa fa-check"></i><b>1.1.3</b> Analyse de la variance (ANOVA)</a></li>
<li class="chapter" data-level="1.1.4" data-path="intro.html"><a href="intro.html#analyse-de-covariance-ancova"><i class="fa fa-check"></i><b>1.1.4</b> Analyse de covariance (ANCOVA)</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#modélisation-dune-variable-binaire-de-comptage"><i class="fa fa-check"></i><b>1.2</b> Modélisation d’une variable binaire, de comptage, …</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#objectifs-du-cours"><i class="fa fa-check"></i><b>1.3</b> Objectifs du cours</a></li>
</ul></li>
<li class="part"><span><b>I Le modèle linéaire général</b></span></li>
<li class="chapter" data-level="2" data-path="DefML.html"><a href="DefML.html"><i class="fa fa-check"></i><b>2</b> Définitions générales</a><ul>
<li class="chapter" data-level="2.1" data-path="DefML.html"><a href="DefML.html#modlinreg"><i class="fa fa-check"></i><b>2.1</b> Modèle linéaire régulier</a></li>
<li class="chapter" data-level="2.2" data-path="DefML.html"><a href="DefML.html#exemples-de-modèle-linéaire-gaussien"><i class="fa fa-check"></i><b>2.2</b> Exemples de modèle linéaire gaussien</a><ul>
<li class="chapter" data-level="2.2.1" data-path="DefML.html"><a href="DefML.html#le-modèle-de-régression-linéaire"><i class="fa fa-check"></i><b>2.2.1</b> Le modèle de régression linéaire</a></li>
<li class="chapter" data-level="2.2.2" data-path="DefML.html"><a href="DefML.html#le-modèle-danalyse-de-la-variance"><i class="fa fa-check"></i><b>2.2.2</b> Le modèle d’analyse de la variance</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="DefML.html"><a href="DefML.html#en-résumé"><i class="fa fa-check"></i><b>2.3</b> En résumé</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="EstML.html"><a href="EstML.html"><i class="fa fa-check"></i><b>3</b> Estimation des paramètres</a><ul>
<li class="chapter" data-level="3.1" data-path="EstML.html"><a href="EstML.html#estimation-de-theta"><i class="fa fa-check"></i><b>3.1</b> Estimation de <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="3.2" data-path="EstML.html"><a href="EstML.html#valeurs-ajustées-et-résidus"><i class="fa fa-check"></i><b>3.2</b> Valeurs ajustées et résidus</a></li>
<li class="chapter" data-level="3.3" data-path="EstML.html"><a href="EstML.html#estimation-de-sigma2"><i class="fa fa-check"></i><b>3.3</b> Estimation de <span class="math inline">\(\sigma^2\)</span></a></li>
<li class="chapter" data-level="3.4" data-path="EstML.html"><a href="EstML.html#erreurs-standards"><i class="fa fa-check"></i><b>3.4</b> Erreurs standards</a></li>
<li class="chapter" data-level="3.5" data-path="EstML.html"><a href="EstML.html#intervalle-de-confiance-de-theta_j-de-xtheta_i-et-de-x_0theta"><i class="fa fa-check"></i><b>3.5</b> Intervalle de confiance de <span class="math inline">\(\theta_j\)</span>, de <span class="math inline">\((X\theta)_i\)</span> et de <span class="math inline">\(X_0\theta\)</span></a><ul>
<li class="chapter" data-level="3.5.1" data-path="EstML.html"><a href="EstML.html#ICthetaj"><i class="fa fa-check"></i><b>3.5.1</b> Intervalle de confiance de <span class="math inline">\(\theta_j\)</span></a></li>
<li class="chapter" data-level="3.5.2" data-path="EstML.html"><a href="EstML.html#ICXthetai"><i class="fa fa-check"></i><b>3.5.2</b> Intervalle de confiance de <span class="math inline">\((X\theta)_i\)</span></a></li>
<li class="chapter" data-level="3.5.3" data-path="EstML.html"><a href="EstML.html#ICX0theta"><i class="fa fa-check"></i><b>3.5.3</b> Intervalle de confiance de <span class="math inline">\(X_0\theta\)</span></a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="EstML.html"><a href="EstML.html#ICpredit"><i class="fa fa-check"></i><b>3.6</b> Intervalles de prédiction</a></li>
<li class="chapter" data-level="3.7" data-path="EstML.html"><a href="EstML.html#qualité-dajustement"><i class="fa fa-check"></i><b>3.7</b> Qualité d’ajustement</a></li>
<li class="chapter" data-level="3.8" data-path="EstML.html"><a href="EstML.html#en-résumé-1"><i class="fa fa-check"></i><b>3.8</b> En résumé</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="Test.html"><a href="Test.html"><i class="fa fa-check"></i><b>4</b> Test de Fisher-Snedecor</a><ul>
<li class="chapter" data-level="4.1" data-path="Test.html"><a href="Test.html#hypothèses-testées"><i class="fa fa-check"></i><b>4.1</b> Hypothèses testées</a><ul>
<li class="chapter" data-level="4.1.1" data-path="Test.html"><a href="Test.html#première-écriture"><i class="fa fa-check"></i><b>4.1.1</b> Première écriture</a></li>
<li class="chapter" data-level="4.1.2" data-path="Test.html"><a href="Test.html#seconde-écriture"><i class="fa fa-check"></i><b>4.1.2</b> Seconde écriture</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="Test.html"><a href="Test.html#le-test-de-fisher-snedecor"><i class="fa fa-check"></i><b>4.2</b> Le test de Fisher-Snedecor</a><ul>
<li class="chapter" data-level="4.2.1" data-path="Test.html"><a href="Test.html#principe"><i class="fa fa-check"></i><b>4.2.1</b> Principe</a></li>
<li class="chapter" data-level="4.2.2" data-path="Test.html"><a href="Test.html#comblinconjointes"><i class="fa fa-check"></i><b>4.2.2</b> La statistique de test</a></li>
<li class="chapter" data-level="4.2.3" data-path="Test.html"><a href="Test.html#règle-de-décision"><i class="fa fa-check"></i><b>4.2.3</b> Règle de décision</a></li>
<li class="chapter" data-level="4.2.4" data-path="Test.html"><a href="Test.html#comblin"><i class="fa fa-check"></i><b>4.2.4</b> Cas particulier où <span class="math inline">\(q=1\)</span> : Test de Student</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="Test.html"><a href="Test.html#intervalle-région-de-confiance-pour-ctheta"><i class="fa fa-check"></i><b>4.3</b> Intervalle (région) de confiance pour <span class="math inline">\(C\theta\)</span></a><ul>
<li class="chapter" data-level="4.3.1" data-path="Test.html"><a href="Test.html#ic-pour-ctheta-in-mathbbr"><i class="fa fa-check"></i><b>4.3.1</b> IC pour <span class="math inline">\(C\theta \in \mathbb{R}\)</span></a></li>
<li class="chapter" data-level="4.3.2" data-path="Test.html"><a href="Test.html#région-de-confiance-pour-ctheta-in-mathbbrq"><i class="fa fa-check"></i><b>4.3.2</b> Région de confiance pour <span class="math inline">\(C\theta \in \mathbb{R}^q\)</span></a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="Test.html"><a href="Test.html#en-résumé-2"><i class="fa fa-check"></i><b>4.4</b> En résumé</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="singulier.html"><a href="singulier.html"><i class="fa fa-check"></i><b>5</b> Modèles singuliers, orthogonalité et importance des hypothèses sur les erreurs</a><ul>
<li class="chapter" data-level="5.1" data-path="singulier.html"><a href="singulier.html#quand-h1-h4-ne-sont-pas-respectées"><i class="fa fa-check"></i><b>5.1</b> Quand H1-H4 ne sont pas respectées…</a><ul>
<li class="chapter" data-level="5.1.1" data-path="singulier.html"><a href="singulier.html#propriétés-de-lestimateur-des-moindres-carrés-widehattheta"><i class="fa fa-check"></i><b>5.1.1</b> Propriétés de l’estimateur des moindres carrés <span class="math inline">\(\widehat{\theta}\)</span></a></li>
<li class="chapter" data-level="5.1.2" data-path="singulier.html"><a href="singulier.html#propriétés-de-lestimateur-des-moindres-carrés-widehatsigma2"><i class="fa fa-check"></i><b>5.1.2</b> Propriétés de l’estimateur des moindres carrés <span class="math inline">\(\widehat{\sigma}^2\)</span></a></li>
<li class="chapter" data-level="5.1.3" data-path="singulier.html"><a href="singulier.html#modèles-avec-corrélations"><i class="fa fa-check"></i><b>5.1.3</b> Modèles avec corrélations</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="singulier.html"><a href="singulier.html#ModSingulier"><i class="fa fa-check"></i><b>5.2</b> Modèles singuliers</a><ul>
<li class="chapter" data-level="5.2.1" data-path="singulier.html"><a href="singulier.html#contraintes-didentifiabilité"><i class="fa fa-check"></i><b>5.2.1</b> Contraintes d’identifiabilité</a></li>
<li class="chapter" data-level="5.2.2" data-path="singulier.html"><a href="singulier.html#fonctions-estimables-et-contrastes"><i class="fa fa-check"></i><b>5.2.2</b> Fonctions estimables et contrastes</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="singulier.html"><a href="singulier.html#orthogonalité"><i class="fa fa-check"></i><b>5.3</b> Orthogonalité</a><ul>
<li class="chapter" data-level="5.3.1" data-path="singulier.html"><a href="singulier.html#orthogonalité-pour-les-modèles-réguliers"><i class="fa fa-check"></i><b>5.3.1</b> Orthogonalité pour les modèles réguliers</a></li>
<li class="chapter" data-level="5.3.2" data-path="singulier.html"><a href="singulier.html#orthogonalité-pour-les-modèles-non-réguliers"><i class="fa fa-check"></i><b>5.3.2</b> Orthogonalité pour les modèles non-réguliers</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="singulier.html"><a href="singulier.html#en-résumé-3"><i class="fa fa-check"></i><b>5.4</b> En résumé</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>6</b> La régression linéaire</a><ul>
<li class="chapter" data-level="6.1" data-path="regression.html"><a href="regression.html#introduction"><i class="fa fa-check"></i><b>6.1</b> Introduction</a><ul>
<li class="chapter" data-level="6.1.1" data-path="regression.html"><a href="regression.html#exemple-illustratif"><i class="fa fa-check"></i><b>6.1.1</b> Exemple illustratif</a></li>
<li class="chapter" data-level="6.1.2" data-path="regression.html"><a href="regression.html#problématique"><i class="fa fa-check"></i><b>6.1.2</b> Problématique</a></li>
<li class="chapter" data-level="6.1.3" data-path="regression.html"><a href="regression.html#le-modèle-de-régression-linéaire-simple"><i class="fa fa-check"></i><b>6.1.3</b> Le modèle de régression linéaire simple</a></li>
<li class="chapter" data-level="6.1.4" data-path="regression.html"><a href="regression.html#le-modèle-de-régression-linéaire-multiple"><i class="fa fa-check"></i><b>6.1.4</b> Le modèle de régression linéaire multiple</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="regression.html"><a href="regression.html#estimation"><i class="fa fa-check"></i><b>6.2</b> Estimation</a><ul>
<li class="chapter" data-level="6.2.1" data-path="regression.html"><a href="regression.html#résultats-généraux"><i class="fa fa-check"></i><b>6.2.1</b> Résultats généraux</a></li>
<li class="chapter" data-level="6.2.2" data-path="regression.html"><a href="regression.html#propriétés-en-régression-linéaire-simple"><i class="fa fa-check"></i><b>6.2.2</b> Propriétés en régression linéaire simple</a></li>
<li class="chapter" data-level="6.2.3" data-path="regression.html"><a href="regression.html#le-coefficient-r2"><i class="fa fa-check"></i><b>6.2.3</b> Le coefficient <span class="math inline">\(R^2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="regression.html"><a href="regression.html#tests-et-intervalles-de-confiance"><i class="fa fa-check"></i><b>6.3</b> Tests et intervalles de confiance</a><ul>
<li class="chapter" data-level="6.3.1" data-path="regression.html"><a href="regression.html#test-de-nullité-dun-paramètre-du-modèle"><i class="fa fa-check"></i><b>6.3.1</b> Test de nullité d’un paramètre du modèle</a></li>
<li class="chapter" data-level="6.3.2" data-path="regression.html"><a href="regression.html#test-de-nullité-de-quelques-paramètres-du-modèle"><i class="fa fa-check"></i><b>6.3.2</b> Test de nullité de quelques paramètres du modèle</a></li>
<li class="chapter" data-level="6.3.3" data-path="regression.html"><a href="regression.html#test-de-nullité-de-tous-les-paramètres-du-modèle"><i class="fa fa-check"></i><b>6.3.3</b> Test de nullité de tous les paramètres du modèle</a></li>
<li class="chapter" data-level="6.3.4" data-path="regression.html"><a href="regression.html#intervalle-de-confiance-de-theta_j-de-xtheta_i-et-de-x_0theta-1"><i class="fa fa-check"></i><b>6.3.4</b> Intervalle de confiance de <span class="math inline">\(\theta_j\)</span>, de <span class="math inline">\((X\theta)_i\)</span> et de <span class="math inline">\(X_0\theta\)</span></a></li>
<li class="chapter" data-level="6.3.5" data-path="regression.html"><a href="regression.html#intervalle-de-prédiction"><i class="fa fa-check"></i><b>6.3.5</b> Intervalle de prédiction</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="regression.html"><a href="regression.html#sélection-des-variables-explicatives"><i class="fa fa-check"></i><b>6.4</b> Sélection des variables explicatives</a><ul>
<li class="chapter" data-level="6.4.1" data-path="regression.html"><a href="regression.html#cadre-général-de-sélection-de-modèles"><i class="fa fa-check"></i><b>6.4.1</b> Cadre général de sélection de modèles</a></li>
<li class="chapter" data-level="6.4.2" data-path="regression.html"><a href="regression.html#quelques-critères-pour-sélectionner-un-modèle"><i class="fa fa-check"></i><b>6.4.2</b> Quelques critères pour sélectionner un modèle</a></li>
<li class="chapter" data-level="6.4.3" data-path="regression.html"><a href="regression.html#algorithmes-de-sélection-de-variables"><i class="fa fa-check"></i><b>6.4.3</b> Algorithmes de sélection de variables</a></li>
<li class="chapter" data-level="6.4.4" data-path="regression.html"><a href="regression.html#illustration-sur-lexemple"><i class="fa fa-check"></i><b>6.4.4</b> Illustration sur l’exemple</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="regression.html"><a href="regression.html#régression-linéaire-régularisée"><i class="fa fa-check"></i><b>6.5</b> Régression linéaire régularisée</a><ul>
<li class="chapter" data-level="6.5.1" data-path="regression.html"><a href="regression.html#régression-ridge"><i class="fa fa-check"></i><b>6.5.1</b> Régression ridge</a></li>
<li class="chapter" data-level="6.5.2" data-path="regression.html"><a href="regression.html#régression-lasso"><i class="fa fa-check"></i><b>6.5.2</b> Régression Lasso</a></li>
<li class="chapter" data-level="6.5.3" data-path="regression.html"><a href="regression.html#régression-elastic-net"><i class="fa fa-check"></i><b>6.5.3</b> Régression Elastic-Net</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="regression.html"><a href="regression.html#ValidationMod"><i class="fa fa-check"></i><b>6.6</b> Validation du modèle</a><ul>
<li class="chapter" data-level="6.6.1" data-path="regression.html"><a href="regression.html#contrôle-graphique-a-posteriori"><i class="fa fa-check"></i><b>6.6.1</b> Contrôle graphique a posteriori</a></li>
<li class="chapter" data-level="6.6.2" data-path="regression.html"><a href="regression.html#pour-vérifier-les-hypothèses-h1-et-h2-adéquation-et-homoscédasticité"><i class="fa fa-check"></i><b>6.6.2</b> Pour vérifier les hypothèses H1 et H2 : adéquation et homoscédasticité</a></li>
<li class="chapter" data-level="6.6.3" data-path="regression.html"><a href="regression.html#pour-vérifier-lhypothèse-h3-indépendance"><i class="fa fa-check"></i><b>6.6.3</b> Pour vérifier l’hypothèse H3 : indépendance</a></li>
<li class="chapter" data-level="6.6.4" data-path="regression.html"><a href="regression.html#pour-vérifier-lhypothèse-h4-gaussianité"><i class="fa fa-check"></i><b>6.6.4</b> Pour vérifier l’hypothèse H4 : gaussianité</a></li>
<li class="chapter" data-level="6.6.5" data-path="regression.html"><a href="regression.html#détection-de-données-aberrantes"><i class="fa fa-check"></i><b>6.6.5</b> Détection de données aberrantes</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="regression.html"><a href="regression.html#en-résumé-4"><i class="fa fa-check"></i><b>6.7</b> En résumé</a></li>
<li class="chapter" data-level="6.8" data-path="regression.html"><a href="regression.html#quelques-codes-python"><i class="fa fa-check"></i><b>6.8</b> Quelques codes python</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ANOVA.html"><a href="ANOVA.html"><i class="fa fa-check"></i><b>7</b> Analyse de variance (ANOVA)</a><ul>
<li class="chapter" data-level="7.1" data-path="ANOVA.html"><a href="ANOVA.html#vocabulaire"><i class="fa fa-check"></i><b>7.1</b> Vocabulaire</a></li>
<li class="chapter" data-level="7.2" data-path="ANOVA.html"><a href="ANOVA.html#analyse-de-variance-à-un-facteur"><i class="fa fa-check"></i><b>7.2</b> Analyse de variance à un facteur</a><ul>
<li class="chapter" data-level="7.2.1" data-path="ANOVA.html"><a href="ANOVA.html#exemple-et-notations"><i class="fa fa-check"></i><b>7.2.1</b> Exemple et notations</a></li>
<li class="chapter" data-level="7.2.2" data-path="ANOVA.html"><a href="ANOVA.html#modèle-régulier"><i class="fa fa-check"></i><b>7.2.2</b> Modèle régulier</a></li>
<li class="chapter" data-level="7.2.3" data-path="ANOVA.html"><a href="ANOVA.html#modèle-singulier"><i class="fa fa-check"></i><b>7.2.3</b> Modèle singulier</a></li>
<li class="chapter" data-level="7.2.4" data-path="ANOVA.html"><a href="ANOVA.html#prédictions-résidus-et-variance"><i class="fa fa-check"></i><b>7.2.4</b> Prédictions, résidus et variance</a></li>
<li class="chapter" data-level="7.2.5" data-path="ANOVA.html"><a href="ANOVA.html#intervalle-de-confiance-et-test-sur-leffet-facteur"><i class="fa fa-check"></i><b>7.2.5</b> Intervalle de confiance et test sur l’effet facteur</a></li>
<li class="chapter" data-level="7.2.6" data-path="ANOVA.html"><a href="ANOVA.html#test-deffet-du-facteur"><i class="fa fa-check"></i><b>7.2.6</b> Test d’effet du facteur</a></li>
<li class="chapter" data-level="7.2.7" data-path="ANOVA.html"><a href="ANOVA.html#tableau-danalyse-de-la-variance-à-un-facteur"><i class="fa fa-check"></i><b>7.2.7</b> Tableau d’analyse de la variance à un facteur</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="ANOVA.html"><a href="ANOVA.html#analyse-de-variance-à-deux-facteurs"><i class="fa fa-check"></i><b>7.3</b> Analyse de variance à deux facteurs</a><ul>
<li class="chapter" data-level="7.3.1" data-path="ANOVA.html"><a href="ANOVA.html#notations-et-exemple"><i class="fa fa-check"></i><b>7.3.1</b> Notations et exemple</a></li>
<li class="chapter" data-level="7.3.2" data-path="ANOVA.html"><a href="ANOVA.html#modélisation"><i class="fa fa-check"></i><b>7.3.2</b> Modélisation</a></li>
<li class="chapter" data-level="7.3.3" data-path="ANOVA.html"><a href="ANOVA.html#estimation-des-paramètres"><i class="fa fa-check"></i><b>7.3.3</b> Estimation des paramètres</a></li>
<li class="chapter" data-level="7.3.4" data-path="ANOVA.html"><a href="ANOVA.html#prédiction-résidus-et-variance"><i class="fa fa-check"></i><b>7.3.4</b> Prédiction, résidus et variance</a></li>
<li class="chapter" data-level="7.3.5" data-path="ANOVA.html"><a href="ANOVA.html#décomposition-de-la-variabilité"><i class="fa fa-check"></i><b>7.3.5</b> Décomposition de la variabilité</a></li>
<li class="chapter" data-level="7.3.6" data-path="ANOVA.html"><a href="ANOVA.html#le-diagramme-dinteractions"><i class="fa fa-check"></i><b>7.3.6</b> Le diagramme d’interactions</a></li>
<li class="chapter" data-level="7.3.7" data-path="ANOVA.html"><a href="ANOVA.html#tests-dhypothèses"><i class="fa fa-check"></i><b>7.3.7</b> Tests d’hypothèses</a></li>
<li class="chapter" data-level="7.3.8" data-path="ANOVA.html"><a href="ANOVA.html#test-dabsence-deffet-du-facteur-b"><i class="fa fa-check"></i><b>7.3.8</b> Test d’absence d’effet du facteur <span class="math inline">\(B\)</span></a></li>
<li class="chapter" data-level="7.3.9" data-path="ANOVA.html"><a href="ANOVA.html#tableau-danalyse-de-variance-à-deux-facteurs-croisés-dans-le-cas-dun-plan-orthogonal"><i class="fa fa-check"></i><b>7.3.9</b> Tableau d’analyse de variance à deux facteurs croisés dans le cas d’un plan orthogonal</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="ANOVA.html"><a href="ANOVA.html#en-résumé-5"><i class="fa fa-check"></i><b>7.4</b> En résumé</a></li>
<li class="chapter" data-level="7.5" data-path="ANOVA.html"><a href="ANOVA.html#quelques-codes-en-python"><i class="fa fa-check"></i><b>7.5</b> Quelques codes en python</a><ul>
<li class="chapter" data-level="7.5.1" data-path="ANOVA.html"><a href="ANOVA.html#exemple-danova-à-un-facteur"><i class="fa fa-check"></i><b>7.5.1</b> Exemple d’ANOVA à un facteur</a></li>
<li class="chapter" data-level="7.5.2" data-path="ANOVA.html"><a href="ANOVA.html#exemple-danova-à-deux-facteurs"><i class="fa fa-check"></i><b>7.5.2</b> Exemple d’ANOVA à deux facteurs</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ANCOVA.html"><a href="ANCOVA.html"><i class="fa fa-check"></i><b>8</b> Analyse de covariance (ANCOVA)</a><ul>
<li class="chapter" data-level="8.1" data-path="ANCOVA.html"><a href="ANCOVA.html#les-données"><i class="fa fa-check"></i><b>8.1</b> Les données</a></li>
<li class="chapter" data-level="8.2" data-path="ANCOVA.html"><a href="ANCOVA.html#modélisation-1"><i class="fa fa-check"></i><b>8.2</b> Modélisation</a><ul>
<li class="chapter" data-level="8.2.1" data-path="ANCOVA.html"><a href="ANCOVA.html#modélisation-régulière"><i class="fa fa-check"></i><b>8.2.1</b> Modélisation régulière</a></li>
<li class="chapter" data-level="8.2.2" data-path="ANCOVA.html"><a href="ANCOVA.html#modélisation-singulière"><i class="fa fa-check"></i><b>8.2.2</b> Modélisation singulière</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="ANCOVA.html"><a href="ANCOVA.html#estimation-des-paramètres-1"><i class="fa fa-check"></i><b>8.3</b> Estimation des paramètres</a></li>
<li class="chapter" data-level="8.4" data-path="ANCOVA.html"><a href="ANCOVA.html#tests-dhypothèses-1"><i class="fa fa-check"></i><b>8.4</b> Tests d’hypothèses</a><ul>
<li class="chapter" data-level="8.4.1" data-path="ANCOVA.html"><a href="ANCOVA.html#absence-de-tout-effet"><i class="fa fa-check"></i><b>8.4.1</b> Absence de tout effet</a></li>
<li class="chapter" data-level="8.4.2" data-path="ANCOVA.html"><a href="ANCOVA.html#test-dabsence-dinteraction"><i class="fa fa-check"></i><b>8.4.2</b> Test d’absence d’interaction</a></li>
<li class="chapter" data-level="8.4.3" data-path="ANCOVA.html"><a href="ANCOVA.html#test-dabsence-de-leffet-de-la-covariable-z"><i class="fa fa-check"></i><b>8.4.3</b> Test d’absence de l’effet de la covariable z</a></li>
<li class="chapter" data-level="8.4.4" data-path="ANCOVA.html"><a href="ANCOVA.html#test-dabsence-de-leffet-facteur-t"><i class="fa fa-check"></i><b>8.4.4</b> Test d’absence de l’effet facteur T</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="ANCOVA.html"><a href="ANCOVA.html#en-résumé-6"><i class="fa fa-check"></i><b>8.5</b> En résumé</a></li>
<li class="chapter" data-level="8.6" data-path="ANCOVA.html"><a href="ANCOVA.html#quelques-codes-en-python-1"><i class="fa fa-check"></i><b>8.6</b> Quelques codes en python</a></li>
</ul></li>
<li class="part"><span><b>II Le modèle linéaire généralisé</b></span></li>
<li class="chapter" data-level="9" data-path="GLM.html"><a href="GLM.html"><i class="fa fa-check"></i><b>9</b> Principe du modèle linéaire généralisé</a><ul>
<li class="chapter" data-level="9.1" data-path="GLM.html"><a href="GLM.html#introduction-1"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="GLM.html"><a href="GLM.html#caractérisation-dun-modèle-linéaire-généralisé"><i class="fa fa-check"></i><b>9.2</b> Caractérisation d’un modèle linéaire généralisé</a><ul>
<li class="chapter" data-level="9.2.1" data-path="GLM.html"><a href="GLM.html#loi-de-la-variable-réponse-y"><i class="fa fa-check"></i><b>9.2.1</b> Loi de la variable réponse <span class="math inline">\(Y\)</span></a></li>
<li class="chapter" data-level="9.2.2" data-path="GLM.html"><a href="GLM.html#prédicteur-linéaire"><i class="fa fa-check"></i><b>9.2.2</b> Prédicteur linéaire</a></li>
<li class="chapter" data-level="9.2.3" data-path="GLM.html"><a href="GLM.html#fonction-de-lien"><i class="fa fa-check"></i><b>9.2.3</b> Fonction de lien</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="GLM.html"><a href="GLM.html#EstimMLG"><i class="fa fa-check"></i><b>9.3</b> Estimation</a><ul>
<li class="chapter" data-level="9.3.1" data-path="GLM.html"><a href="GLM.html#estimation-par-maximum-de-vraisemblance"><i class="fa fa-check"></i><b>9.3.1</b> Estimation par maximum de vraisemblance</a></li>
<li class="chapter" data-level="9.3.2" data-path="GLM.html"><a href="GLM.html#algorithmes-de-newton-raphson-et-fisher-scoring"><i class="fa fa-check"></i><b>9.3.2</b> Algorithmes de Newton-Raphson et Fisher-scoring</a></li>
<li class="chapter" data-level="9.3.3" data-path="GLM.html"><a href="GLM.html#equations-de-vraisemblance"><i class="fa fa-check"></i><b>9.3.3</b> Equations de vraisemblance</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="GLM.html"><a href="GLM.html#NormalitéAsymptotique"><i class="fa fa-check"></i><b>9.4</b> Loi asymptotique de l’EMV et inférence</a></li>
<li class="chapter" data-level="9.5" data-path="GLM.html"><a href="GLM.html#tests-dhypothèses-2"><i class="fa fa-check"></i><b>9.5</b> Tests d’hypothèses</a><ul>
<li class="chapter" data-level="9.5.1" data-path="GLM.html"><a href="GLM.html#test-de-modèles-emboîtés"><i class="fa fa-check"></i><b>9.5.1</b> Test de modèles emboîtés</a></li>
<li class="chapter" data-level="9.5.2" data-path="GLM.html"><a href="GLM.html#TestParamMLG"><i class="fa fa-check"></i><b>9.5.2</b> Test d’un paramètre <span class="math inline">\(\theta_j\)</span></a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="GLM.html"><a href="GLM.html#MLGIC"><i class="fa fa-check"></i><b>9.6</b> Intervalle de confiance pour <span class="math inline">\(\theta_j\)</span></a><ul>
<li class="chapter" data-level="9.6.1" data-path="GLM.html"><a href="GLM.html#par-wald"><i class="fa fa-check"></i><b>9.6.1</b> Par Wald</a></li>
<li class="chapter" data-level="9.6.2" data-path="GLM.html"><a href="GLM.html#fondé-sur-le-rapport-de-vraisemblances"><i class="fa fa-check"></i><b>9.6.2</b> Fondé sur le rapport de vraisemblances</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="GLM.html"><a href="GLM.html#qualité-dajustement-1"><i class="fa fa-check"></i><b>9.7</b> Qualité d’ajustement</a><ul>
<li class="chapter" data-level="9.7.1" data-path="GLM.html"><a href="GLM.html#le-pseudo-r2"><i class="fa fa-check"></i><b>9.7.1</b> Le pseudo <span class="math inline">\(R^2\)</span></a></li>
<li class="chapter" data-level="9.7.2" data-path="GLM.html"><a href="GLM.html#le-chi2-de-pearson-généralisé"><i class="fa fa-check"></i><b>9.7.2</b> Le <span class="math inline">\(\chi^2\)</span> de Pearson généralisé</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="GLM.html"><a href="GLM.html#ResidusGLM"><i class="fa fa-check"></i><b>9.8</b> Diagnostic, résidus</a></li>
<li class="chapter" data-level="9.9" data-path="GLM.html"><a href="GLM.html#en-résumé-7"><i class="fa fa-check"></i><b>9.9</b> En résumé</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="RegLogistique.html"><a href="RegLogistique.html"><i class="fa fa-check"></i><b>10</b> Régression logistique</a><ul>
<li class="chapter" data-level="10.1" data-path="RegLogistique.html"><a href="RegLogistique.html#introduction-2"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="RegLogistique.html"><a href="RegLogistique.html#pourquoi-des-modèles-particuliers"><i class="fa fa-check"></i><b>10.2</b> Pourquoi des modèles particuliers ?</a></li>
<li class="chapter" data-level="10.3" data-path="RegLogistique.html"><a href="RegLogistique.html#odds-et-odds-ratio"><i class="fa fa-check"></i><b>10.3</b> Odds et odds ratio</a></li>
<li class="chapter" data-level="10.4" data-path="RegLogistique.html"><a href="RegLogistique.html#régression-logistique-simple"><i class="fa fa-check"></i><b>10.4</b> Régression logistique simple</a><ul>
<li class="chapter" data-level="10.4.1" data-path="RegLogistique.html"><a href="RegLogistique.html#subquanti"><i class="fa fa-check"></i><b>10.4.1</b> Avec une variable explicative quantitative</a></li>
<li class="chapter" data-level="10.4.2" data-path="RegLogistique.html"><a href="RegLogistique.html#sect1expquali"><i class="fa fa-check"></i><b>10.4.2</b> Avec une variable explicative qualitative</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="RegLogistique.html"><a href="RegLogistique.html#régression-logistique-multiple"><i class="fa fa-check"></i><b>10.5</b> Régression logistique multiple</a><ul>
<li class="chapter" data-level="10.5.1" data-path="RegLogistique.html"><a href="RegLogistique.html#modèle-sans-interaction"><i class="fa fa-check"></i><b>10.5.1</b> Modèle sans interaction</a></li>
<li class="chapter" data-level="10.5.2" data-path="RegLogistique.html"><a href="RegLogistique.html#modèle-avec-interactions"><i class="fa fa-check"></i><b>10.5.2</b> Modèle avec interactions</a></li>
<li class="chapter" data-level="10.5.3" data-path="RegLogistique.html"><a href="RegLogistique.html#etude-complémentaire-du-modèle-retenu"><i class="fa fa-check"></i><b>10.5.3</b> Etude complémentaire du modèle retenu</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="RegLogistique.html"><a href="RegLogistique.html#quelques-codes-avec-python"><i class="fa fa-check"></i><b>10.6</b> Quelques codes avec python</a></li>
<li class="chapter" data-level="10.7" data-path="RegLogistique.html"><a href="RegLogistique.html#régression-polytomique"><i class="fa fa-check"></i><b>10.7</b> Régression polytomique</a><ul>
<li class="chapter" data-level="10.7.1" data-path="RegLogistique.html"><a href="RegLogistique.html#régression-multinomiale-ou-polytomique-non-ordonnée"><i class="fa fa-check"></i><b>10.7.1</b> Régression multinomiale ou polytomique non-ordonnée</a></li>
<li class="chapter" data-level="10.7.2" data-path="RegLogistique.html"><a href="RegLogistique.html#régression-polytomique-ordonnée"><i class="fa fa-check"></i><b>10.7.2</b> Régression polytomique ordonnée</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="RegLogLin.html"><a href="RegLogLin.html"><i class="fa fa-check"></i><b>11</b> Régression de Poisson / régression loglinéaire</a><ul>
<li class="chapter" data-level="11.1" data-path="RegLogLin.html"><a href="RegLogLin.html#modèle-de-régression-loglinéaire"><i class="fa fa-check"></i><b>11.1</b> Modèle de régression loglinéaire</a><ul>
<li class="chapter" data-level="11.1.1" data-path="RegLogLin.html"><a href="RegLogLin.html#pourquoi-un-modèle-particulier"><i class="fa fa-check"></i><b>11.1.1</b> Pourquoi un modèle particulier ?</a></li>
<li class="chapter" data-level="11.1.2" data-path="RegLogLin.html"><a href="RegLogLin.html#estimation-des-paramètres-3"><i class="fa fa-check"></i><b>11.1.2</b> Estimation des paramètres</a></li>
<li class="chapter" data-level="11.1.3" data-path="RegLogLin.html"><a href="RegLogLin.html#ajustement-et-prédiction"><i class="fa fa-check"></i><b>11.1.3</b> Ajustement et prédiction</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="RegLogLin.html"><a href="RegLogLin.html#exemple-de-régression-loglinéaire-avec-r"><i class="fa fa-check"></i><b>11.2</b> Exemple de régression loglinéaire avec R</a><ul>
<li class="chapter" data-level="11.2.1" data-path="RegLogLin.html"><a href="RegLogLin.html#régression-loglinéaire-simple"><i class="fa fa-check"></i><b>11.2.1</b> Régression loglinéaire simple</a></li>
<li class="chapter" data-level="11.2.2" data-path="RegLogLin.html"><a href="RegLogLin.html#régression-loglinéaire-multiple"><i class="fa fa-check"></i><b>11.2.2</b> Régression loglinéaire multiple</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="RegLogLin.html"><a href="RegLogLin.html#sur-dispersion-et-modèle-binomial-négatif"><i class="fa fa-check"></i><b>11.3</b> Sur-dispersion et modèle binomial négatif</a></li>
<li class="chapter" data-level="11.4" data-path="RegLogLin.html"><a href="RegLogLin.html#quelques-codes-avec-python-1"><i class="fa fa-check"></i><b>11.4</b> Quelques codes avec python</a></li>
</ul></li>
<li class="appendix"><span><b>Annexes</b></span></li>
<li class="chapter" data-level="A" data-path="rappels-de-probabilités-statistiques-et-doptimisation.html"><a href="rappels-de-probabilités-statistiques-et-doptimisation.html"><i class="fa fa-check"></i><b>A</b> Rappels de probabilités, statistiques et d’optimisation</a><ul>
<li class="chapter" data-level="A.1" data-path="rappels-de-probabilités-statistiques-et-doptimisation.html"><a href="rappels-de-probabilités-statistiques-et-doptimisation.html#rappels-sur-les-échantillons-gaussiens"><i class="fa fa-check"></i><b>A.1</b> Rappels sur les échantillons gaussiens</a><ul>
<li class="chapter" data-level="A.1.1" data-path="rappels-de-probabilités-statistiques-et-doptimisation.html"><a href="rappels-de-probabilités-statistiques-et-doptimisation.html#la-loi-normale"><i class="fa fa-check"></i><b>A.1.1</b> La loi normale</a></li>
<li class="chapter" data-level="A.1.2" data-path="rappels-de-probabilités-statistiques-et-doptimisation.html"><a href="rappels-de-probabilités-statistiques-et-doptimisation.html#vecteurs-gaussiens"><i class="fa fa-check"></i><b>A.1.2</b> Vecteurs gaussiens</a></li>
<li class="chapter" data-level="A.1.3" data-path="rappels-de-probabilités-statistiques-et-doptimisation.html"><a href="rappels-de-probabilités-statistiques-et-doptimisation.html#loi-du-khi-deux-loi-de-student-loi-de-fisher"><i class="fa fa-check"></i><b>A.1.3</b> Loi du khi-deux, loi de Student, loi de Fisher</a></li>
<li class="chapter" data-level="A.1.4" data-path="rappels-de-probabilités-statistiques-et-doptimisation.html"><a href="rappels-de-probabilités-statistiques-et-doptimisation.html#estimation-de-la-moyenne-et-de-la-variance-dun-échantillon-gaussien"><i class="fa fa-check"></i><b>A.1.4</b> Estimation de la moyenne et de la variance d’un échantillon gaussien</a></li>
<li class="chapter" data-level="A.1.5" data-path="rappels-de-probabilités-statistiques-et-doptimisation.html"><a href="rappels-de-probabilités-statistiques-et-doptimisation.html#construction-dintervalles-de-confiance"><i class="fa fa-check"></i><b>A.1.5</b> Construction d’intervalles de confiance</a></li>
</ul></li>
<li class="chapter" data-level="A.2" data-path="rappels-de-probabilités-statistiques-et-doptimisation.html"><a href="rappels-de-probabilités-statistiques-et-doptimisation.html#estimation-sans-biais-de-variance-minimale"><i class="fa fa-check"></i><b>A.2</b> Estimation sans biais de variance minimale</a></li>
<li class="chapter" data-level="A.3" data-path="rappels-de-probabilités-statistiques-et-doptimisation.html"><a href="rappels-de-probabilités-statistiques-et-doptimisation.html#Newton-Raphson"><i class="fa fa-check"></i><b>A.3</b> La méthode de Newton-Raphson</a></li>
<li class="chapter" data-level="A.4" data-path="rappels-de-probabilités-statistiques-et-doptimisation.html"><a href="rappels-de-probabilités-statistiques-et-doptimisation.html#théorème-central-limite-condition-de-lindeberg"><i class="fa fa-check"></i><b>A.4</b> Théorème central limite: condition de Lindeberg</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="preuves-de-quelques-résultats-du-cours.html"><a href="preuves-de-quelques-résultats-du-cours.html"><i class="fa fa-check"></i><b>B</b> Preuves de quelques résultats du cours</a><ul>
<li class="chapter" data-level="B.1" data-path="preuves-de-quelques-résultats-du-cours.html"><a href="preuves-de-quelques-résultats-du-cours.html#ProofFisher"><i class="fa fa-check"></i><b>B.1</b> Preuve pour le test de Fisher</a></li>
<li class="chapter" data-level="B.2" data-path="preuves-de-quelques-résultats-du-cours.html"><a href="preuves-de-quelques-résultats-du-cours.html#annexe:ortho"><i class="fa fa-check"></i><b>B.2</b> Preuve de la proposition @ref(prp:Proportho)</a></li>
<li class="chapter" data-level="B.3" data-path="preuves-de-quelques-résultats-du-cours.html"><a href="preuves-de-quelques-résultats-du-cours.html#annexe:risque"><i class="fa fa-check"></i><b>B.3</b> Preuve de la proposition @ref(prp:risque)</a></li>
<li class="chapter" data-level="B.4" data-path="preuves-de-quelques-résultats-du-cours.html"><a href="preuves-de-quelques-résultats-du-cours.html#annexe:KL"><i class="fa fa-check"></i><b>B.4</b> Preuve de la proposition @ref(prp:KL)</a></li>
<li class="chapter" data-level="B.5" data-path="preuves-de-quelques-résultats-du-cours.html"><a href="preuves-de-quelques-résultats-du-cours.html#annexe:Mallows"><i class="fa fa-check"></i><b>B.5</b> Critère du <span class="math inline">\(C_p\)</span> de Mallows</a></li>
<li class="chapter" data-level="B.6" data-path="preuves-de-quelques-résultats-du-cours.html"><a href="preuves-de-quelques-résultats-du-cours.html#annexe:Sj"><i class="fa fa-check"></i><b>B.6</b> Preuve de la proposition @ref(prp:eqSj)</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li>Cathy Maugis-Rabusseau</li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Modèle linéaire général et modèle linéaire généralisé</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regression" class="section level1">
<h1><span class="header-section-number">Chapitre 6</span> La régression linéaire</h1>
<blockquote>
<p>Les slides associés à la régression linéaire sont disponibles ici <a href="">SlidesRegLineaire.pdf</a></p>
<p>Le jeu de données utilisé dans ce chapitre est disponible ici <a href="Data/fitness.txt">fitness.txt</a></p>
</blockquote>
<div id="introduction" class="section level2">
<h2><span class="header-section-number">6.1</span> Introduction</h2>
<div id="exemple-illustratif" class="section level3">
<h3><span class="header-section-number">6.1.1</span> Exemple illustratif</h3>
<p>Pour illustrer les notions abordées dans ce chapitre, nous allons considérer l’exemple suivant :
on mesure pour 31 personnes lors de séances d’aérobic les <span class="math inline">\(7\)</span> variables suivantes :</p>
<ul>
<li>age (a): age</li>
<li>weight (w): poids</li>
<li>oxy (oxy): consommation d’oxygène</li>
<li>runtime (run): temps de l’effort</li>
<li>rstpulse (rst): mesure de pulsation cardiaque 1</li>
<li>runpulse (rp): mesure de pulsation cardiaque 2</li>
<li>maxpulse (maxp): mesure de pulsation cardiaque 3</li>
</ul>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="regression.html#cb7-1"></a>fitness=<span class="kw">read.table</span>(<span class="st">&quot;Data/fitness.txt&quot;</span>,<span class="dt">header=</span>T)</span>
<span id="cb7-2"><a href="regression.html#cb7-2"></a><span class="kw">head</span>(fitness)</span></code></pre></div>
<pre><code>  age weight    oxy runtime rstpulse runpulse maxpulse
1  44  89.47 44.609   11.37       62      178      182
2  40  75.07 45.313   10.07       62      185      185
3  44  85.84 54.297    8.65       45      156      168
4  42  68.15 59.571    8.17       40      166      172
5  38  89.02 49.874    9.22       55      178      180
6  47  77.45 44.811   11.63       58      176      176</code></pre>
<!--Le jeu de données **fitness.txt** ainsi que le fichier **ExRegressionLineaire.Rmd** contenant le script R illustrant ce chapitre sont disponibles sur la page moodle du cours. 
Quelques statistiques descriptives sont codées dans le script, les boxplots et les corrélations deux à deux entre les variables quantitatives sont représentés sur la Figure~\ref{FigEx}-->
<p>L’objectif est d’étudier si la consommation d’oxygène (variable réponse <span class="math inline">\(Y\)</span>=oxy) peut être expliquée linéairement par les <span class="math inline">\(6\)</span> autres variables quantitatives.</p>
<p>Quelques statistiques descriptives sont données ci-dessous. Les boxplots et les corrélations deux à deux entre les variables quantitatives sont représentés sur la Figure <a href="regression.html#fig:FigEx">6.1</a></p>
<div class="figure"><span id="fig:FigEx"></span>
<img src="Bookdown-poly_files/figure-html/FigEx-1.png" alt="\label{fig:FigEx}Description des données. A gauche, boxplot des différentes variables quantitatives. A droite, représentation graphique des corrélations deux à deux des variables quantitatives." width="672" />
<p class="caption">
Figure 6.1: Description des données. A gauche, boxplot des différentes variables quantitatives. A droite, représentation graphique des corrélations deux à deux des variables quantitatives.
</p>
</div>
</div>
<div id="problématique" class="section level3">
<h3><span class="header-section-number">6.1.2</span> Problématique</h3>
<p>La régression est une des méthodes les plus connues et les plus appliquées en statistique pour l’analyse des données quantitatives. Elle est utilisée pour établir une liaison entre une variable quantitative, et une ou plusieurs autres variables quantitatives, sous la forme d’un modèle.</p>
<p>Si on s’intéresse à la relation entre deux variables (par exemple, la consommation d’oxygène <em>oxy</em> en fonction du temps de l’effort <em>runtime</em>), on parlera de <strong>régression simple</strong> en exprimant une variable en fonction de l’autre.
Si la relation porte entre une variable et plusieurs autres variables (par exemple, la variable <em>oxy</em> fonction de toutes les autres variables quantitatives), on parlera de <strong>régression multiple</strong>. La mise en oeuvre d’une régression impose l’existence d’une relation de cause à effet entre les variables prises en compte dans le modèle.</p>
<p>Cette méthode peut être mise en place sur des données quantitatives observées sur <span class="math inline">\(n\)</span> individus et présentées sous la forme :</p>
<ul>
<li>une variable quantitative <span class="math inline">\(Y\)</span> prenant la valeur <span class="math inline">\(Y_i\)</span> pour l’individu <span class="math inline">\(i, i=1,\cdots, n\)</span> appelée <strong>variable à expliquer</strong> ou <strong>variable réponse</strong>,</li>
<li><span class="math inline">\(p\)</span> variables quantitatives <span class="math inline">\(z^{(1)}, z^{(2)}, \cdots, z^{(p)}\)</span> prenant respectivement les valeurs <span class="math inline">\(z^{(1)}_i, z^{(2)}_i, \cdots, z^{(p)}_i\)</span> pour l’individu <span class="math inline">\(i\)</span>, appelées <strong>variables explicatives</strong> ou <strong>prédicteurs</strong>. Si <span class="math inline">\(p=1\)</span>, on est dans le cas de la régression simple.
<!--Lorsque les valeurs prises par une variable explicative sont choisies par l'expérimentateur, on dit que la variable explicative est *contrôlée*.--></li>
</ul>
<p>Dans notre exemple, <span class="math inline">\(n=31\)</span>, <span class="math inline">\(Y\)</span> est la variable <em>oxy</em> et <span class="math inline">\(p=6\)</span>.</p>
<p>Considérons un couple de variables quantitatives <span class="math inline">\((Y,Z)\)</span>. S’il existe une liaison entre ces deux variables, la connaissance de la valeur prise par <span class="math inline">\(Z\)</span> change notre incertitude concernant la réalisation de <span class="math inline">\(Y\)</span>. Si l’on admet qu’il existe une relation de cause à effet entre <span class="math inline">\(Z\)</span> et <span class="math inline">\(Y\)</span>, le phénomène représenté par <span class="math inline">\(Z\)</span> peut donc servir à prédire celui représenté par <span class="math inline">\(Y\)</span> et la liaison s’écrit sous la forme <span class="math inline">\(y=f(z)\)</span>. On dit que l’on fait de la régression de <span class="math inline">\(Y\)</span> sur <span class="math inline">\(Z\)</span>.</p>
<p>Dans le cas les plus fréquents, on choisit l’ensemble des fonctions affines (du type <span class="math inline">\(f(z)=\theta_0 + \theta_1 z\)</span> ou <span class="math inline">\(f(z^{(1)},z^{(2)},\cdots,z^{(p)})=\theta_0+\theta_1z^{(1)}+\theta_2z^{(2)}+\cdots+\theta_p z^{(p)}\)</span>) et on parle de <strong>régression linéaire</strong>.</p>
</div>
<div id="le-modèle-de-régression-linéaire-simple" class="section level3">
<h3><span class="header-section-number">6.1.3</span> Le modèle de régression linéaire simple</h3>
<p>Soit un échantillon de <span class="math inline">\(n\)</span> individus. Pour un individu <span class="math inline">\(i\)</span> <span class="math inline">\((i=1,\cdots,n)\)</span>, on a observé</p>
<ul>
<li><span class="math inline">\(Y_i\)</span> la valeur de la variable quantitative <span class="math inline">\(Y\)</span> (ex: la consommation d’oxygène <em>oxy</em>),</li>
<li><span class="math inline">\(z_i\)</span> la valeur de la variable quantitative <span class="math inline">\(z\)</span> (ex: le temps d’effort <em>runtime</em>)</li>
</ul>
<p>On veut étudier la relation entre ces deux variables, et en particulier, l’effet de <span class="math inline">\(z\)</span> (<em>variable explicative</em>) sur <span class="math inline">\(Y\)</span> (<em>variable réponse</em>). Dans un premier temps, on peut représenter graphiquement cette relation en traçant le nuage des <span class="math inline">\(n\)</span> points de coordonnées <span class="math inline">\((z_i,Y_i)_{1\leq i \leq n}\)</span> (cf Figure <a href="regression.html#fig:FigExRegSimple">6.2</a>). Dans le cas où le nuage de points est de forme “linéaire”, on cherchera à ajuster ce nuage de points par une droite. La relation entre <span class="math inline">\(Y_i\)</span> et <span class="math inline">\(z_i\)</span> s’écrit alors sous la forme d’un modèle de régression linéaire simple :
<span class="math display" id="eq:eqRegSimple">\[\begin{equation}
\left\{ \begin{array}{l}
Y_i=\theta_0+\theta_1z_i +\varepsilon_i, \, \forall i =1,\cdots, n, \\
\\
\varepsilon_1,\ldots,\varepsilon_n \textrm{ i.i.d de loi } \mathcal{N}(0,\sigma^2)
\end{array}\right.
\tag{6.1}
\end{equation}\]</span></p>
<p>La première partie du modèle <span class="math inline">\(\theta_0+\theta_1z_i\)</span> représente la moyenne de <span class="math inline">\(Y_i\)</span> sachant <span class="math inline">\(z_i\)</span> et la seconde partie <span class="math inline">\(\varepsilon_i\)</span>, la différence entre cette moyenne et la valeur <span class="math inline">\(Y_i\)</span>. Le nuage de points est résumé par la droite d’équation <span class="math inline">\(y=\theta_0+\theta_1z\)</span>.</p>
<div class="figure"><span id="fig:FigExRegSimple"></span>
<img src="Bookdown-poly_files/figure-html/FigExRegSimple-1.png" alt="\label{fig:FigExRegSimple}Représentation du oxy en fonction de runtime. En rouge, la droite de régression linéaire simple ajustée." width="672" />
<p class="caption">
Figure 6.2: Représentation du oxy en fonction de runtime. En rouge, la droite de régression linéaire simple ajustée.
</p>
</div>
</div>
<div id="le-modèle-de-régression-linéaire-multiple" class="section level3">
<h3><span class="header-section-number">6.1.4</span> Le modèle de régression linéaire multiple</h3>
<p>On dispose d’un échantillon de <span class="math inline">\(n\)</span> individus pour lesquels on a observé</p>
<ul>
<li><span class="math inline">\(Y_i\)</span> la valeur de la variable réponse <span class="math inline">\(Y\)</span> quantitative (ex: variable <em>oxy</em>),</li>
<li><span class="math inline">\(z^{(1)}_i, \cdots, z^{(p)}_i\)</span> les valeurs de <span class="math inline">\(p\)</span> autres variables quantitatives <span class="math inline">\(z^{(1)},\cdots,z^{(p)}\)</span>.</li>
</ul>
<p>On veut expliquer la variable quantitative <span class="math inline">\(Y\)</span> par les <span class="math inline">\(p\)</span> variables quantitatives <span class="math inline">\(z^{(1)},\cdots,z^{(p)}\)</span>.
Le modèle s’écrit
<span class="math display" id="eq:eqRegMulti">\[\begin{equation}
\left\{\begin{array}{l}
Y_i=\theta_0+\theta_1z^{(1)}_i+\cdots, +\theta_pz^{(p)}_i+\varepsilon_i,  \forall i=1,\cdots, n,\\
\\
\varepsilon_1,\ldots,\varepsilon_n \textrm{ i.i.d de loi } \mathcal{N}(0,\sigma^2)
\end{array}\right.
\tag{6.2}
\end{equation}\]</span></p>
</div>
</div>
<div id="estimation" class="section level2">
<h2><span class="header-section-number">6.2</span> Estimation</h2>
<div id="résultats-généraux" class="section level3">
<h3><span class="header-section-number">6.2.1</span> Résultats généraux</h3>
<p>Le modèle <a href="regression.html#eq:eqRegMulti">(6.2)</a> peut se réécrire sous la forme matricielle</p>
<p><span class="math display">\[
\underbrace{\left(\begin{array}{c} Y_1 \\ Y_2 \\ \vdots \\ Y_n \end{array}\right)}_{Y}
=
\underbrace{\left(\begin{array}{c c c c c } 
1&amp; z_1^{(1)}&amp; z_1^{(2)}&amp; \ldots&amp; z_1^{(p)}\\ 
1&amp; z_2^{(1)}&amp; z_2^{(2)}&amp; \ldots&amp; z_2^{(p)}\\ 
\vdots&amp;\vdots &amp;\vdots &amp;\vdots &amp; \vdots\\ 
1&amp; z_n^{(1)}&amp; z_n^{(2)}&amp; \ldots&amp; z_n^{(p)} 
\end{array}\right)}_{X}
\underbrace{\left(\begin{array}{c} \theta_0\\ \theta_1\\ \vdots \\ \theta_p\end{array}\right)}_{\theta}
+
\underbrace{\left(\begin{array}{c} \varepsilon_1 \\ \varepsilon_2 \\ \vdots \\ \varepsilon_n \end{array}\right)}_{\varepsilon}
\]</span></p>
<p>où <span class="math inline">\(X \in \mathcal{M}_{n,p+1}(\mathbb{R})\)</span> (ici, <span class="math inline">\(k=p+1\)</span>).
Si le modèle est régulier, on peut alors estimer le vecteur des paramètres <span class="math inline">\(\theta\)</span> par la méthode des moindres carrés d’où
<span class="math display">\[
\widehat{\theta}=(X&#39;X)^{-1}X&#39;Y \sim \mathcal{N}_{p+1}(\theta,\sigma^2(X&#39;X)^{-1}).
\]</span>
On en déduit alors <span class="math inline">\(\widehat{Y}_i=(X \widehat{\theta})_i = \widehat{\theta}_0+\sum_{j=1}^p\widehat{\theta}_j z^{(j)}_i\)</span> la valeur ajustée de <span class="math inline">\(Y_i\)</span> et le résidu <span class="math inline">\(\widehat{\varepsilon_i}=Y_i-\widehat{Y_i}\)</span>.
<!--, de valeur observée $(\widehat\varepsilon_i)^{obs} = y_i-\widehat{y_i}$.--></p>
<p>La variance <span class="math inline">\(\sigma^2\)</span> est estimée par
<span class="math display">\[\widehat{\sigma}^2=\frac{\|Y - X \widehat{\theta}\|^2}{n-(p+1)}= \frac{1}{n-(p+1)}\sum_{i=1}^n \left(\widehat{\varepsilon_i}\right)^2.\]</span></p>
<p>Les erreurs standards des estimateurs <span class="math inline">\(\widehat{\theta_0}, \cdots, \widehat{\theta_p}\)</span>, des valeurs ajustées et des résidus calculés valent:</p>
<ul>
<li>erreur standard de <span class="math inline">\(\widehat{\theta_j}\)</span> vaut <span class="math inline">\(se(\widehat{\theta_j}) = \sqrt{\widehat{\sigma^2}[(X&#39;X)^{-1}]_{j+1,j+1}}\)</span></li>
<li>erreur standard de <span class="math inline">\(\widehat{Y_i}\)</span> vaut <span class="math inline">\(se(\widehat{Y_i})= \sqrt{\widehat{\sigma^2}[X(X&#39;X)^{-1}X&#39;]_{ii}}=\sqrt{\widehat{\sigma^2}H_{ii}}\)</span></li>
<li>erreur standard de <span class="math inline">\(\widehat{\varepsilon_i}\)</span> vaut <span class="math inline">\(se(\widehat{\varepsilon_i}) = \sqrt{\widehat{\sigma^2}(1-H_{ii})}.\)</span></li>
</ul>
<div class="exercise">
<p><span id="exr:ExoRegSimple" class="exercise"><strong>Exercise 6.1  </strong></span>On se place dans le cadre de la régression linéaire simple d’équation <a href="regression.html#eq:eqRegSimple">(6.1)</a>. Montrez que les estimateurs de <span class="math inline">\(\theta_0\)</span> et <span class="math inline">\(\theta_1\)</span> par la méthode des moindres carrés sont donnés par :
<span class="math display">\[\left\lbrace
  \begin{array}{l}
     \widehat{\theta}_1 =  \displaystyle \frac{cov(Y,z)}{ var(z)}=\frac{\strut \sum\limits_{i=1}^n(z_i - \overline{z}) (Y_i - \overline{Y})}{\sum\limits_{i=1}^n (z_i - \overline{z})^2 \strut},\\
     \widehat{\theta}_0 = \overline{Y} - \widehat{\theta}_1 \, \overline{z},
  \end{array}
       \right.\]</span>
où <span class="math inline">\(\overline{z}=\frac{1}{n}\sum_{i=1}^n z_i\)</span> et <span class="math inline">\(\overline{Y}=\frac{1}{n}\sum_{i=1}^n Y_i\)</span>.</p>
<p>Pour cela, on cherche à minimiser la fonction des moindres carrés
<span class="math display">\[
(a,b) \mapsto \sum_{i=1}^n (Y_i - a - b z_i)^2.
\]</span></p>
</div>
<div id="exregsimple" class="section level4">
<h4><span class="header-section-number">6.2.1.1</span> Exemple en régression linéaire simple</h4>
<p>Sous R, on peut ajuster le modèle de régression linéaire à l’aide de la fonction <code>lm()</code>avec la syntaxe suivante :</p>
<pre><code>
Call:
lm(formula = oxy ~ runtime, data = fitness)

Residuals:
    Min      1Q  Median      3Q     Max 
-5.3352 -1.8424 -0.0569  1.5342  6.2033 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  82.4218     3.8553  21.379  &lt; 2e-16 ***
runtime      -3.3106     0.3612  -9.166 4.59e-10 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 2.745 on 29 degrees of freedom
Multiple R-squared:  0.7434,    Adjusted R-squared:  0.7345 
F-statistic: 84.01 on 1 and 29 DF,  p-value: 4.585e-10</code></pre>
<p>On a en particulier <span class="math inline">\((\widehat{\theta}_0)^{obs} =\)</span> 82.422 et <span class="math inline">\((\widehat{\theta}_1)^{obs} =\)</span> -3.311 ainsi que leur erreur standard dans la colonne suivante.</p>
<p>Sur le nuage de point, on obtient donc l’ajustement de la droite de régression linéaire en rouge</p>
<p><img src="Bookdown-poly_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
</div>
<div id="exregmulti" class="section level4">
<h4><span class="header-section-number">6.2.1.2</span> Exemple en régression linéaire multiple</h4>
<p>Les résultats obtenus avec la commande <code>lm()</code> pour l’exemple de la régression linéaire multiple sont donnés ci-dessous. Les deux premières colonnes correspondent aux estimations et aux erreurs standards respectivement pour chaque paramètre.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="regression.html#cb10-1"></a>reg.multi&lt;-<span class="kw">lm</span>(oxy<span class="op">~</span>.,<span class="dt">data=</span>fitness)</span>
<span id="cb10-2"><a href="regression.html#cb10-2"></a><span class="kw">summary</span>(reg.multi)</span></code></pre></div>
<pre><code>
Call:
lm(formula = oxy ~ ., data = fitness)

Residuals:
    Min      1Q  Median      3Q     Max 
-5.4026 -0.8991  0.0706  1.0496  5.3847 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 102.93448   12.40326   8.299 1.64e-08 ***
age          -0.22697    0.09984  -2.273  0.03224 *  
weight       -0.07418    0.05459  -1.359  0.18687    
runtime      -2.62865    0.38456  -6.835 4.54e-07 ***
rstpulse     -0.02153    0.06605  -0.326  0.74725    
runpulse     -0.36963    0.11985  -3.084  0.00508 ** 
maxpulse      0.30322    0.13650   2.221  0.03601 *  
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 2.317 on 24 degrees of freedom
Multiple R-squared:  0.8487,    Adjusted R-squared:  0.8108 
F-statistic: 22.43 on 6 and 24 DF,  p-value: 9.715e-09</code></pre>
</div>
</div>
<div id="propriétés-en-régression-linéaire-simple" class="section level3">
<h3><span class="header-section-number">6.2.2</span> Propriétés en régression linéaire simple</h3>
<p>On se place dans cette section dans le cadre de la régression linéaire simple (cf Equation <a href="regression.html#eq:eqRegSimple">(6.1)</a>). La proposition suivante donne des propriétés entre les résidus et les valeurs prédites par le modèle.</p>
<div class="proposition">
<p><span id="prp:prop61" class="proposition"><strong>Proposition 6.1  </strong></span>Les résidus et les valeurs prédites vérifient les propriétés suivantes</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\sum_{i=1}^n \widehat{\varepsilon_i } = 0\)</span>, <span class="math inline">\(\sum_{i=1}^n \widehat{Y_i } = \sum_{i=1}^n Y_i\)</span>.</li>
<li>La droite de régression passe par le point de coordonnées <span class="math inline">\((\overline z, \overline Y)\)</span>.</li>
<li>Le vecteur des résidus n’est pas corrélé avec la variable explicative : <span class="math inline">\(cov(z, \widehat{\varepsilon})=0\)</span>.</li>
<li>Le vecteur des résidus n’est pas corrélé avec la variable ajustée : <span class="math inline">\(cov(\widehat{Y},\widehat{\varepsilon})=0\)</span>.</li>
<li>La variance de <span class="math inline">\(Y\)</span> admet la décomposition :
<span class="math display" id="eq:DecompVar">\[\begin{equation}
\tag{6.3}
var(Y)=var(\widehat{Y})+ var(\widehat{\varepsilon}).
\end{equation}\]</span></li>
<li>Le carré du coefficient de corrélation de <span class="math inline">\(z\)</span> et de <span class="math inline">\(Y\)</span> s’écrit sous les formes suivantes :
<span class="math display">\[r^2(z,Y)=\frac{var(\widehat{Y})}{var(Y)}= 1- \frac{var(\widehat{\varepsilon})}{var(Y)}.\]</span>
On en déduit que la variance empirique de <span class="math inline">\(Y\)</span> se décompose en somme d’une part de <em>variance expliquée</em> <span class="math inline">\((var(\widehat{Y}))\)</span> et d’une <em>variance résiduelle</em> <span class="math inline">\((var(\widehat{\varepsilon}))\)</span>, et que <span class="math inline">\(r^2(z,Y)\)</span> est le rapport de la variance expliquée sur la variance de la variable à expliquer.</li>
</ol>
</div>
<div class="proof">
<p><span id="unlabeled-div-33" class="proof"><em>Proof</em>. </span>En utilisant que <span class="math inline">\(\widehat\varepsilon_i = Y_i - \widehat{Y}_i\)</span>, <span class="math inline">\(\widehat{Y_i } = \widehat{\theta_0} + \widehat{\theta_1} z_i\)</span>
et <span class="math inline">\(\widehat{\theta_0} = \bar Y - \widehat{\theta_1} \bar z\)</span>, on a</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\frac 1 n \sum_{i=1}^n \widehat{\varepsilon_i } = \frac 1 n \sum_{i=1}^n \left\{Y_i - [\widehat\theta_0 + \widehat\theta_1 z_i] \right\} = \bar Y - \widehat\theta_0 - \widehat\theta_1 \bar z = 0\)</span> par définition de <span class="math inline">\(\widehat\theta_0\)</span>.</li>
<li><span class="math inline">\(\widehat\theta_0 + \widehat\theta_1 \bar z = \left[\bar Y - \widehat\theta_1 \bar z \right] + \widehat\theta_1 \bar z =\bar Y\)</span></li>
<li>On a
<span class="math display">\[\begin{eqnarray*}
n\ cov(z,\widehat\varepsilon) &amp;=&amp; \sum_{i=1}^n \widehat\varepsilon_i (z_i - \bar z)\\
&amp;=&amp; \sum_{i=1}^n [Y_i - \bar Y - \widehat{\theta}_1(z_i - \bar z)][z_i - \bar z]\\
&amp;=&amp; n\, \left\{cov(Y,z) - \widehat{\theta}_1var(z)\right\} = 0
\end{eqnarray*}\]</span>
par définition de <span class="math inline">\(\widehat\theta_1\)</span>.</li>
<li><span class="math inline">\(n\ cov(\widehat Y, \widehat\varepsilon) = \sum_{i=1}^n \widehat\varepsilon_i (\widehat Y_i -\bar Y) = \sum_{i=1}^n \widehat\varepsilon_i \widehat\theta_1 (z_i - \bar z) = n \widehat\theta_1 cov(z,\widehat\varepsilon) = 0.\)</span></li>
<li><span class="math inline">\(n\, var(Y) = \sum_{i=1}^n (Y_i - \widehat Y_i + \widehat Y_i - \bar Y)^2 = n\, var(\widehat\varepsilon) + n\, var(\widehat Y) + 2 n\, cov(\widehat\varepsilon,\widehat Y).\)</span></li>
<li>On a <span class="math inline">\(r^2(z,Y) = \frac{cov(z,Y)^2}{var(z) var(Y)}\)</span> et
<span class="math display">\[
n\, cov(z,Y) = \sum_{i=1}^n (Y_i -\widehat Y_i + \widehat Y_i - \bar Y) (z_i - \bar z) = n\,cov(\widehat\varepsilon,z) + n\, cov(\widehat Y,z) = n\, cov(\widehat Y,z).
\]</span>
Ainsi, <span class="math display">\[r^2(z,Y) = \frac{cov(\widehat Y, z)^2}{var(z) var(\widehat Y)} \frac{var(\widehat Y)}{var(Y)} = cor(\widehat Y,z)^2 \frac{var(\widehat Y)}{var(Y)} = \frac{var(\widehat Y)}{var(Y)}\]</span>
car <span class="math inline">\(\widehat Y_i = \widehat\theta_0 + \widehat \theta_1 z_i, \forall i\)</span> (relation linéaire).<br />
</li>
</ol>
</div>
</div>
<div id="le-coefficient-r2" class="section level3">
<h3><span class="header-section-number">6.2.3</span> Le coefficient <span class="math inline">\(R^2\)</span></h3>
<div id="définition" class="section level4">
<h4><span class="header-section-number">6.2.3.1</span> Définition</h4>
<p>Le coefficient <span class="math inline">\(R^2\)</span>, défini comme le carré du coefficient de corrélation de <span class="math inline">\(z\)</span> et <span class="math inline">\(Y\)</span> est une mesure de qualité de l’ajustement, égale au rapport de la variance effectivement expliquée sur la variance à expliquer :
<span class="math display">\[R^2= r^2(z,Y)=\frac{var(\widehat{Y})}{var(Y)}.\]</span>
Ainsi <span class="math inline">\(R^2 \in [0,1]\)</span> et s’interprète comme la <em>proportion de variance expliquée par la régression</em>.</p>
<p>La plupart des logiciels n’utilise pas la décomposition <a href="regression.html#eq:DecompVar">(6.3)</a>, mais plutôt la décomposition obtenue en multipliant cette expression par <span class="math inline">\(n\)</span> :
<span class="math display">\[SST = SSE + SSR\]</span>
où</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\displaystyle SST = \|Y - \overline Y \mathbb{1}_n\|^2=\sum_{i=1}^n (Y_i-\overline Y)^2\)</span> est la somme totale des carrés corrigés de <span class="math inline">\(Y\)</span>,</li>
<li><span class="math inline">\(\displaystyle SSE =\|\widehat{Y}-\overline Y \mathbb{1}_n\|^2= \sum_{i=1}^n (\widehat{Y_i}-\overline Y)^2\)</span> est la somme des carrés expliquée par le modèle,</li>
<li><span class="math inline">\(\displaystyle SSR =\|Y - \widehat{Y}\|^2= \sum_{i=1}^n (\widehat{\varepsilon_i})^2\)</span> est la somme des carrés des résidus.</li>
</ol>
<p>Ainsi, pour calculer le <span class="math inline">\(R^2\)</span>, on utilise également l’expression
<span class="math display">\[R^2=  \frac{SSE}{SST}= 1- \frac{SSR}{SST}.\]</span></p>
<p>Dans l’exemple de la régression linéaire simple, la valeur du <span class="math inline">\(R^2\)</span> vaut <span class="math inline">\(0.8078\)</span>. Pour retrouver les valeurs de <span class="math inline">\(SST\)</span>, <span class="math inline">\(SSR\)</span> et <span class="math inline">\(SSE\)</span>, on peut utiliser la commande <code>anova()</code>.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="regression.html#cb12-1"></a><span class="kw">anova</span>(reg.simple)</span></code></pre></div>
<pre><code>Analysis of Variance Table

Response: oxy
          Df Sum Sq Mean Sq F value    Pr(&gt;F)    
runtime    1 632.90  632.90  84.008 4.585e-10 ***
Residuals 29 218.48    7.53                      
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Dans le cas d’une régression multiple de <span class="math inline">\(Y\)</span> par <span class="math inline">\(z^{(1)}, \cdots, z^{(p)}\)</span>, le <em>coefficient de corrélation multiple</em> noté <span class="math inline">\(r(Y,z^{(1)},\cdots,z^{(p)})\)</span> est défini comme le coefficient de corrélation linéaire empirique de <span class="math inline">\(Y\)</span> par <span class="math inline">\(\widehat{Y}\)</span> :
<span class="math display">\[r(Y,z^{(1)}, \cdots, z^{(p)})=r(Y,\widehat{Y}).\]</span>
Ainsi le coefficient <span class="math inline">\(R^2\)</span> de la régression multiple est égal au carré du coefficient de corrélation linéaire multiple empirique <span class="math inline">\(r(Y,z^{(1)}, \cdots, z^{(p)})\)</span>.
Dans l’exemple de la régression linéaire multiple, la valeur du <span class="math inline">\(R^2\)</span> vaut 0.849.</p>
</div>
<div id="augmentation-mécanique-du-r2" class="section level4">
<h4><span class="header-section-number">6.2.3.2</span> Augmentation mécanique du <span class="math inline">\(R^2\)</span></h4>
<p>Lorsque l’on ajoute une variable explicative à un modèle, la somme des carrés des résidus diminue ou au moins reste stable. En effet, si on considère un modèle à <span class="math inline">\(p-1\)</span> variables :
<span class="math display">\[Y_i=\theta_0+\theta_1z^{(1)}_i + \cdots + \theta_{p-1}z^{(p-1)}_i + \varepsilon_i\]</span>
alors les coefficients <span class="math inline">\((\widehat{\theta}_0, \widehat{\theta}_1, \cdots, \widehat{\theta}_{p-1})\)</span> estimés minimisent
<span class="math display">\[\phi(\theta_0,\theta_1, \cdots, \theta_{p-1})=\sum_{i=1}^n\left[Y_i-(\theta_0+\theta_1z^{(1)}_i + \cdots + \theta_{p-1}z^{(p-1)}_i)\right]^2.\]</span>
Si on rajoute une nouvelle variable explicative <span class="math inline">\(z^{(p)}\)</span> au modèle, on obtient
<span class="math display">\[Y_i=\theta_0+\theta_1z^{(1)}_i + \cdots + \theta_{p-1}z^{(p-1)}_i + \theta_pz^{(p)}_i+ \varepsilon_i,\]</span>
et les coefficients estimés, notés <span class="math inline">\((\widetilde{\theta}_0, \widetilde{\theta}_1, \cdots, \widetilde{\theta}_p)\)</span> minimisent la fonction :
<span class="math display">\[\tilde\psi(\theta_0,\theta_1, \cdots, \theta_p)=\sum_{i=1}^n\left[Y_i-(\theta_0+\theta_1z^{(1)}_i + \cdots + \theta_pz^{(p)}_i)\right]^2\]</span>
qui, par construction, vérifie l’égalité :
<span class="math display">\[\tilde\psi(\theta_0,\theta_1, \cdots, \theta_{p-1},0)=\phi(\theta_0,\theta_1, \cdots, \theta_{p-1}).\]</span>
D’où l’inégalité :
<span class="math display">\[\tilde\psi(\widetilde{\theta}_0,\widetilde{\theta}_1, \cdots, \widetilde{\theta}_p) \leq \tilde\psi(\widehat{\theta}_0,\widehat{\theta}_1, \cdots, \widehat{\theta}_{p-1},0) = \phi(\widehat{\theta}_0,\widehat{\theta}_1, \cdots, \widehat{\theta}_{p-1}).\]</span>
Ceci prouve l’augmentation “mécanique” du <span class="math inline">\(R^2\)</span> sans pour autant améliorer le modèle, comme nous le verrons par la suite.</p>
</div>
</div>
</div>
<div id="tests-et-intervalles-de-confiance" class="section level2">
<h2><span class="header-section-number">6.3</span> Tests et intervalles de confiance</h2>
<div id="test-de-nullité-dun-paramètre-du-modèle" class="section level3">
<h3><span class="header-section-number">6.3.1</span> Test de nullité d’un paramètre du modèle</h3>
<p>En testant l’hypothèse nulle <span class="math inline">\(\mathcal{H}_0^{(j)} : \theta_j=0\)</span> où <span class="math inline">\(\theta_j\)</span> est le paramètre associé à la variable explicative <span class="math inline">\(z^{(j)}\)</span>,
on étudie l’effet de la présence de la variable explicative <span class="math inline">\(z^{(j)}\)</span>.</p>
<p>Pour tester <span class="math inline">\(\mathcal{H}_0^{(j)} : \theta_j = 0\)</span> contre <span class="math inline">\(\mathcal{H}_1^{(j)} : \theta_j\neq 0\)</span>, on met en place un test classique de Student.</p>
<ul>
<li>On estime <span class="math inline">\(\theta\)</span> par l’EMC <span class="math inline">\(\widehat{\theta}\)</span> donc <span class="math inline">\(\theta_j\)</span> est estimé par <span class="math inline">\(\widehat{\theta}_j\)</span></li>
<li>Comme <span class="math inline">\(\widehat{\theta}\sim\mathcal{N}_k(\theta,\sigma^2 (X&#39;X)^{-1})\)</span>, on a <span class="math inline">\(\widehat{\theta}_j\underset{\mathcal{H}_0}{\sim} \mathcal{N}(0,\sigma^2 [(X&#39;X)^{-1}]_{jj})\)</span></li>
<li>On estime <span class="math inline">\(\sigma^2\)</span> par <span class="math inline">\(\widehat{\sigma}^2= \frac{\|Y - X \widehat\theta\|^2}{n-k}\)</span></li>
<li>D’après Cochran, <span class="math inline">\(\frac{(n-k)\widehat{\sigma}^2}{\sigma^2}\sim \chi^2(n-k)\)</span> et <span class="math inline">\(\widehat{\theta}_j\)</span> et <span class="math inline">\(\widehat{\sigma}^2\)</span> sont indépendants.</li>
<li>Ainsi la statistique de test
<span class="math display">\[
T_j := \frac{\widehat\theta_j}{\sqrt{\widehat{\sigma}^2 [(X&#39;X)^{-1}]_{jj}}}\underset{\mathcal{H}_0}{\sim} \mathcal{T}(n-k)
\]</span></li>
<li>La zone de rejet est de la forme
<span class="math display">\[
\mathcal{R}_\alpha = \left\{|T_j|&gt; t_{1-\alpha/2,n-k}\right\}
\]</span>
où <span class="math inline">\(t_{1-\alpha/2,n-k}\)</span> est le <span class="math inline">\(1-\alpha/2\)</span> quantile de la loi de Student <span class="math inline">\(\mathcal{T}(n-k)\)</span>.</li>
</ul>
<p>Dans les exemples de la régression linéaire simple (section <a href="regression.html#exregsimple">6.2.1.1</a>) et la régression linéaire multiple (section <a href="regression.html#exregmulti">6.2.1.2</a>), la pvaleur associée au test de nullité de chacun des coefficients <span class="math inline">\(\theta_j\)</span> est donnée dans la dernière colonne (la valeur de la statistique de test est donnée dans l’avant dernière colonne). D’après les résultats dans l’exemple de la régression simple, on rejette fortement la nullité de chacun des coefficients au niveau <span class="math inline">\(5\%\)</span>. Dans l’exemple de la régression multiple, on rejette la nullité des coefficients <span class="math inline">\(\theta_0\)</span>, <span class="math inline">\(\theta_1\)</span>, <span class="math inline">\(\theta_3\)</span>, <span class="math inline">\(\theta_6\)</span> et <span class="math inline">\(\theta_7\)</span> au niveau <span class="math inline">\(5\%\)</span> chacun. Chaque test de nullité est fait séparément, attention aux conclusions trop rapides!</p>
</div>
<div id="test-de-nullité-de-quelques-paramètres-du-modèle" class="section level3">
<h3><span class="header-section-number">6.3.2</span> Test de nullité de quelques paramètres du modèle</h3>
<p>Soit un modèle de référence à <span class="math inline">\(p\)</span> variables explicatives. On veut étudier l’influence de <span class="math inline">\(q\)</span> variables explicatives (avec <span class="math inline">\(q \leq p)\)</span> sur la variable à expliquer. Cela revient à tester l’hypothèse de <strong>nullité de <span class="math inline">\(q\)</span> paramètres du modèle</strong> :
<span class="math display">\[\mathcal{H}_0 : \theta_1 = \theta_2 = \cdots = \theta_q=0, \mbox{ avec } q \leq p.\]</span>
Sous l’hypothèse alternative, au moins un des paramètres <span class="math inline">\(\theta_1, \cdots, \theta_q\)</span> est non nul.</p>
<p>Ce test peut être formulé comme la comparaison de deux modèles emboîtés, l’un à <span class="math inline">\(p+1\)</span> paramètres et l’autre à <span class="math inline">\(p+1-q\)</span> paramètres :
<span class="math display">\[
\begin{array}{c l l}
(M1)  &amp; Y_i=\theta_0 + \theta_1 z^{(1)}_i + \cdots + \theta_p z^{(p)}_i + \varepsilon_i  &amp; \textrm{ sous } \mathcal{H}_1\\
\textrm{versus} &amp; &amp;\\
(M0) &amp; Y_i=\theta_0 + \theta_{q+1} z^{(q+1)}_i + \cdots + \theta_p z^{(p)}_i + \varepsilon_i &amp; \textrm{ sous } \mathcal{H}_0.
\end{array}
\]</span></p>
<p>Dans la suite, on note <span class="math inline">\(Y=Z\beta+\varepsilon\)</span> et <span class="math inline">\(Y=X\theta+\varepsilon\)</span> les formes matricielles des modèles <span class="math inline">\((M0)\)</span> et <span class="math inline">\((M1)\)</span> respectivement.</p>
<p>Pour ce test, on considère la statistique de Fisher :
<span class="math display">\[F=\frac{(SSR_0-SSR_1) / q}{SSR_1 / (n-(p+1))} \underset{\mathcal{H}_0}{\sim} \mathcal F(q,n-(p+1))\]</span>
où <span class="math inline">\(SSR_0=\|Y - Z \widehat{\beta}\|^2\)</span> désigne la somme des carrés des résidus du modèle “réduit” sous <span class="math inline">\(\mathcal{H}_0\)</span> et <span class="math inline">\(SSR_1=\|Y - X \widehat{\theta}\|^2\)</span> correspond à la somme des carrés des résidus du modèle de référence.</p>
<p>La zone de rejet est de la forme <span class="math inline">\(\mathcal{R}_\alpha=\left\{F\geq f_{q,n-p-1,1-\alpha}\right\}\)</span> où
<span class="math inline">\(f_{q,n-p-1,1-\alpha}\)</span> est le <span class="math inline">\(1-\alpha\)</span> quantile de la loi de Fosher <span class="math inline">\(\mathcal{F}(q,n-(p+1))\)</span>.</p>
<p>On remarque que dans le cas où <span class="math inline">\(q=1\)</span>, on teste la nullité d’un seul paramètre du modèle et on retrouve les mêmes conclusions qu’avec le test précédent de Student.</p>
<p>Dans notre exemple en régression linéaire multiple, on souhaite tester le sous-modèle composé uniquement des variables <em>age</em>, <em>runtime</em>,<em>runpulse</em> et <em>maxpulse</em>. A l’aide de la fonction <code>anova()</code>, on va faire un test de Fisher entre ce sous-modèle et le modèle complet :</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="regression.html#cb14-1"></a>regfin&lt;-<span class="kw">lm</span>(oxy<span class="op">~</span>age <span class="op">+</span><span class="st"> </span>runtime<span class="op">+</span>runpulse<span class="op">+</span>maxpulse,<span class="dt">data=</span>fitness)</span>
<span id="cb14-2"><a href="regression.html#cb14-2"></a><span class="kw">anova</span>(regfin,reg.multi)</span></code></pre></div>
<pre><code>Analysis of Variance Table

Model 1: oxy ~ age + runtime + runpulse + maxpulse
Model 2: oxy ~ age + weight + runtime + rstpulse + runpulse + maxpulse
  Res.Df    RSS Df Sum of Sq    F Pr(&gt;F)
1     26 138.93                         
2     24 128.84  2    10.092 0.94 0.4045</code></pre>
<p>La pvaleur valant <span class="math inline">\(0.4045\)</span>, on ne rejette pas le sous-modèle <span class="math inline">\(M_0\)</span> au risque <span class="math inline">\(5\%\)</span>.</p>
<div class="exercise">
<p><span id="exr:unlabeled-div-34" class="exercise"><strong>Exercise 6.2  </strong></span>Dans la sortie R de <code>anova(regfin,reg.multi)</code> ci-dessus, à quoi corresponde chacune des valeurs numériques ?</p>
</div>
</div>
<div id="test-de-nullité-de-tous-les-paramètres-du-modèle" class="section level3">
<h3><span class="header-section-number">6.3.3</span> Test de nullité de tous les paramètres du modèle</h3>
<p>Dans cette section, on souhaite tester l’hypothèse de nullité de tous les paramètres du modèle (associés aux variables explicatives) :
<span class="math display">\[\mathcal{H}_0 : \theta_1 = \cdots = \theta_p=0.\]</span> Ce test revient à comparer la qualité d’ajustement du modèle de référence à celle du “modèle blanc”.
Cette hypothèse composée de <span class="math inline">\(p\)</span> contraintes signifie que les <span class="math inline">\(p\)</span> paramètres associés aux <span class="math inline">\(p\)</span> variables explicatives sont nuls, c’est-à-dire qu’aucune variable explicative présente dans le modèle ne permet d’expliquer la variable <span class="math inline">\(Y\)</span>.</p>
<p>Sous <span class="math inline">\(\mathcal{H}_0\)</span>, le modèle s’écrit :
<span class="math display">\[Y_i=\theta_0+\varepsilon_i \mbox{ avec } \widehat{\theta}_0 = \overline Y\]</span>
et la somme des carrés des résidus (<span class="math inline">\(SRR_0\)</span>) est égale à la somme des carrés totales <span class="math inline">\((SST)\)</span>:
<span class="math display">\[
SSR_0=\|Y - \widehat{\theta}_0 \mathbb{1}_n\|^2=\|Y - \overline Y \mathbb{1}_n\|^2=SST. 
\]</span></p>
<p>La statistique de test de Fisher dans ce cas s’écrit :
<span class="math display">\[
\begin{eqnarray*}
F&amp;=&amp; \frac{SSR_0 - SSR_1 / (P+1-1)}{SSR_1/n-(P+1)}\\
&amp;=&amp;\frac{SSE_1 / p}{SCR_1 / n-(p+1)}\\  &amp;=&amp;\frac{R^2}{1-R^2}\times\frac{n-p-1}{p} \underset{\mathcal{H}_0}{\sim} \mathcal F(p,n-p-1)
\end{eqnarray*}
\]</span>
où <span class="math inline">\(SSE_1\)</span> désigne la somme des carrés du modèle de référence avec <span class="math inline">\(SST = SSE_1+SSR_1\)</span> et <span class="math inline">\(R^2\)</span> est le critère d’ajustement du modèle de référence.</p>
<p>La zone de rejet est de la forme
<span class="math display">\[
\mathcal{R}_\alpha = \left\{ F \geq f_{p,n-(p+1),1-\alpha} \right\}
\]</span>
où <span class="math inline">\(f_{p,n-(p+1),1-\alpha}\)</span> est le <span class="math inline">\(1-\alpha\)</span> quantile de la loi de Fisher <span class="math inline">\(\mathcal{F}(p,n-(p+1))\)</span>.</p>
<p>Dans l’exemple de régression linéaire multiple, on peut mettre en place ce test avec la fonction <code>anova()</code>. On peut aussi remarquer que le résultat de ce test est donné directement dans <code>summary(reg.multi)</code> (voir section <a href="regression.html#exregmulti">6.2.1.2</a>).</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="regression.html#cb16-1"></a>regblanc&lt;-<span class="kw">lm</span>(oxy<span class="op">~</span><span class="dv">1</span>,<span class="dt">data=</span>fitness)</span>
<span id="cb16-2"><a href="regression.html#cb16-2"></a><span class="kw">anova</span>(regblanc,reg.multi)</span></code></pre></div>
<pre><code>Analysis of Variance Table

Model 1: oxy ~ 1
Model 2: oxy ~ age + weight + runtime + rstpulse + runpulse + maxpulse
  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    
1     30 851.38                                  
2     24 128.84  6    722.54 22.433 9.715e-09 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Ici, la pvaleur vaut <span class="math inline">\(9.715e^{-091}\)</span>, on rejette donc l’hypothèse que tous les coefficients sont nuls.</p>
</div>
<div id="intervalle-de-confiance-de-theta_j-de-xtheta_i-et-de-x_0theta-1" class="section level3">
<h3><span class="header-section-number">6.3.4</span> Intervalle de confiance de <span class="math inline">\(\theta_j\)</span>, de <span class="math inline">\((X\theta)_i\)</span> et de <span class="math inline">\(X_0\theta\)</span></h3>
<div id="intervalle-de-confiance-de-theta_j" class="section level4">
<h4><span class="header-section-number">6.3.4.1</span> Intervalle de confiance de <span class="math inline">\(\theta_j\)</span></h4>
<p>On reprend ici la construction générale faite en section <a href="EstML.html#ICthetaj">3.5.1</a>, ici <span class="math inline">\(k=1+p\)</span>.
En utilisant que</p>
<ul>
<li><span class="math inline">\(\widehat \theta_j \sim \mathcal N(\theta_j, \sigma^2 [(X&#39;X)^{-1}]_{j+1,j+1})\)</span></li>
<li><span class="math inline">\(\displaystyle \frac{(n-(p+1)) \widehat\sigma^2}{\sigma^2} \sim \chi^2(n-(p+1))\)</span></li>
<li><span class="math inline">\(\widehat \theta_j\)</span> et <span class="math inline">\(\widehat \sigma^2\)</span> indépendants</li>
</ul>
<p>on obtient que
<span class="math display">\[\frac{\widehat{\theta_j} - \theta_j}{ \sqrt{\widehat \sigma^2 [(X&#39;X)^{-1}]_{j+1,j+1}}} \sim \mathcal{T}(n-(p+1)).\]</span>
On construit alors l’intervalle de confiance suivant pour le paramètre <span class="math inline">\(\theta_j\)</span> au niveau de confiance <span class="math inline">\(1-\alpha\)</span> :
<span class="math display">\[
        IC_{1-\alpha}(\theta_j) = \left[\widehat{\theta_j}\pm t_{n-(p+1),1-\alpha/2} \times \sqrt{\widehat \sigma^2 [(X&#39;X)^{-1}]_{j+1,j+1}}\right].
\]</span>
Dans les deux exemples de ce chapitre, on peut facilement obtenir les intervalles de confiance pour les coefficient <span class="math inline">\(\theta_j\)</span> à l’aide de la fonction <code>confint()</code>.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="regression.html#cb18-1"></a><span class="kw">confint</span>(reg.simple,<span class="dt">level=</span><span class="fl">0.9</span>)</span></code></pre></div>
<pre><code>                  5 %      95 %
(Intercept) 75.871122 88.972424
runtime     -3.924271 -2.696839</code></pre>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="regression.html#cb20-1"></a><span class="kw">confint</span>(reg.multi)</span></code></pre></div>
<pre><code>                  2.5 %       97.5 %
(Intercept) 77.33541293 128.53354604
age         -0.43302821  -0.02091938
weight      -0.18685216   0.03849733
runtime     -3.42235018  -1.83495545
rstpulse    -0.15786297   0.11479569
runpulse    -0.61699207  -0.12226345
maxpulse     0.02150491   0.58492935</code></pre>
</div>
<div id="intervalle-de-confiance-de-xtheta_i" class="section level4">
<h4><span class="header-section-number">6.3.4.2</span> Intervalle de confiance de <span class="math inline">\((X\theta)_i\)</span></h4>
<p>En reprenant la construction faite en section <a href="EstML.html#ICXthetai">3.5.2</a>, l’intervalle de confiance de <span class="math inline">\((X\theta)_i\)</span> au niveau de confiance de <span class="math inline">\(1-\alpha\)</span> est donc donné par :
<span class="math display">\[
IC_{1-\alpha}((X\theta)_i) = \left[\widehat{Y_i}\pm t_{n-(p+1),1-\alpha/2} \times \sqrt{ \widehat \sigma^2 [X(X&#39;X)^{-1}X&#39;]_{ii}}\right].
\]</span>
Pour l’exemple de la régression linéaire simple, ces intervalles de confiance sont représentés en Figure <a href="regression.html#fig:FigICXthetai">6.3</a>.</p>
<div class="figure"><span id="fig:FigICXthetai"></span>
<img src="Bookdown-poly_files/figure-html/FigICXthetai-1.png" alt="\label{fig:FigICXthetai} Intervalle de confiance la réponse moyenne." width="672" />
<p class="caption">
Figure 6.3:  Intervalle de confiance la réponse moyenne.
</p>
</div>
</div>
<div id="intervalle-de-confiance-de-x_0theta" class="section level4">
<h4><span class="header-section-number">6.3.4.3</span> Intervalle de confiance de <span class="math inline">\(X_0\theta\)</span></h4>
<p>Pour des nouvelles données <span class="math inline">\(z_0^{(1)}, \cdots, z_0^{(p)}\)</span> des variables explicatives, on définit <span class="math inline">\(X_0=(1,\, z^{(1)}_0, \, \cdots, \,z_0^{(p)}) \in \mathcal{M}_{1,(p+1)}(\mathbb{R})\)</span>. La réponse moyenne est alors :
<span class="math display">\[
X_0\theta=\theta_0+\sum_{j=1}^p \theta_j z^{(j)}_0 .
\]</span>
En reprenant la construction faite en section <a href="EstML.html#ICX0theta">3.5.3</a>, on obtient que l’intervalle de confiance de <span class="math inline">\(X_0\theta\)</span> au niveau de confiance de <span class="math inline">\(1-\alpha\)</span> s’écrit :
<span class="math display">\[
IC_{1-\alpha}(X_0\theta) = \left[X_0\widehat{\theta}\pm t_{n-(p+1),1-\alpha/2} \times \sqrt{\widehat{\sigma}^2X_0(X&#39;X)^{-1}X&#39;_0}\right].
\]</span>
Dans l’exemple de la régression linéaire simple, voir Figure <a href="regression.html#fig:FigICreg">6.4</a>.</p>
<div class="figure"><span id="fig:FigICreg"></span>
<img src="Bookdown-poly_files/figure-html/FigICreg-1.png" alt="\label{fig:FigICreg} Intervalle de confiance pour la réponse moyenne d'un nouvel individu." width="672" />
<p class="caption">
Figure 6.4:  Intervalle de confiance pour la réponse moyenne d’un nouvel individu.
</p>
</div>
</div>
</div>
<div id="intervalle-de-prédiction" class="section level3">
<h3><span class="header-section-number">6.3.5</span> Intervalle de prédiction</h3>
<p>On veut prédire dans quel intervalle se trouvera le résultat d’un nouvel essai <span class="math inline">\((z^{(1)}_0, \cdots ,z_0^{(p)})\)</span>. On veut donc construire un intervalle de prédiction pour
une nouvelle observation <span class="math inline">\(Y_0\)</span>, correspondant à <span class="math inline">\(X_0=(1, z_0^{(1)}, z_0^{(2)} , \cdots, z_0^{(p)})\)</span> :
<span class="math display">\[
Y_0=X_0\theta+\varepsilon_0,
\]</span>
où <span class="math inline">\(\varepsilon_0\)</span> est indépendant des <span class="math inline">\(\varepsilon_i, \, 1 \leq i \leq n\)</span> et où <span class="math inline">\(\varepsilon_0 \sim \mathcal{N}(0,\sigma^2)\)</span>.
En reprenant la construction faite en section <a href="EstML.html#ICpredit">3.6</a>, on obtient que l’intervalle de prédiction de la variable <span class="math inline">\(Y\)</span> pour une nouvelle observation au point <span class="math inline">\(X_0\)</span> est défini par
<span class="math display">\[IC_{1-\alpha}(Y_0)=\left[X_0\widehat{\theta}\pm t_{n-(p+1),1-\alpha/2}\widehat\sigma \sqrt{1+X_0(X&#39;X)^{-1}X&#39;_0} \right].\]</span></p>
<p>Notez bien la différence entre <span class="math inline">\(IC_{1-\alpha}(Y_0)\)</span> et
<span class="math display">\[
    IC_{1-\alpha}(X_0\theta) = \left[X_0\widehat{\theta}\pm t_{n-(p+1),1-\alpha/2} \times \widehat{\sigma} \sqrt{X_0(X&#39;X)^{-1}X&#39;_0}\right].
\]</span>
Dans l’exemple de la régression linéaire simple, les intervalles de confiance <span class="math inline">\(IC_{1-\alpha}(X_0\theta)\)</span> et <span class="math inline">\(IC_{1-\alpha}(Y_0)\)</span> sont représentés sur la Figure <a href="regression.html#fig:FigICpred">6.5</a>.</p>
<div class="figure"><span id="fig:FigICpred"></span>
<img src="Bookdown-poly_files/figure-html/FigICpred-1.png" alt="\label{fig:FigICpred} Intervalle de confiance pour la réponse moyenne (en gris foncé)et intervalle de prédiction (pointillés rouge) pour un nouvel individu." width="672" />
<p class="caption">
Figure 6.5:  Intervalle de confiance pour la réponse moyenne (en gris foncé)et intervalle de prédiction (pointillés rouge) pour un nouvel individu.
</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-35" class="remark"><em>Remark</em>. </span>Pour faire de la prédiction à l’aide de ce modèle de régression linéaire, il est recommandé de n’utiliser ce modèle que dans le domaine couvert par les données. En effet, le phénomène étudié peut être linéaire dans le domaine observé et avoir un comportement différent dans un autre domaine.</p>
</div>
<!--
\begin{figure}[hbtp]
\centerline{\includegraphics[width=11cm]{Image/ChapRegLin/imagesRegression/plotICpredict-1.pdf}}
\caption{Intervalle de confiance de $X_0\theta$ en gris foncé et intervalle de prédiction de $Y_0$ en rouge pointillé}\label{FigICreg}
\end{figure}
-->
</div>
</div>
<div id="sélection-des-variables-explicatives" class="section level2">
<h2><span class="header-section-number">6.4</span> Sélection des variables explicatives</h2>
<p>En présence de <span class="math inline">\(p\)</span> variables explicatives dont on ignore celles qui sont réellement influentes, on doit rechercher un modèle d’explication de <span class="math inline">\(Y\)</span> à la fois performant (résidus les plus petits possibles) et économique (le moins possible de variables explicatives). Nous allons donc maintenant nous concentrer sur l’étude de la matrice <span class="math inline">\(X\)</span> autrement dit sur les variables explicatives elles-mêmes. Dans cette partie, nous allons voir comment choisir le modèle le plus en adéquation avec nos données et éliminer certaines variables peu explicatives pour gagner en interprétation. Ce problème de sélection de variables est en fait un problème de sélection de modèles.</p>
<div id="cadre-général-de-sélection-de-modèles" class="section level3">
<h3><span class="header-section-number">6.4.1</span> Cadre général de sélection de modèles</h3>
<p>Par soucis de simplicité, on présente ce problème dans le cadre de la régression linéaire multiple. Les outils présentés ici peuvent être bien sûr utilisés dans un cadre plus général (bien souvent sans travail supplémentaire).</p>
<p>On se donne une famille de modèles <span class="math inline">\(\mathcal{M}\)</span> représentant formellement une famille de sous-ensembles de <span class="math inline">\(\lbrace 1, \dots, p \rbrace\)</span>. Ce choix est fait a priori et peut ne pas être exhaustif. Par exemple, on peut considérer</p>
<ul>
<li>famille exhaustive : <span class="math inline">\(\mathcal{M}=\mathcal{P}(\lbrace 1,\dots, p \rbrace )\)</span> i.e. la famille de tous les sous-ensembles de <span class="math inline">\(\lbrace 1,\dots, p\rbrace\)</span>,</li>
<li>famille croissante : <span class="math inline">\(\mathcal{M}= \left( \lbrace 1,\dots, m \rbrace \right)_{m=1,\ldots ,p}\)</span>.</li>
</ul>
<p>Par la suite, pour <span class="math inline">\(m \in \mathcal{M}\)</span>, on notera <span class="math inline">\(|m|\)</span> le cardinal de <span class="math inline">\(m\)</span> et <span class="math inline">\(X_{(m)}\)</span> représente la matrice constituée des vecteurs <span class="math inline">\(\textbf{z}^{(j)}\)</span> pour <span class="math inline">\(j\in m\)</span>. On supposera également que pour tout <span class="math inline">\(m \in \mathcal{M}\)</span>, la matrice <span class="math inline">\(X_{(m)}\)</span> est régulière, i.e. de rang <span class="math inline">\(|m|+1\)</span>. Il faut noter que le “<span class="math inline">\(+1\)</span>” vient de la constante (de l’intercept) qui est supposée être présente systématiquement dans tous les modèles.</p>
<p><strong>Hypothèses sur le vrai modèle</strong> : On suppose qu’il existe <span class="math inline">\(m^{\star} \in \mathcal{M}\)</span>, inconnu, tel que le vrai modèle s’écrit :
<span class="math display">\[Y=\mu^{\star}+\varepsilon^{\star} = X_{(m^{\star})}\theta_{(m^{\star})} + \varepsilon^{\star}, \mbox{ avec } \varepsilon^{\star} \sim \mathcal{N}(0_{n},\sigma^{\star\, 2}I_n),\]</span>
le vecteur <span class="math inline">\(\theta_{(m^{\star})} \in \mathbb{R}^{|m^{\star}|+1}\)</span> ayant toutes ses coordonnées non nulles.</p>
<p><strong>Modèles d’analyse</strong> : Pour modéliser l’expérience et essayer d’identifier le vrai modèle on utilise la famille de modèles suivante, qui est en correspondance avec <span class="math inline">\(\mathcal{M}\)</span>, i.e.
<span class="math display">\[Y=\mu+\varepsilon = X_{(m)}\theta_{(m)} + \varepsilon, \mbox{ avec } \varepsilon \sim \mathcal{N}(0_{n},\sigma^2 I_n).\]</span></p>
<p>Pour préciser la modélisation, nous utiliserons le vocabulaire suivant :</p>
<div class="definition">
<p><span id="def:unlabeled-div-36" class="definition"><strong>Definition 6.1  </strong></span>On suppose que le modèle d’analyse est <span class="math inline">\(m \in \mathcal{M}\)</span>. Alors</p>
<ul>
<li>si <span class="math inline">\(m=m_p=\{1,\cdots,p\}\)</span>, on dit que le modèle est <strong>complet</strong>, i.e. que toutes les variables explicatives disponibles sont significatives</li>
<li>si <span class="math inline">\(m^{\star} \subset m\)</span> avec <span class="math inline">\(m \neq m^{\star}\)</span>, on dit que le modèle est <strong>sur-ajusté</strong></li>
<li>si <span class="math inline">\(|m \cap m^{\star}| &lt; |m^{\star}|\)</span>, on dit que le modèle est <strong>faux</strong></li>
<li>si <span class="math inline">\(m \subset m^{\star}\)</span> avec <span class="math inline">\(m \neq m^{\star}\)</span>, on dit que le modèle est <strong>sous-ajusté</strong>.</li>
</ul>
</div>
<p>Rappelons que chaque modèle correspond à un choix parmi l’ensemble des variables explicatives, et qu’il y a donc potentiellement des variables explicatives superflues. En cas de sur-ajustement, i.e. s’il y a des variables superflues, un <strong>modèle sur-ajusté</strong> est un modèle contenant toutes les variables du vrai modèle plus un certain nombre de variables superflues. Un <strong>faux modèle</strong> est typiquement un modèle où les variables du vrai modèle n’ont pas toutes été choisies et où certaines variables superflues ont pu être choisies. Un cas particulier est celui du <strong>sous-ajustement</strong> correspondant à un faux modèle ne contenant aucune variable superflue.</p>
<p>Nous allons voir dans la suite diverses approches permettant, non pas de retrouver <span class="math inline">\(m^{\star}\)</span>, mais au moins de s’en approcher. Ceci correspond aux bases de la <strong>sélection de modèle</strong>.</p>
</div>
<div id="quelques-critères-pour-sélectionner-un-modèle" class="section level3">
<h3><span class="header-section-number">6.4.2</span> Quelques critères pour sélectionner un modèle</h3>
<div id="les-coefficients-dajustement" class="section level4">
<h4><span class="header-section-number">6.4.2.1</span> Les coefficients d’ajustement</h4>
<p>Dans la situation où seul un petit nombre de régresseurs est en jeu, il existe déjà un certain nombre d’approches s’inspirant plus ou moins directement des outils étudiés précédemment. Pour “tester” la validité d’un sous-modèle <span class="math inline">\(m\)</span> par rapport à un modèle plus grand, il existe deux indices (ou coefficients) dont le calcul et l’interprétation sont assez immédiats.</p>
<p>Une première possibilité consiste à s’intéresser au coefficient de détermination :
<span class="math display">\[ 
R_m^2= \frac{SST-SSR(m)}{SST} = 1-\frac{\| Y- X_{(m)} \hat\theta_{(m)} \|^2}{\| Y - \overline Y \mathbb{1}_n\|^2}. 
\]</span>
Cet indice compare donc les valeurs prédites de <span class="math inline">\(Y\)</span> aux valeurs observées par l’intermédiaire de <span class="math inline">\(\| \widehat{Y}_{(m)} - Y \|^2\)</span>, le dénominateur correspondant en quelque sorte à une renormalisation. Plus le coefficient <span class="math inline">\(R^2_m\)</span> sera proche de <span class="math inline">\(1\)</span>, plus l’adéquation du modèle retenu aux données sera importante. Si on est amené à choisir entre deux modèles explicatifs, on est donc facilement tenté de retenir celui possédant le coefficient de détermination le plus important.</p>
<p>Il est cependant important d’apporter un petit bémol à ce type de raisonnement. En effet la maximisation de ce critère <span class="math inline">\(R_m^2\)</span> revenant à maximiser <span class="math inline">\(\|Y-\widehat{Y}_{(m)}\|^2\)</span>, il est clair que la quantité <span class="math inline">\(\|Y-\widehat{Y}_{(m)}\|^2=\|P_{[X_{(m)}]^{\perp}}Y\|^2\)</span> décroît pour une suite emboîtée de modèles. Par conséquent, la maximisation de <span class="math inline">\(R_m^2\)</span> conduit à coup sûr à choisir le modèle complet <span class="math inline">\(m_k\)</span>. Utiliser ce type de critère favorise ainsi la sélection de modèles très paramétrés. En revanche, pour des modèles de même cardinal <span class="math inline">\(|m|\)</span>, ce coefficient peut être utilisé pour choisir un modèle optimal.</p>
<p>Il est possible d’améliorer le coefficient <span class="math inline">\(R^2\)</span> pour permettre de sélectionner des modèles comportant un nombre différent de variables explicatives en définissant <strong>le coefficient de détermination ajusté</strong> <span class="math inline">\(\widetilde R^2_m\)</span>. Ce coefficient permet de tenir compte du nombre de régresseurs retenus et propose donc un compromis entre l’adéquation et le paramétrage du modèle. Cet indice est défini par :
<span class="math display">\[ \widetilde{R^2}_m = 1- \frac{n-1}{n-|m|-1}.\frac{SSR(m)}{SST}= 1- \frac{n-1}{n-|m|-1}.\frac{\| Y - X_m \hat\theta_{(m)} \|^2}{\| Y - \overline Y \mathbb{1}_n\|^2}.\]</span>
L’interprétation est similaire à celle du <span class="math inline">\(R^2\)</span>.</p>
</div>
<div id="les-stratégies-de-sélections-ascendantes-et-descendantes-par-le-test-de-fisher" class="section level4">
<h4><span class="header-section-number">6.4.2.2</span> Les stratégies de sélections ascendantes et descendantes par le test de Fisher</h4>
<p>Le coefficient d’ajustement peut être utilisé en présence d’un petit nombre de modèles. Dans le cas contraire, on peut utiliser une stratégie dite de <strong>régression descendante</strong> faisant appel au test de Fisher sur la présence d’un sous-modèle. La méthodologie est la suivante: on part du modèle utilisant tous les régresseurs possibles. À chaque étape, on calcule la statistique de Fisher correspondant au retrait de chacune des variables encore présentes. On retire alors la variable possédant la plus petite valeur, i.e. la plus grande <span class="math inline">\(p\)</span>-valeur. En fait à chaque étape, on retire la variable la moins significative au sens du test de Fisher. On réitère ensuite ce processus jusqu’à ce que toutes les statistiques soient supérieures à un seuil pré-déterminé, i.e. lorsque toutes les <span class="math inline">\(p\)</span>-valeurs sont toutes plus petites qu’un seuil fixé au préalable, par exemple <span class="math inline">\(5\%\)</span>. Attention, cette stratégie peut être extrêmement lourde à mettre en place suivant le nombre de variables en question (on peut aller jusqu’à <span class="math inline">\(|m|!\)</span> tests de Fisher).</p>
<div class="algobox">
<ul>
<li>Initialisation : on se donne un seuil <span class="math inline">\(s\)</span> et <span class="math inline">\(m_{[0]} =\{1,\ldots,p\}\)</span></li>
<li>Itération <span class="math inline">\(t\)</span> :
<ul>
<li>Etape 1 : Pour tout <span class="math inline">\(j\in m_{[t]}\)</span>, on calcule la p-valeur <span class="math inline">\(p_j\)</span> du test de Fisher de sous-modèle de
<span class="math display">\[
(M_0) : m_{[t]}\setminus \{j\} \textrm{ contre } (M_1) : m_{[t]}
\]</span></li>
<li>Etape 2 : <span class="math inline">\(\hat{\jmath} = \arg\underset{j\in m_{[t]}}{\max}\ p_j\)</span></li>
<li>Etape 3 :
<ul>
<li>Si <span class="math inline">\(p_{\hat{\jmath}} &gt;s\)</span>, <span class="math inline">\(m_{[t+1]} = m_{[t]} \setminus \{\hat{\jmath}\}\)</span> et on retourne à l’étape 1</li>
<li>Sinon stop.</li>
</ul></li>
</ul></li>
</ul>
</div>
<p>La sélection de modèle par régression ascendante reprend exactement les mêmes arguments, sauf que l’on part du modèle vide (sans régresseur, uniquement l’intercept) et l’on rajoute au fur et à mesure les variables les plus significatives (au sens du test de Fisher), jusqu’au dépassement par les p-valeurs d’un seuil fixé préalablement.</p>
</div>
<div id="le-critère-c_p-de-mallows" class="section level4">
<h4><span class="header-section-number">6.4.2.3</span> Le critère <span class="math inline">\(C_p\)</span> de Mallows</h4>
<p>Le risque quadratique est un critère usuel pour mesurer l’écart entre le vrai modèle <span class="math inline">\(m^\star\)</span> et un modèle d’analyse <span class="math inline">\(m\in\mathcal M\)</span>.</p>
<div class="definition">
<p><span id="def:unlabeled-div-37" class="definition"><strong>Definition 6.2  </strong></span>Soit <span class="math inline">\(m\in \mathcal{M}\)</span>. <strong>Le risque quadratique</strong> entre les modèles <span class="math inline">\(m\)</span> et <span class="math inline">\(m^{\star}\)</span> est défini par :
<span class="math display">\[\mathcal R(m,m^{\star})=\mathbb{E}\left[\left\|\mu^{\star}-\widehat{Y}_{(m)}\right\|^2\right]= \mathbb{E}\left[ \left\| X_{(m^{\star})} \theta_{(m^{\star})} - X_{(m)} \hat\theta_{(m)} \right\|^2 \right],\]</span>
où <span class="math inline">\(\mu^{\star}=X_{(m^{\star})} \theta_{(m^{\star})}\)</span> et $<em>{(m)}=X</em>{(m)} _{(m)} $.</p>
</div>
<p>Par la suite, pour tout <span class="math inline">\(m\in \mathcal{M}\)</span>, on définit
<span class="math inline">\(\mu_{(m)}^{\star}=P_{[X_{(m)}]} \mu^{\star}\)</span>,
le projeté orthogonal de <span class="math inline">\(\mu^{\star}\)</span> sur l’espace vectoriel <span class="math inline">\(Im(X_{(m)})\)</span>. Il est alors possible de calculer explicitement ce risque quadratique.</p>
<div class="proposition">
<p><span id="prp:risque" class="proposition"><strong>Proposition 6.2  </strong></span>Pour tout <span class="math inline">\(m\in \mathcal{M}\)</span>, on a :
<span class="math display" id="eq:risque">\[\begin{equation}
\mathcal R(m,m^{\star}) = \sigma^{\star\,2} (|m|+1) + \| \mu_{(m)}^{\star} - \mu^{\star} \|^2.
\tag{6.4}
\end{equation}\]</span></p>
</div>
<p>La preuve de la proposition <a href="regression.html#prp:risque">6.2</a> est donnée en annexe <a href="preuves-de-quelques-résultats-du-cours.html#annexe:risque">B.3</a>.</p>
<p>Afin de minimiser la distance entre <span class="math inline">\(m\)</span> et <span class="math inline">\(m^{\star}\)</span>, il y a donc un compromis à trouver. Si <span class="math inline">\(|m|\)</span> est petit, il en sera de même pour le terme de variance <span class="math inline">\(\sigma^{\star\,2} (|m|+1)\)</span>, au dépend du terme de biais <span class="math inline">\(\| \mu_{(m)}^{\star} - \mu^{\star} \|^2\)</span>. Au contraire, pour de grandes valeurs de <span class="math inline">\(|m|\)</span>, on peut espérer avoir un petit biais, mais au risque d’avoir une erreur plus importante, ce qui se traduit par une augmentation du terme <span class="math inline">\(\sigma^{\star\,2} (|m|+1)\)</span>. Ce compromis biais-variance est très classique dans ce cadre de sélection de modèle et se retrouve dans un grand nombre de thématiques.</p>
<div class="remark">
<p><span id="unlabeled-div-38" class="remark"><em>Remark</em>. </span>À partir du moment où <span class="math inline">\(m^{\star} \subset m\)</span>, on a <span class="math inline">\(\| \mu_{(m)}^{\star} - \mu^{\star} \|^2=0\)</span>, puisque <span class="math inline">\(\mu_{(m)}^{\star}\)</span> correspond au projeté orthogonal de <span class="math inline">\(\mu^{\star}\)</span> sur <span class="math inline">\([X_{(m)}]\)</span>.</p>
</div>
<p>La question qui se pose à présent est : comment approcher le modèle qui va minimiser le risque quadratique ? Clairement, trouver le meilleur modèle possible nécessite la connaissance de <span class="math inline">\(\mu^{\star}\)</span>… que l’on cherche justement à estimer ! L’idée proposée par Mallows <span class="citation">(Mallows <a href="#ref-Mallows" role="doc-biblioref">2000</a>)</span> consiste à estimer le risque quadratique à partir des données elles-mêmes et de prendre ensuite une décision à partir de cette estimation.
Le modèle <span class="math inline">\(\hat m_{CP}\)</span> retenu vérifie :
<span class="math display">\[\hat m_{CP} = \mathrm{arg} \min_{m\in \mathcal{M}} C_p(m)\]</span>
où le critère <span class="math inline">\(C_p\)</span> de Mallows est défini par
<span class="math display">\[C_p(m)= \| Y - \widehat{Y}_{(m)} \|^2 + 2|m| \sigma^2\]</span>
si la variance est connue. Dans le cas où la variance est inconnue, on utilisera l’estimateur <span class="math inline">\(\widehat{\sigma}^2 = \widehat{\sigma}^2_{(m_p)}\)</span> où <span class="math inline">\(m_p=\lbrace 1,\dots, p \rbrace\)</span> est le modèle prenant en compte tous les régresseurs. La construction de ce critère est présentée en annexe <a href="preuves-de-quelques-résultats-du-cours.html#annexe:Mallows">B.5</a>.</p>
</div>
<div id="les-critères-aic-et-bic" class="section level4">
<h4><span class="header-section-number">6.4.2.4</span> Les critères AIC et BIC</h4>
<p>Le critère <span class="math inline">\(C_p\)</span> de Mallows est basé sur une volonté de minimiser la distance entre <span class="math inline">\(m\)</span> et le vrai modèle au sens du risque quadratique. Les critères AIC (Akaike Information Criterion) (<span class="citation">Akaike (<a href="#ref-Akaike78" role="doc-biblioref">1978</a>)</span>,<span class="citation">Akaike (<a href="#ref-Akaike98" role="doc-biblioref">1998</a>)</span>) et BIC (Bayesian Information Criterion) <span class="citation">(Schwarz and others <a href="#ref-Schwarz" role="doc-biblioref">1978</a>)</span> sont eux construits pour minimiser la dissemblance de Kullback entre les 2 modèles.</p>
<p>À chaque modèle d’analyse qui, en général, est un faux modèle, on peut faire correspondre la mesure de probabilité de <span class="math inline">\(Y\)</span> en procédant comme si ce modèle d’analyse était réellement le vrai modèle. On fait donc correspondre la loi de <span class="math inline">\(X_{(m)}\hat\theta_{(m)} +\hat \varepsilon\)</span>. On peut ainsi mesurer l’écart entre la loi du vrai modèle (loi paramétrée par des paramètres inconnus) et la loi engendrée par le modèle d’analyse. Pour mesurer cet écart, un outil souvent utilisé est la dissemblance de Kullback-Leibler.</p>
<div class="definition">
<p><span id="def:unlabeled-div-39" class="definition"><strong>Definition 6.3  </strong></span>Soient <span class="math inline">\(\mathbb{P}\)</span> et <span class="math inline">\(\mathbb{P}^{\star}\)</span> deux mesures de probabilité dominées par une même mesure (dans notre cas la mesure de Lebesgue). La dissemblance de Kullback entre ces deux mesures est donnée par:
<span class="math display">\[KL(\mathbb{P}^\star,\mathbb{P})= \mathbb{E}_{\mathbb{P}^{\star}} \left[ \log \frac{d \mathbb{P}^{\star}}{d \mathbb{P}} \right].\]</span>
Si <span class="math inline">\(\displaystyle f =\frac{d\mathbb{P}}{d\nu}\)</span> et si <span class="math inline">\(\displaystyle f^{\star} =\frac{d\mathbb{P}^{\star}}{d\nu}\)</span>, alors
<span class="math inline">\(\displaystyle KL(\mathbb{P}^{\star},\P)=\left\{ \begin{array}{l} \int f^{\star}\log \frac{f^{\star}}{f}d\nu \mbox{ si } \mathbb{P}^{\star} \ll \P,\\ +\infty \mbox{ sinon.} \end{array} \right.\)</span></p>
</div>
Remarquons, en premier lieu la non symétrie de <span class="math inline">\(KL(.,.)\)</span>. C’est pour cette raison que l’on préfèrera parler de dissemblance plutôt que de distance. Cependant, cette dissemblance vérifie comme toute distance “classique” les propriétés suivantes :

<p>Ces propriétés peuvent être démontrées par des arguments de convexité.</p>
<p>Dans le cas où les erreurs sont gaussiennes, ce que nous avons supposé jusqu’à présent, il est possible d’obtenir une expression relativement simple de la dissemblance de Kullback.</p>
<div class="proposition">
<p><span id="prp:KL" class="proposition"><strong>Proposition 6.3  </strong></span>Soit <span class="math inline">\(m\in \mathcal{M}\)</span> fixé. On a alors :
<span class="math display">\[KL(m^{\star},m) = \frac{n}{2} \left[ \log\left( \frac{\sigma_{(m)}^2}{\sigma^{\star\, 2}} \right) + \frac{\sigma^{\star\,2}}{\sigma_{(m)}^2} - 1 \right] + \frac{1}{2\sigma_{(m)}^2} \|\mu^{\star}- \mu_{(m)}^\star\|^2,\]</span>
où <span class="math inline">\(KL(m^{\star},m)\)</span> désigne la dissemblance de Kullback entre les deux modèles <span class="math inline">\(m^{\star}\)</span> et <span class="math inline">\(m\)</span>.</p>
</div>
<p>La preuve de la proposition <a href="regression.html#prp:KL">6.3</a> en annexe <a href="preuves-de-quelques-résultats-du-cours.html#annexe:KL">B.4</a>.</p>
<div class="proposition">
<p><span id="prp:unlabeled-div-40" class="proposition"><strong>Proposition 6.4  </strong></span>Le critère AIC consiste à sélection le modèle vérifiant
<span class="math display">\[
\hat m = \mathrm{arg} \min_{m\in \mathcal{M}} \mbox{AIC}(m) 
\]</span>
avec
<span class="math display">\[
\mbox{AIC}(m) = - 2 \textrm{logvraisemblance au maximum de vraisemblance} + 2 D_m
\]</span>
où <span class="math inline">\(D_m\)</span> est la dimension du modèle <span class="math inline">\(m\)</span> (i.e le nombre de paramètres pour le modèle <span class="math inline">\(m\)</span>).</p>
</div>
<p>Nous n’allons pas ici présenter la construction théorique de ce critère AIC. Une preuve est disponible dans <span class="citation">(Azaïs and Bardet <a href="#ref-Azais" role="doc-biblioref">2005</a>)</span>.</p>
<p>Dans le cas gaussien, la logvraisemblance au maximum de vraisemblance vaut
<span class="math display">\[\ln\left[ (2\pi \tilde \sigma^2_{(m)})^{-n/2} \exp\left(-\frac{1}{2 \tilde \sigma^2_{(m)}} \|Y - \hat Y_{(m)}\|^2\right)\right]
= -\frac n 2 \ln(2\pi) -\frac n 2 \ln(\tilde \sigma^2_{(m)}) -\frac{n}{2}\]</span>
car <span class="math inline">\(\tilde \sigma^2_{(m)} = \frac 1 n \|Y - \hat Y_{(m)}\|^2.\)</span></p>
<p>Ainsi la sélection de modèle par le critère AIC peut se réécrire sous la forme
<span class="math display">\[
\hat m = \mathrm{arg} \min_{m\in \mathcal{M}}  n \ln(\tilde \sigma^2_{(m)})  + 2 (|m|+2).
\]</span></p>
<p>Ce type de critère fonctionne plutôt bien pour de petites collections de modèles. Des simulations numériques montrent toutefois que la qualité d’estimation a tendance à se dégrader lorsque <span class="math inline">\(m\)</span> augmente.</p>
<p>Afin de pallier ce problème, il est possible d’utiliser le critère <span class="math inline">\(AIC\)</span> corrigé :</p>
<p><span class="math display">\[
\mbox{AIC}_c(m) = n \ln \left( \tilde \sigma_{(m)}^2 \right)+ n  \frac{n+|m|-1}{n-|m| -3}.
\]</span></p>
<p>Le critère BIC (Bayesian Information Criterion) introduit en 1978 par Schwarz <span class="citation">(Schwarz and others <a href="#ref-Schwarz" role="doc-biblioref">1978</a>)</span>, est une extension de l’écriture générale du critère d’AIC et utilise le point de vue bayésien. On ne considère plus le paramètre inconnu <span class="math inline">\(\theta\)</span> comme un vecteur de <span class="math inline">\(\mathbb{R}^{p+1}\)</span> mais plutôt comme une variable aléatoire à valeurs dans <span class="math inline">\(\mathbb{R}^{p+1}\)</span>. Une loi a priori est alors placée sur le ‘paramètre’ à estimer. La démarche consiste ensuite à essayer d’exploiter cette information pour l’estimation. Ce type d’approche apporte en théorie plus de richesse puisque l’on étend l’éventail des solutions possibles.</p>
<p>Cette approche conduit au critère BIC défini par :
<span class="math display" id="eq:CritBIC">\[\begin{equation}
BIC(m)= n \log (\hat\sigma^2_{(m)} ) + \log n \times |m|.
\tag{6.5}
\end{equation}\]</span>
Le modèle correspondant <span class="math inline">\(\hat m_{BIC}\)</span> est obtenu en posant :
<span class="math display" id="eq:BIC">\[\begin{equation}
\hat m_{BIC} = \mathrm{arg} \min_{m \in \mathcal{M}} BIC(m).
\tag{6.6}
\end{equation}\]</span>
Nous ne nous étendrons pas sur les détails permettant d’arriver à la construction de ce critère.</p>
</div>
</div>
<div id="algorithmes-de-sélection-de-variables" class="section level3">
<h3><span class="header-section-number">6.4.3</span> Algorithmes de sélection de variables</h3>
<p>En pratique, une fois un critère de sélection de modèles choisi, la détermination du “meilleur” modèle par une recherche exhaustive est impossible en raison du nombre de modèles à explorer. On a donc recourt à des méthodes pas à pas :</p>
<ul>
<li><p><strong>Les méthodes descendantes</strong> :</p>
<p>On part du modèle en utilisant les <span class="math inline">\(p\)</span> variables explicatives et on cherche, à chaque étape de l’algorithme, la variable la plus pertinente à retirer selon le critère choisi. On itère ainsi l’algorithme jusqu’à atteindre l’ensemble vide. Parmi les ensembles de variables visités pendant l’algorithme, on retient le meilleur au vu du critère. Certains algorithmes s’arrêtent dès lors qu’un seuil donné est atteint.</p></li>
</ul>
<div class="algobox">
<ul>
<li>Initialisation : <span class="math inline">\(m_{[0]} = \{1,\ldots,p\}\)</span></li>
<li>Itération <span class="math inline">\(t\)</span> :
<ul>
<li>Etape 1 Pour tout <span class="math inline">\(j\in m_{[t]}\)</span>, on calcule <span class="math inline">\(c_j = \mbox{CRIT}(m_{[t]} \setminus \{j\} )\)</span>.</li>
<li>Etape 2] <span class="math inline">\(\hat{\jmath} = \arg\underset{j\in m_{[t]}}{\max}\ c_j\)</span></li>
<li>Etape 3 <span class="math inline">\(m_{[t+1]} = m_{[t]} \setminus \{\hat{\jmath}\}\)</span>
<ul>
<li>Si <span class="math inline">\(m_{[t+1]} \neq \emptyset\)</span>, on retourne à l’étape 1</li>
<li>Sinon stop.</li>
</ul></li>
</ul></li>
</ul>
</div>
<ul>
<li><strong>Les méthodes ascendantes</strong> :
On part de l’ensemble vide de variables et on cherche, à chaque étape de l’algorithme, la variable la plus pertinente à ajouter selon le critère choisi. On itère ainsi l’algorithme jusqu’à intégrer toutes les variables. Parmi les ensembles de variables visités pendant l’algorithme, on retient le meilleur au vu du critère. Certains algorithmes s’arrêtent dès lors qu’un seuil donné est atteint.</li>
</ul>
<div class="algobox">
<ul>
<li>Initialisation : <span class="math inline">\(m_{[0]} = \emptyset\)</span></li>
<li>Itération <span class="math inline">\(t\)</span> :
<ul>
<li>Etape 1 Pour tout <span class="math inline">\(j\in \{1,\ldots,p\} \setminus m_{[t]}\)</span>,\ on calcule <span class="math inline">\(c_j = \mbox{CRIT}(m_{[t]} \cup \{j\} )\)</span>.</li>
<li>Etape 2 <span class="math inline">\(\hat{\jmath} = \arg\underset{j}{\min}\ c_j\)</span></li>
<li>Etape 3 <span class="math inline">\(m_{[t+1]} = m_{[t]} \cup \{\hat{\jmath}\}\)</span>
<ul>
<li>Si <span class="math inline">\(m_{[t+1]} \neq \{1,\ldots,p\}\)</span>, on retourne à l’étape 1</li>
<li>Sinon stop.</li>
</ul></li>
</ul></li>
</ul>
</div>
<ul>
<li><p><strong>Les méthodes stepwise</strong> :</p>
<p>Partant d’un modèle donné, on opère une sélection d’une nouvelle variable (comme avec une méthode ascendante), puis on cherche si on peut éliminer une des variables du modèle (comme pour une méthode descendante) et ainsi de suite. Il faut définir pour une telle méthode un critère d’entrée et un critère de sortie.</p></li>
<li><p>On peut citer la méthode des “<span class="math inline">\(s\)</span> best subsets” ( ou “<span class="math inline">\(s\)</span> meilleurs sous-ensembles”) : On cherche de façon exhaustive parmi tous les sous-ensembles de <span class="math inline">\(s\)</span> variables, les <span class="math inline">\(s\)</span> meilleures, au sens du critère considéré.</p></li>
</ul>
</div>
<div id="illustration-sur-lexemple" class="section level3">
<h3><span class="header-section-number">6.4.4</span> Illustration sur l’exemple</h3>
<p>Dans cette section, nous allons illustrer sur notre exemple quelques stratégies de sélection de variables. Grâce à la fonction <code>regsubsets()</code>, on peut mettre en place une méthode ascendante, descendante ou séquentielle. On peut également choisir un critère parmi le Cp de Mallows, le <span class="math inline">\(R^2\)</span> ajusté et le critère BIC.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="regression.html#cb22-1"></a><span class="kw">library</span>(leaps)</span>
<span id="cb22-2"><a href="regression.html#cb22-2"></a>choixb&lt;-<span class="kw">regsubsets</span>(oxy<span class="op">~</span>.,<span class="dt">data=</span>fitness,<span class="dt">nbest=</span><span class="dv">1</span>,<span class="dt">nvmax=</span><span class="dv">10</span>,<span class="dt">method=</span><span class="st">&quot;backward&quot;</span>)</span>
<span id="cb22-3"><a href="regression.html#cb22-3"></a>choixf&lt;-<span class="kw">regsubsets</span>(oxy<span class="op">~</span>.,<span class="dt">data=</span>fitness,<span class="dt">nbest=</span><span class="dv">1</span>,<span class="dt">nvmax=</span><span class="dv">10</span>,<span class="dt">method=</span><span class="st">&quot;forward&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="regression.html#cb23-1"></a><span class="kw">plot</span>(choixb,<span class="dt">scale=</span><span class="st">&quot;Cp&quot;</span>)</span></code></pre></div>
<div class="figure"><span id="fig:FigselvarCp"></span>
<img src="Bookdown-poly_files/figure-html/FigselvarCp-1.png" alt="\label{fig:FigselvarCp} Sélection de variables avec le critère Cp de Mallows" width="672" />
<p class="caption">
Figure 6.6:  Sélection de variables avec le critère Cp de Mallows
</p>
</div>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="regression.html#cb24-1"></a><span class="kw">plot</span>(choixb,<span class="dt">scale=</span><span class="st">&quot;adjr2&quot;</span>)</span></code></pre></div>
<div class="figure"><span id="fig:FigselvarR2"></span>
<img src="Bookdown-poly_files/figure-html/FigselvarR2-1.png" alt="\label{fig:FigselvarR2} Sélection de variables avec le R2 ajusté." width="672" />
<p class="caption">
Figure 6.7:  Sélection de variables avec le R2 ajusté.
</p>
</div>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="regression.html#cb25-1"></a><span class="kw">plot</span>(choixb,<span class="dt">scale=</span><span class="st">&quot;bic&quot;</span>)</span></code></pre></div>
<div class="figure"><span id="fig:FigselvarBIC"></span>
<img src="Bookdown-poly_files/figure-html/FigselvarBIC-1.png" alt="\label{fig:FigselvarBIC} Sélection de variables avec le critère BIC" width="672" />
<p class="caption">
Figure 6.8:  Sélection de variables avec le critère BIC
</p>
</div>
<p>Par exemple, le Cp de Mallows et le critère BIC (Figures <a href="regression.html#fig:FigselvarCp">6.6</a> et <a href="regression.html#fig:FigselvarBIC">6.8</a>), on retient le modèle composé des variables <em>age</em>, <em>runtime</em>, <em>maxpulse</em> et <em>runpulse</em>. Avec le <span class="math inline">\(R^2\)</span> ajusté, on conserve en plus la variable <em>weight</em> (Figure <a href="regression.html#fig:FigselvarR2">6.7</a>).</p>
<p>On valide ensuite cette proposition de sous-modèle par un test de Fisher adapté :</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="regression.html#cb26-1"></a>reg.fin&lt;-<span class="kw">lm</span>(oxy<span class="op">~</span>age<span class="op">+</span>runtime<span class="op">+</span>maxpulse<span class="op">+</span>runpulse,<span class="dt">data=</span>fitness)</span>
<span id="cb26-2"><a href="regression.html#cb26-2"></a><span class="kw">anova</span>(reg.fin,reg.multi)</span></code></pre></div>
<pre><code>Analysis of Variance Table

Model 1: oxy ~ age + runtime + maxpulse + runpulse
Model 2: oxy ~ age + weight + runtime + rstpulse + runpulse + maxpulse
  Res.Df    RSS Df Sum of Sq    F Pr(&gt;F)
1     26 138.93                         
2     24 128.84  2    10.092 0.94 0.4045</code></pre>
<p>On peut aussi utiliser la fonction <code>stepAIC()</code> pour faire de la sélection de variable avec les critère AIC (<span class="math inline">\(k=2\)</span> par défaut) ou le critère BIC (mettre l’option <span class="math inline">\(k=log(nrow(Data))\)</span>).</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="regression.html#cb28-1"></a><span class="kw">library</span>(MASS)</span>
<span id="cb28-2"><a href="regression.html#cb28-2"></a>modselect_aic=<span class="kw">stepAIC</span>(reg.multi,<span class="dt">trace=</span>F,<span class="dt">direction=</span><span class="st">&quot;backward&quot;</span>)</span>
<span id="cb28-3"><a href="regression.html#cb28-3"></a>modselect_bic=<span class="kw">stepAIC</span>(reg.multi,<span class="dt">trace=</span>T,<span class="dt">direction=</span><span class="st">&quot;backward&quot;</span>,<span class="dt">k=</span><span class="kw">log</span>(<span class="kw">nrow</span>(fitness)))</span></code></pre></div>
<pre><code>Start:  AIC=68.2
oxy ~ age + weight + runtime + rstpulse + runpulse + maxpulse

           Df Sum of Sq    RSS    AIC
- rstpulse  1     0.571 129.41 64.903
- weight    1     9.911 138.75 67.063
&lt;none&gt;                  128.84 68.200
- maxpulse  1    26.491 155.33 70.562
- age       1    27.746 156.58 70.812
- runpulse  1    51.058 179.90 75.114
- runtime   1   250.822 379.66 98.268

Step:  AIC=64.9
oxy ~ age + weight + runtime + runpulse + maxpulse

           Df Sum of Sq    RSS     AIC
- weight    1      9.52 138.93  63.669
&lt;none&gt;                  129.41  64.903
- maxpulse  1     26.83 156.23  67.309
- age       1     27.37 156.78  67.417
- runpulse  1     52.60 182.00  72.041
- runtime   1    320.36 449.77 100.087

Step:  AIC=63.67
oxy ~ age + runtime + runpulse + maxpulse

           Df Sum of Sq    RSS    AIC
&lt;none&gt;                  138.93 63.669
- maxpulse  1     21.90 160.83 64.773
- age       1     22.84 161.77 64.954
- runpulse  1     46.90 185.83 69.252
- runtime   1    352.94 491.87 99.427</code></pre>
<p>Avec une procédure descendante et le critère AIC, on retient le même sous-modèle avec les variables explicatives <em>age</em>, <em>runtime</em>, <em>maxpulse</em> et <em>runpulse</em>.</p>
</div>
</div>
<div id="régression-linéaire-régularisée" class="section level2">
<h2><span class="header-section-number">6.5</span> Régression linéaire régularisée</h2>
<p>Quand on se retrouve avec un modèle singulier, <span class="math inline">\(\mbox{rg}(X) &lt; k\)</span>, la matrice <span class="math inline">\(X&#39;X\)</span> n’est plus inversible. Ce cas se présente quand</p>
<ul>
<li>le nombre de variables explicatives est supérieur au nombre d’observations (<span class="math inline">\(n&lt;p\)</span>)</li>
<li><span class="math inline">\(n&gt;p\)</span> mais des variables sont linéairement redondantes (la famille <span class="math inline">\(\{X^{(1)},\ldots,X^{(p)}\}\)</span> est liée)</li>
</ul>
<p>Dans cette situation, on a vu précédemment que l’estimateur des moindres carrés <span class="math inline">\(\widehat{\theta}\)</span> n’existe pas. La projection <span class="math inline">\(\widehat{Y}=P_{[X]}Y\)</span> de la réponse <span class="math inline">\(Y\)</span> sur <span class="math inline">\(Im(X)=[X]\)</span> n’a pas une décomposition unique sur les colonnes de X (le modèle est non identifiable, voir Chapitre <a href="singulier.html#singulier">5</a>). De plus, comme la matrice de variance-covariance de <span class="math inline">\(\widehat{\theta}\)</span> vaut <span class="math inline">\(\sigma^2 (X&#39;X)^{-1}\)</span>, la précision de l’estimateur <span class="math inline">\(\widehat{\theta}\)</span> diminue quand <span class="math inline">\(X&#39;X\)</span> se rapproche d’une matrice non inversible.</p>
<p>Du point de vue de la prédiction, si <span class="math inline">\(x^\star\)</span> est un nouveau vecteur de valeurs des variables explicatives, on sait que la qualité (au sens écart quadratique) de la prédiction <span class="math inline">\(\hat Y^\star\)</span> de la vraie réponse <span class="math inline">\(Y^\star\)</span> se décompose en le biais<span class="math inline">\(^2\)</span> + variance.
Donc pour améliorer la prédiction, on peut préférer une augmentation légère du biais pour avoir une diminution de la variance.</p>
<p>On va donc chercher dans ce contexte à utiliser des méthodes de régression dites régularisées pour pallier ces difficultés. Elles ont pour formalisme commun l’optimisation d’un critère de la forme
<span class="math display">\[\underset{\theta\in\mathbb{R}^k}{\mbox{argmin}}\ \|Y - X \theta\|^2 + \lambda\ \mbox{pen}(\theta)\]</span>
où <span class="math inline">\(\lambda&gt;0\)</span> est une quantité à choisir. Elles se distinguent par la forme de la fonction de pénalité <span class="math inline">\(\mbox{pen}(\theta)\)</span> qui fera intervenir le contrôle d’une norme de <span class="math inline">\(\theta\)</span>.</p>
<p>En pratique on commence par centrer et réduire les variables explicatives <span class="math inline">\(z^{(j)}\)</span> pour ne pas pénaliser ou favoriser un coefficient de <span class="math inline">\(\theta\)</span> car les pénalisations que nous allons considérer portent sur une norme de <span class="math inline">\(\theta\)</span>. Il est donc préférable que chaque coefficient soit affecté de façon “semblable”. La matrice des variables explicatives centrées-réduites est notée <span class="math inline">\(\tilde X\)</span>. De plus, l’intercept <span class="math inline">\(\theta_0\)</span> étant un coefficient qui a un rôle particulier assurant au modèle de se positionner autour du comportement moyen de <span class="math inline">\(Y\)</span>, il n’a pas à intervenir dans la contrainte sur la norme de <span class="math inline">\(\theta\)</span>. Aussi, on centre le vecteur réponse <span class="math inline">\(Y\)</span>, <span class="math inline">\(\tilde Y = Y - \bar Y \mathbb{1}_n\)</span>, et on peut potentiellement le réduire. A noter que le modèle est alors de la forme <span class="math inline">\(\tilde Y = \tilde X \theta + \varepsilon\)</span> avec <span class="math inline">\(\theta=(\theta_1,\ldots,\theta_p)&#39;\)</span> (donc <span class="math inline">\(k=p\)</span> et sans intercept).</p>
<p>Ainsi, après transformation initiale des données, nous allons ici nous intéresser à des méthodes de régression régularisées qui cherchent à minimiser le risque empirique régularisé (pour la perte quadratique) :<br />
<span class="math display">\[
\underset{\theta\in\mathbb{R}^k}{\mbox{argmin}}\ \left\{\|\tilde Y - \tilde X \theta\|^2 + \lambda \|\theta\|_q^q\right\}
\textrm{ où } \|\theta\|_q^q = \sum_{j=1}^p (\theta_j)^q.
\]</span>
On parle de régression ridge quand <span class="math inline">\(q=2\)</span>, de régression Lasso quand <span class="math inline">\(q=1\)</span>. Nous allons détailler ces deux méthodes et la régression Elasticnet qui combine les deux premières. Pour illustrer cette section, nous reprenons le jeu de données <em>fitness</em> auquel on a ajouté <span class="math inline">\(5\)</span> variables de bruit (simulation selon une loi <span class="math inline">\(\mathcal{N}(0,1)\)</span>).</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="regression.html#cb30-1"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb30-2"><a href="regression.html#cb30-2"></a>fitnessplus =<span class="st"> </span><span class="kw">cbind</span>(fitness<span class="op">$</span>oxy,fitness[,<span class="op">-</span><span class="dv">3</span>],<span class="kw">matrix</span>(<span class="kw">rnorm</span>(<span class="dt">n=</span><span class="kw">nrow</span>(fitness)<span class="op">*</span><span class="dv">5</span>,<span class="dv">0</span>,<span class="dv">1</span>),<span class="dt">nrow=</span><span class="kw">nrow</span>(fitness)))</span>
<span id="cb30-3"><a href="regression.html#cb30-3"></a><span class="kw">colnames</span>(fitnessplus)=<span class="kw">c</span>(<span class="kw">colnames</span>(fitness)[<span class="dv">3</span>],<span class="kw">colnames</span>(fitness[,<span class="op">-</span><span class="dv">3</span>]),<span class="kw">paste</span>(<span class="st">&quot;Z&quot;</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,<span class="dt">sep=</span><span class="st">&quot;&quot;</span>))</span>
<span id="cb30-4"><a href="regression.html#cb30-4"></a>y_var=fitnessplus[,<span class="dv">1</span>]</span>
<span id="cb30-5"><a href="regression.html#cb30-5"></a>x_var=fitnessplus[,<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb30-6"><a href="regression.html#cb30-6"></a>tildeY=<span class="kw">scale</span>(y_var,<span class="dt">center=</span>T,<span class="dt">scale=</span>T)</span>
<span id="cb30-7"><a href="regression.html#cb30-7"></a>tildeX=<span class="kw">scale</span>(x_var,<span class="dt">center=</span>T,<span class="dt">scale=</span>T)</span>
<span id="cb30-8"><a href="regression.html#cb30-8"></a><span class="kw">corrplot</span>(<span class="kw">cor</span>(fitnessplus),<span class="dt">method=</span><span class="st">&quot;ellipse&quot;</span>,<span class="dt">tl.cex=</span><span class="fl">0.5</span>)</span></code></pre></div>
<p><img src="Bookdown-poly_files/figure-html/unnamed-chunk-30-1.png" width="672" /></p>
<div id="régression-ridge" class="section level3">
<h3><span class="header-section-number">6.5.1</span> Régression ridge</h3>
<p>Dans le contexte présenté précédemment, la difficulté vient de l’inversibilité de <span class="math inline">\(\tilde X&#39; \tilde X\in\mathcal{M}_p(\mathbb{R})\)</span>. Cette matrice
<span class="math inline">\(\tilde X&#39; \tilde X\)</span> est une matrice semi-définie positive donc ses valeurs propres sont positives et on les ordonne <span class="math inline">\(\tau_1\geq\tau_2\geq\ldots\geq \tau_p\)</span>. Si <span class="math inline">\(\tilde X&#39; \tilde X\)</span> n’est pas inversible, c’est qu’au moins l’une de ses valeurs propres est nulle.</p>
<div class="proposition">
<p><span id="prp:propalglinmat" class="proposition"><strong>Proposition 6.5  </strong></span>Soit <span class="math inline">\(\lambda&gt;0\)</span>. Les matrices <span class="math inline">\(\tilde X&#39; \tilde X\)</span> et <span class="math inline">\(\tilde X&#39; \tilde X + \lambda I_p\)</span> ont les mêmes vecteurs propres mais leur valeurs propres sont <span class="math inline">\(\{\tau_j\}_{j\in[|1,p|]}\)</span> et <span class="math inline">\(\{\tau_j + \lambda\}_{j\in[|1,p|]}\)</span> respectivement. Ainsi,
<span class="math inline">\(det( \tilde X&#39; \tilde X + \lambda I_p) &gt; det( \tilde X&#39; \tilde X)\)</span>,
donc <span class="math inline">\(\tilde X&#39; \tilde X + \lambda I_p\)</span> a “plus de chance” d’être inversible que <span class="math inline">\(\tilde X&#39; \tilde X\)</span>.</p>
</div>
<p>En exploitant la Proposition <a href="regression.html#prp:propalglinmat">6.5</a>, l’idée consiste à remplacer <span class="math inline">\((\tilde X&#39; \tilde X)^{-1}\)</span> dans l’expression de l’estimateur des moindres carrés <span class="math inline">\(\widehat{\theta}\)</span> par <span class="math inline">\(( \tilde X&#39; \tilde X+ \lambda I_p)^{-1}\)</span>. Ainsi l’estimateur ridge est donné par</p>
<p><span class="math display">\[
\widehat{\theta}_{\scriptsize ridge} (\lambda) = (\tilde X&#39;\tilde X+ \lambda I_p)^{-1} \tilde X&#39; \tilde Y. 
\]</span></p>
<p>Cet estimateur ridge est solution du problème optimisation suivant
<span class="math display">\[
\widehat{\theta}_{\scriptsize ridge} (\lambda) \in \underset{\theta\in\mathbb{R}^p}{\mbox{argmin}}\ \|\tilde Y - \tilde X \theta\|_2^2 + \lambda \|\theta\|_2^2, 
\]</span>
qui peut être reformulé en le problème de minimisation sous contrainte suivant :
<span class="math display">\[\|\tilde Y - \tilde X \theta\|_2^2 \textrm{ sous la contrainte } \|\theta\|_2^2\leq r(\lambda)\]</span>
où <span class="math inline">\(r(.)\)</span> est bijective. La régression ridge conserve toutes les variables mais avec la contrainte <span class="math inline">\(\|\theta\|_2^2\leq r(\lambda)\)</span>, elle empêche les estimateurs de prendre de trop grandes valeurs et limite ainsi la variance des prédictions. On parle de “shrinkage” car on rétrécit l’étendue des valeurs possibles des paramètres estimés.</p>
<div class="proposition">
<p><span id="prp:unlabeled-div-41" class="proposition"><strong>Proposition 6.6  </strong></span>Soit l’estimateur ridge <span class="math inline">\(\widehat{\theta}_{\scriptsize ridge} (\lambda) = (\tilde X&#39;\tilde X+ \lambda I_p)^{-1} \tilde X&#39; \tilde Y\)</span>.
On a</p>
<ul>
<li><p><span class="math inline">\(\mathbb{E}[\widehat{\theta}_{\scriptsize ridge} (\lambda) ] = \theta - \lambda(\tilde X&#39;\tilde X+ \tau I_p)^{-1} \theta\)</span> donc il est biaisé.</p></li>
<li><p><span class="math inline">\(\mbox{Var}(\widehat{\theta}_{\scriptsize ridge} (\lambda) ) = \sigma^2 (\tilde X&#39;\tilde X+ \lambda I_p)^{-1} (\tilde X&#39;\tilde X) (\tilde X&#39;\tilde X+ \lambda I_p)^{-1} \leq \sigma^2 (\tilde X&#39;\tilde X)^{-1} = \mbox{Var}(\widehat{\theta})\)</span>.</p></li>
<li><p>Les valeurs ajustées pour <span class="math inline">\(Y\)</span> sont
<span class="math display">\[
\widehat{Y}_{\scriptsize ridge}(\lambda) = \tilde X \widehat{\theta}_{\scriptsize ridge} (\lambda) + \bar Y \mathbb{1}_n
\]</span></p></li>
<li><p>Quand <span class="math inline">\(\lambda \rightarrow +\infty\)</span>, <span class="math inline">\(\widehat{\theta}_{\scriptsize ridge} (\lambda)\rightarrow 0\)</span></p></li>
<li><p>Quand <span class="math inline">\(\lambda\rightarrow 0\)</span>, <span class="math inline">\(\widehat{\theta}_{\scriptsize ridge} (\lambda)\rightarrow \widehat{\theta}\)</span></p></li>
</ul>
</div>
<p>L’estimateur <span class="math inline">\(\widehat{\theta}_{\scriptsize ridge} (\lambda)\)</span> dépend du choix de <span class="math inline">\(\lambda\)</span> qui est un point délicat. C’est pratiquement impossible de pouvoir faire ce choix a priori. On peut tracer le <em>chemin de régularisation</em> de la régression ridge qui est l’ensemble des fonctions <span class="math inline">\(\tau\mapsto (\widehat{\theta}_{\scriptsize ridge} (\lambda))_j\)</span> pour <span class="math inline">\(j=1,\ldots,p\)</span> (voir Figure <a href="regression.html#fig:Figridge1">6.9</a>).<br />
On constate que le chemin de régularisation de la régression ridge est continu, ne permettant pas un ajustement aisé de <span class="math inline">\(\tau\)</span>. On peut également suivre les recommandations proposées dans la littérature, voir par exemple <span class="citation">(Hoerl, Kannard, and Baldwin <a href="#ref-Hoerl" role="doc-biblioref">1975</a>)</span>, <span class="citation">(Hoerl and Kennard <a href="#ref-hoerl76" role="doc-biblioref">1976</a>)</span>, <span class="citation">(Mallows <a href="#ref-Mallows" role="doc-biblioref">2000</a>)</span> et <span class="citation">(McDonald and Galarneau <a href="#ref-McDonald" role="doc-biblioref">1975</a>)</span>. En pratique, on passe par une procédure de validation croisée pour calibrer <span class="math inline">\(\tau\)</span> (Figure <a href="regression.html#fig:Figridge2">6.10</a>):</p>
<ul>
<li>On commence par séparer les données en un jeu d’apprentissage <span class="math inline">\((Y_a,X_a)\)</span> et un jeu de test <span class="math inline">\((Y_v,X_v)\)</span>.</li>
<li>On estime alors la régression ridge sur le jeu d’apprentissage pour chaque valeur de <span class="math inline">\(\tau\)</span> dans une grille de valeurs choisie et on prédit la réponse sur le jeu de test pour chaque valeur de <span class="math inline">\(\lambda\)</span>: <span class="math inline">\(\widehat{Y}_{{\scriptsize ridge}, v}(\lambda)\)</span>.</li>
<li>La qualité du modèle est alors obtenue en comparant les vraies données <span class="math inline">\(Y_v\)</span> et les valeurs prédites <span class="math inline">\(\widehat{Y}_{{\scriptsize ridge}, v}(\lambda)\)</span>. Par exemple, on peut utiliser le critère PRESS
<span class="math display">\[
PRESS(\lambda) = \|Y_v - \widehat{Y}_{{\scriptsize ridge}, v}(\lambda)\|^2.
\]</span></li>
<li>Finalement on choisit la valeur de <span class="math inline">\(\lambda\)</span> qui minime ce critère.</li>
</ul>
<p>Le principe de la validation croisée est de répéter plusieurs fois le découpage entre test et apprentissage et de considérer la moyenne des valeurs du critère pour chaque valeur de <span class="math inline">\(\lambda\)</span>.</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="regression.html#cb31-1"></a>lambda_seq &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dt">by =</span> <span class="fl">0.001</span>)</span>
<span id="cb31-2"><a href="regression.html#cb31-2"></a>fitridge &lt;-<span class="st"> </span><span class="kw">glmnet</span>(tildeX,tildeY, <span class="dt">alpha =</span> <span class="dv">0</span>, <span class="dt">lambda  =</span> lambda_seq,<span class="dt">family=</span><span class="kw">c</span>(<span class="st">&quot;gaussian&quot;</span>),<span class="dt">intercept=</span>F)</span>
<span id="cb31-3"><a href="regression.html#cb31-3"></a>df=<span class="kw">data.frame</span>(<span class="dt">tau =</span> <span class="kw">rep</span>(<span class="op">-</span><span class="kw">log</span>(fitridge<span class="op">$</span>lambda),<span class="kw">ncol</span>(tildeX)), <span class="dt">theta=</span><span class="kw">as.vector</span>(<span class="kw">t</span>(fitridge<span class="op">$</span>beta)),</span>
<span id="cb31-4"><a href="regression.html#cb31-4"></a>              <span class="dt">variable=</span><span class="kw">rep</span>(<span class="kw">colnames</span>(x_var),<span class="dt">each=</span><span class="kw">length</span>(fitridge<span class="op">$</span>lambda)))</span>
<span id="cb31-5"><a href="regression.html#cb31-5"></a>g1 =<span class="st"> </span><span class="kw">ggplot</span>(df,<span class="kw">aes</span>(<span class="dt">x=</span>tau,<span class="dt">y=</span>theta,<span class="dt">col=</span>variable))<span class="op">+</span></span>
<span id="cb31-6"><a href="regression.html#cb31-6"></a><span class="st">  </span><span class="kw">geom_line</span>()<span class="op">+</span></span>
<span id="cb31-7"><a href="regression.html#cb31-7"></a><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&#39;Estimator of theta&#39;</span>)<span class="op">+</span><span class="kw">xlab</span>(<span class="st">&quot;-log(lambda)&quot;</span>)<span class="op">+</span></span>
<span id="cb31-8"><a href="regression.html#cb31-8"></a><span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.title =</span> <span class="kw">element_text</span>(<span class="dt">size =</span> <span class="dv">5</span>),<span class="dt">legend.text =</span> <span class="kw">element_text</span>(<span class="dt">size =</span> <span class="dv">3</span>))</span>
<span id="cb31-9"><a href="regression.html#cb31-9"></a>g1</span></code></pre></div>
<div class="figure"><span id="fig:Figridge1"></span>
<img src="Bookdown-poly_files/figure-html/Figridge1-1.png" alt="\label{fig:Figridge1} Chemins de regularisation pour la régression ridge sur notre exemple." width="672" />
<p class="caption">
Figure 6.9:  Chemins de regularisation pour la régression ridge sur notre exemple.
</p>
</div>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="regression.html#cb32-1"></a>ridge_cv &lt;-<span class="st"> </span><span class="kw">cv.glmnet</span>(tildeX, tildeY, <span class="dt">alpha =</span> <span class="dv">0</span>, <span class="dt">lambda =</span> lambda_seq,<span class="dt">nfolds=</span><span class="dv">10</span>, <span class="dt">type.measure=</span><span class="kw">c</span>(<span class="st">&quot;mse&quot;</span>),<span class="dt">intercept=</span>F)</span>
<span id="cb32-2"><a href="regression.html#cb32-2"></a>best_lambda &lt;-<span class="st"> </span>ridge_cv<span class="op">$</span>lambda.min</span>
<span id="cb32-3"><a href="regression.html#cb32-3"></a>g1<span class="op">+</span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="op">-</span><span class="kw">log</span>(best_lambda),<span class="dt">linetype=</span><span class="st">&quot;dotted&quot;</span>,<span class="dt">color =</span> <span class="st">&quot;red&quot;</span>)<span class="op">+</span></span>
<span id="cb32-4"><a href="regression.html#cb32-4"></a><span class="st">  </span><span class="kw">xlim</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="op">-</span><span class="kw">log</span>(best_lambda)<span class="op">+</span><span class="dv">2</span>))</span></code></pre></div>
<div class="figure"><span id="fig:Figridge2"></span>
<img src="Bookdown-poly_files/figure-html/Figridge2-1.png" alt="\label{fig:Figridge2} Sélection de lambda par validation croisée pour la régression ridge sur notre exemple." width="672" />
<p class="caption">
Figure 6.10:  Sélection de lambda par validation croisée pour la régression ridge sur notre exemple.
</p>
</div>
</div>
<div id="régression-lasso" class="section level3">
<h3><span class="header-section-number">6.5.2</span> Régression Lasso</h3>
<p>L’idée de la régression LASSO (Least Absolute Selection and Shrinkage Operator) proposée par Tibshirani <span class="citation">(Tibshirani <a href="#ref-tibshirani96" role="doc-biblioref">1996</a>)</span> est d’essayer d’annuler des coefficients du vecteur <span class="math inline">\(\theta\)</span> afin d’avoir un estimateur parcimonieux (sparse en anglais). Cela induit une sélection de variables rendant le modèle plus interprétable et une matrice des variables explicatives avec de meilleures propriétés que <span class="math inline">\(X&#39;X\)</span>. Pour forcer à annuler des coordonnées de <span class="math inline">\(\theta\)</span>, on contraint la norme <span class="math inline">\(\ell_1\)</span> : <span class="math inline">\(\|\theta\|_1 = \sum_{j=1}^p |\theta_j|\)</span>. Comme pour la régression ridge, on commence par centrer-réduire les variables explicatives (<span class="math inline">\(\tilde X\)</span>) et au moins centrer le vecteur des réponses (<span class="math inline">\(\tilde Y\)</span>).</p>
<p>L’estimateur LASSO est défini pour <span class="math inline">\(\tau&gt;0\)</span> par
<span class="math display" id="eq:Lasso">\[\begin{equation}
\tag{6.7}
\widehat{\theta}_{{\scriptsize lasso}}(\lambda) \in \underset{\theta\in\mathbb{R}^p}{\mbox{argmin}}\ \|\tilde Y - \tilde X \theta\|_2^2 + \lambda \|\theta\|_1.
\end{equation}\]</span></p>
<p>Ce problème de minimisation est équivalent à minimiser <span class="math inline">\(\|\tilde Y - \tilde X \theta\|_2^2\)</span> sous la contrainte <span class="math inline">\(\|\theta\|_1\leq r(\lambda)\)</span> avec <span class="math inline">\(r(.)\)</span> bijective.
La solution du problème <a href="regression.html#eq:Lasso">(6.7)</a> peut ne pas être unique mais le vecteur des valeurs ajustées en résultant <span class="math inline">\(\tilde X \widehat{\theta}_{{\scriptsize lasso}}(\lambda)\)</span> est lui toujours unique.
L’estimateur LASSO a l’avantage d’avoir un certain nombre de coefficients nuls lorsque <span class="math inline">\(\lambda\)</span> est suffisamment grand. C’est un estimateur parcimonieux qui induit une sélection des variables.
Quand <span class="math inline">\(\lambda=0\)</span>, <span class="math inline">\(\widehat{\theta}_{{\scriptsize lasso}}(0) = \widehat{\theta}\)</span>; quand <span class="math inline">\(\lambda\rightarrow +\infty\)</span>, <span class="math inline">\(\widehat{\theta}_{{\scriptsize lasso}}(+\infty)=0\)</span>.
<!--
%Si $\tau \geq \|X'Y\|_\infty = \underset{j}{max}\ |(X'Y)_j|$ alors $\widehat{\theta}_{\mbox{\tiny lasso}}(\tau)=0$ est solution et aucune variable n'est intégrée au modèle. Dès que $\tau$ est sous le seuil, la variable $j_0$ telle que $\|X'Y\|_\infty=|(X'Y)_{j_0}|$ est ajoutée au modèle. Si les variables sont centrées réduites préalablement, cela correspond à la variable la plus corrélée à $Y$. 
--></p>
<p>Comme pour la régression ridge, le choix de <span class="math inline">\(\lambda\)</span> est délicat, il est impossible de faire ce choix a priori. On peut tracer le <em>chemin de régularisation</em> de la régression Lasso c’est-à-dire l’ensemble des fonctions <span class="math inline">\(\lambda\mapsto \widehat{\theta}_{{\scriptsize lasso}}(\lambda)_j\)</span> pour <span class="math inline">\(j=1,\ldots,p\)</span> (voir Figure <a href="regression.html#fig:Figlasso1">6.11</a>). Comme pour la régression ridge, on passe par une procédure de validation croisée pour stabiliser le choix de <span class="math inline">\(\lambda\)</span> (voir Figure <a href="#fig:Figlasso2"><strong>??</strong></a>).</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="regression.html#cb33-1"></a>lambda_seq=<span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="fl">0.001</span>)</span>
<span id="cb33-2"><a href="regression.html#cb33-2"></a>fitlasso &lt;-<span class="st"> </span><span class="kw">glmnet</span>(tildeX,tildeY, <span class="dt">alpha =</span> <span class="dv">1</span>, <span class="dt">lambda  =</span> lambda_seq,<span class="dt">family=</span><span class="kw">c</span>(<span class="st">&quot;gaussian&quot;</span>),<span class="dt">intercept=</span>F)</span>
<span id="cb33-3"><a href="regression.html#cb33-3"></a>lasso_cv &lt;-<span class="st"> </span><span class="kw">cv.glmnet</span>(tildeX, tildeY, <span class="dt">alpha =</span> <span class="dv">1</span>, <span class="dt">lambda =</span> lambda_seq,<span class="dt">nfolds=</span><span class="dv">10</span>,<span class="dt">type.measure=</span><span class="kw">c</span>(<span class="st">&quot;mse&quot;</span>),<span class="dt">intercept=</span>F)</span>
<span id="cb33-4"><a href="regression.html#cb33-4"></a>best_lambda &lt;-lasso_cv<span class="op">$</span>lambda.min  <span class="co"># red</span></span>
<span id="cb33-5"><a href="regression.html#cb33-5"></a>best_lambda<span class="fl">.1</span>se &lt;-<span class="st"> </span>lasso_cv<span class="op">$</span>lambda<span class="fl">.1</span>se <span class="co"># black</span></span></code></pre></div>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="regression.html#cb34-1"></a>df=<span class="kw">data.frame</span>(<span class="dt">tau =</span> <span class="kw">rep</span>(<span class="op">-</span><span class="kw">log</span>(fitlasso<span class="op">$</span>lambda),<span class="kw">ncol</span>(tildeX)), <span class="dt">theta=</span><span class="kw">as.vector</span>(<span class="kw">t</span>(fitlasso<span class="op">$</span>beta)),<span class="dt">variable=</span><span class="kw">rep</span>(<span class="kw">colnames</span>(x_var),<span class="dt">each=</span><span class="kw">length</span>(fitlasso<span class="op">$</span>lambda)))</span>
<span id="cb34-2"><a href="regression.html#cb34-2"></a>g3 =<span class="st"> </span><span class="kw">ggplot</span>(df,<span class="kw">aes</span>(<span class="dt">x=</span>tau,<span class="dt">y=</span>theta,<span class="dt">col=</span>variable))<span class="op">+</span></span>
<span id="cb34-3"><a href="regression.html#cb34-3"></a><span class="st">  </span><span class="kw">geom_line</span>()<span class="op">+</span></span>
<span id="cb34-4"><a href="regression.html#cb34-4"></a><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Estim. de theta&quot;</span>)<span class="op">+</span></span>
<span id="cb34-5"><a href="regression.html#cb34-5"></a><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;-log(lambda)&quot;</span>)</span>
<span id="cb34-6"><a href="regression.html#cb34-6"></a>g3 <span class="op">+</span><span class="st"> </span></span>
<span id="cb34-7"><a href="regression.html#cb34-7"></a><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="op">-</span><span class="kw">log</span>(best_lambda),<span class="dt">linetype=</span><span class="st">&quot;dotted&quot;</span>, <span class="dt">color =</span> <span class="st">&quot;red&quot;</span>)<span class="op">+</span></span>
<span id="cb34-8"><a href="regression.html#cb34-8"></a><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span>  <span class="op">-</span><span class="kw">log</span>(lasso_cv<span class="op">$</span>lambda<span class="fl">.1</span>se),<span class="dt">linetype=</span><span class="st">&quot;dotted&quot;</span>, <span class="dt">color =</span> <span class="st">&quot;black&quot;</span>)</span></code></pre></div>
<div class="figure"><span id="fig:Figlasso1"></span>
<img src="Bookdown-poly_files/figure-html/Figlasso1-1.png" alt="\label{fig:Figlasso1} Chemins de régularisation pour la régression Lasso sur notre exemple." width="672" />
<p class="caption">
Figure 6.11:  Chemins de régularisation pour la régression Lasso sur notre exemple.
</p>
</div>
<!--
\begin{figure}[htbp]
\centerline{\includegraphics[width=12cm]{Image/ChapRegLin/imagesRegression/lasso2-1.pdf}}
\caption{Sélection de $\tau$ par validation croisée pour la régression lasso sur notre exemple.}\label{Figlasso2}
\end{figure}
-->
</div>
<div id="régression-elastic-net" class="section level3">
<h3><span class="header-section-number">6.5.3</span> Régression Elastic-Net</h3>
<p>La régression Elastic-Net combine les avantages de la régression ridge et de la régression Lasso. En particulier, elle pallie le défaut de l’estimation Lasso lorsque les <span class="math inline">\(x^{(j)}\)</span> sont fortement corrélées.
L’estimateur Elastic-Net <span class="citation">(Zou and Hastie <a href="#ref-zou05" role="doc-biblioref">2005</a>)</span> est défini pour <span class="math inline">\(\lambda&gt;0\)</span> et <span class="math inline">\(\alpha&gt;0\)</span> par
<span class="math display">\[
\widehat{\theta}_{{\scriptsize net}}(\lambda,\alpha) \in \underset{\theta\in\mathbb{R}^p}{\mbox{argmin}}\ \|\tilde Y - \tilde X\theta\|_2^2 + \lambda \{\alpha \|\theta\|_1 + (1-\alpha) \|\theta\|_2^2\}
\]</span>
ce qui peut se reformuler en minimiser <span class="math inline">\(\|\tilde Y - \tilde X \theta\|_2^2\)</span> sous la contrainte <span class="math inline">\(\alpha \|\theta\|_1 + (1-\alpha) \|\theta\|_2^2 \leq r(\lambda)\)</span>.
Il faut alors utiliser des algorithmes d’optimisation pour déterminer <span class="math inline">\(\widehat{\theta}_{{\scriptsize net}}(\lambda,\alpha)\)</span> et la calibration des seuils <span class="math inline">\(\lambda\)</span> et <span class="math inline">\(\alpha\)</span> est souvent faite par validation croisée en pratique. La Figure <a href="regression.html#fig:Figcheminreg">6.12</a> illustre les différences sur les chemins de régularisation des trois méthodes.</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="regression.html#cb35-1"></a>fitEN &lt;-<span class="st"> </span><span class="kw">glmnet</span>(tildeX,tildeY, <span class="dt">alpha =</span> <span class="fl">0.3</span>, <span class="dt">lambda  =</span> lambda_seq,<span class="dt">family=</span><span class="kw">c</span>(<span class="st">&quot;gaussian&quot;</span>),<span class="dt">intercept=</span>F)</span>
<span id="cb35-2"><a href="regression.html#cb35-2"></a>EN_cv &lt;-<span class="st"> </span><span class="kw">cv.glmnet</span>(tildeX, tildeY, <span class="dt">alpha =</span> <span class="fl">0.3</span>, <span class="dt">lambda =</span> lambda_seq,<span class="dt">nfolds=</span><span class="dv">10</span>,<span class="dt">type.measure=</span><span class="kw">c</span>(<span class="st">&quot;mse&quot;</span>),<span class="dt">intercept=</span>F)</span></code></pre></div>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="regression.html#cb36-1"></a>best_lambda &lt;-EN_cv<span class="op">$</span>lambda.min</span>
<span id="cb36-2"><a href="regression.html#cb36-2"></a>df=<span class="kw">data.frame</span>(<span class="dt">tau =</span> <span class="kw">rep</span>(<span class="op">-</span><span class="kw">log</span>(fitEN<span class="op">$</span>lambda),<span class="kw">ncol</span>(tildeX)), <span class="dt">theta=</span><span class="kw">as.vector</span>(<span class="kw">t</span>(fitEN<span class="op">$</span>beta)),<span class="dt">variable=</span><span class="kw">rep</span>(<span class="kw">colnames</span>(x_var),<span class="dt">each=</span><span class="kw">length</span>(fitEN<span class="op">$</span>lambda)))</span>
<span id="cb36-3"><a href="regression.html#cb36-3"></a>g6 =<span class="st"> </span><span class="kw">ggplot</span>(df,<span class="kw">aes</span>(<span class="dt">x=</span>tau,<span class="dt">y=</span>theta,<span class="dt">col=</span>variable))<span class="op">+</span></span>
<span id="cb36-4"><a href="regression.html#cb36-4"></a><span class="st">  </span><span class="kw">geom_line</span>()<span class="op">+</span></span>
<span id="cb36-5"><a href="regression.html#cb36-5"></a><span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span><span class="op">-</span><span class="kw">log</span>(best_lambda),<span class="dt">linetype=</span><span class="st">&quot;dotted&quot;</span>,<span class="dt">color =</span> <span class="st">&quot;red&quot;</span>)<span class="op">+</span></span>
<span id="cb36-6"><a href="regression.html#cb36-6"></a><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Estim. theta&quot;</span>)<span class="op">+</span></span>
<span id="cb36-7"><a href="regression.html#cb36-7"></a><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;-log(lambda)&quot;</span>)</span>
<span id="cb36-8"><a href="regression.html#cb36-8"></a> g6</span></code></pre></div>
<div class="figure"><span id="fig:Figcheminreg"></span>
<img src="Bookdown-poly_files/figure-html/Figcheminreg-1.png" alt="\label{fig:Figcheminreg} Chemins de régularisation pour 3 variables du jeu de données pour la régression Lasso (alpha=1), régression ridge (alpha=0) et la régression Elastic Net (ici alpha=0.5)" width="672" />
<p class="caption">
Figure 6.12:  Chemins de régularisation pour 3 variables du jeu de données pour la régression Lasso (alpha=1), régression ridge (alpha=0) et la régression Elastic Net (ici alpha=0.5)
</p>
</div>
</div>
</div>
<div id="ValidationMod" class="section level2">
<h2><span class="header-section-number">6.6</span> Validation du modèle</h2>
<div id="contrôle-graphique-a-posteriori" class="section level3">
<h3><span class="header-section-number">6.6.1</span> Contrôle graphique a posteriori</h3>
<p>Une fois le modèle mis en oeuvre, on doit vérifier <em>a posteriori</em> le “bien-fondé statistique” de ce modèle du point de vue de la normalité des erreurs, l’adéquation de la valeur ajustée <span class="math inline">\(\widehat{Y_i}\)</span> à la valeur observée <span class="math inline">\(Y_i\)</span> et l’absence de données aberrantes. Il est alors indispensable de commencer par s’entourer de “protections” graphiques pour vérifier empiriquement les 4 postulats de base (au moins les hypothèses H1-H3, puisque l’hypothèse H4 n’est pas vraiment important dès que l’on dispose de suffisamment de données).</p>
<ul>
<li>En régression linéaire simple, la confrontation graphique entre le nuage de points <span class="math inline">\((z_i,y_i)\)</span> et la droite de régression de <span class="math inline">\(Y\)</span> par <span class="math inline">\(z\)</span> par moindres carrés ordinaires donne une information quasi exhaustive (cf Figure <a href="regression.html#fig:FigExRegSimple">6.2</a>).
Sur ce graphique, si nous voyons une courbure de la “vraie” courbe de régression de <span class="math inline">\(Y\)</span>, nous pouvons alors penser que le modèle est inadéquat et que l’hypothèse H1 n’est pas vérifiée.</li>
<li>Dans le cas de la régression multiple, ce type de graphique n’est pas utilisable car il y a plusieurs régresseurs. Les différentes hypothèses sont donc à vérifier sur les termes des erreurs <span class="math inline">\(\varepsilon_i\)</span> qui sont malheureusement inobservables. Nous utilisons alors leurs prédicteurs naturels, les résidus <span class="math inline">\(\widehat{\varepsilon_i}=Y_i-\widehat{Y_i}\)</span>.</li>
<li>Le graphe des <span class="math inline">\(n\)</span> points <span class="math inline">\((y_i, \widehat{y_i})\)</span> est également très informatif. Il suffit alors de vérifier si les points sont alignés selon la première bissectrice (cf. Figure <a href="regression.html#fig:FigYhatY">6.13</a>).</li>
</ul>
<!--
\begin{figure}[h]
\centerline{\includegraphics[width=12cm]{Image/ChapRegLin/imagesRegression/plotYhatY-1.pdf}}
\caption{Graphique des points $(y_i, \widehat{y_i})$ pour l'exemple en régression linéaire simple (à gauche) et multiple (à droite)}\label{FigYhatY}
\end{figure}
-->
<div class="figure"><span id="fig:FigYhatY"></span>
<img src="Bookdown-poly_files/figure-html/FigYhatY-1.png" alt="\label{FigYhatY} Graphique des réponses par rapport aux valeurs ajustées pour l'exemple en régression linéaire simple (à gauche) et multiple (à droite)" width="672" />
<p class="caption">
Figure 6.13:  Graphique des réponses par rapport aux valeurs ajustées pour l’exemple en régression linéaire simple (à gauche) et multiple (à droite)
</p>
</div>
<p>Voici maintenant plusieurs démarches permettant de s’assurer de la légitimité des conclusions, démarches à effectuer pour toute régression linéaire multiple.</p>
</div>
<div id="pour-vérifier-les-hypothèses-h1-et-h2-adéquation-et-homoscédasticité" class="section level3">
<h3><span class="header-section-number">6.6.2</span> Pour vérifier les hypothèses H1 et H2 : adéquation et homoscédasticité</h3>
<p>Le graphique le plus classique consiste à représenter les résidus <span class="math inline">\((\widehat{\varepsilon_i})_i\)</span> en fonction des valeurs prédites <span class="math inline">\((\widehat{Y_i})_i\)</span> (cf graphique en haut à gauche Figures <a href="regression.html#fig:Figresidussimple">6.14</a> et <a href="regression.html#fig:Figresidus">6.15</a>). Ce graphique doit être fait pratiquement systématiquement. Cela revient encore à tracer les coordonnées du vecteur <span class="math inline">\(P_{[X]^{\perp}}Y\)</span> en fonction de celles de <span class="math inline">\(P_{[X]}Y\)</span>. L’intérêt d’un tel graphique réside dans le fait que si les 4 hypothèses H1-H4 sont bien respectées, il y a indépendance entre ces 2 vecteurs qui sont centrés et gaussiens (d’après le théorème de Cochran). Cependant, à partir de ce graphe, nous ne pourrons nous apercevoir que de la possible déficience des hypothèses H1 et H2. Concrètement, si on ne voit rien de notable sur le graphique, i.e. si l’on observe un nuage de points centrés et alignés quelconque, c’est très bon signe : les résidus ne semblent alors n’avoir aucune propriété intéressante et c’est bien ce que l’on demande à l’erreur.</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="regression.html#cb37-1"></a><span class="kw">autoplot</span>(reg.simple, <span class="dt">label.size =</span> <span class="dv">2</span>)</span></code></pre></div>
<div class="figure"><span id="fig:Figresidussimple"></span>
<img src="Bookdown-poly_files/figure-html/Figresidussimple-1.png" alt="\label{Figresidussimple} Graphiques pour l'étude des résidus pour l'exemple en régression linéaire simple" width="672" />
<p class="caption">
Figure 6.14:  Graphiques pour l’étude des résidus pour l’exemple en régression linéaire simple
</p>
</div>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="regression.html#cb38-1"></a><span class="kw">autoplot</span>(reg.multi, <span class="dt">label.size =</span> <span class="dv">2</span>)</span></code></pre></div>
<div class="figure"><span id="fig:Figresidus"></span>
<img src="Bookdown-poly_files/figure-html/Figresidus-1.png" alt="\label{Figresidus} Graphiques pour l'étude des résidus pour l'exemple en régression linéaire multiple" width="672" />
<p class="caption">
Figure 6.15:  Graphiques pour l’étude des résidus pour l’exemple en régression linéaire multiple
</p>
</div>
<p>Voyons maintenant 2 types de graphes résidus/valeurs prédites “pathologiques” (Figure <a href="regression.html#fig:exforme">6.16</a>) :</p>
<ol style="list-style-type: decimal">
<li>Type 1 “forme banane” :<br />
Dans ce cas, on peut penser que le modèle n’est pas adapté aux données. En effet, il ne semble pas y avoir indépendance entre les <span class="math inline">\(\widehat{\varepsilon_i}\)</span> et les <span class="math inline">\(\widehat{Y_i}\)</span>, puisque, par exemple, les <span class="math inline">\(\widehat{\varepsilon_i}\)</span> ont tendance à décroître lorsque les <span class="math inline">\(\widehat{Y_i}\)</span> sont dans un certain intervalle et croissent. Il faut donc améliorer l’analyse du problème pour proposer d’autres régresseurs pertinents ou transformer les régresseurs <span class="math inline">\(z^{(j)}\)</span> par une fonction de type <span class="math inline">\((log,sin)\)</span>.</li>
<li>Type 2 “forme trompette”<br />
Dans ce cas la variance des résidus semble inhomogène, puisque les <span class="math inline">\(\widehat{\varepsilon_i}\)</span> ont une dispersion de plus en plus importante au fur et à mesure que les <span class="math inline">\(\widehat{Y_i}\)</span> croissent. Un changement de variable pour <span class="math inline">\(Y\)</span> pourrait être une solution envisageable afin de “rendre” constante la variance du bruit (cf pararaphe suivant).</li>
</ol>
<div class="figure"><span id="fig:exforme"></span>
<img src="image/exforme.png" alt="\label{exforme} Exemple du type forme banane (gauche) et forme trompette (droite)" width="80%" />
<p class="caption">
Figure 6.16:  Exemple du type forme banane (gauche) et forme trompette (droite)
</p>
</div>
<p>En cas de comportement inadéquat, les modifications possibles à apporter au modèle sont :</p>
<ul>
<li>On peut librement transformer les régresseurs <span class="math inline">\(z^{(1)}, \cdots, z^{(p)}\)</span> par toutes les transformations algébriques ou analytiques connues (fonctions puissances, exponentielles, logarithmiques…), pourvu que le nouveau modèle reste interprétable. Cela peut permettre d’améliorer l’adéquation du modèle ou diminuer son nombre de termes si on utilise ensuite une procédure de choix de modèles.<br />
</li>
<li>En revanche, on ne peut envisager de transformer <span class="math inline">\(Y\)</span> que si les graphiques font suspecter une hétéroscédasticité. Dans ce cas, cette transformation doit obéir à des règles précises basées sur la relation suspectée entre l’écart-type résiduel <span class="math inline">\(\sigma\)</span> et la réponse <span class="math inline">\(Y\)</span> : c’est ce que précise le tableau en figure <a href="regression.html#fig:TabStabVariance">6.17</a>.</li>
</ul>
<div class="figure"><span id="fig:TabStabVariance"></span>
<img src="image/TableauTransforme.png" alt="\label{TabStabVariance} Table de changements de variable pour la variable à expliquer afin de stabiliser la variance de Y" width="80%" />
<p class="caption">
Figure 6.17:  Table de changements de variable pour la variable à expliquer afin de stabiliser la variance de Y
</p>
</div>
<!--
\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Nature de la relation & Domaine pour $Y$ & Transformation \\
\hline  
$\sigma = (cste)Y^k, \, k \neq 1$ & $\R^{+*}$ & $Y \mapsto Y^{1-k}$ \\
$\sigma = (cste)\sqrt{Y}$ & $\R^{+*}$ & $Y \mapsto \sqrt{Y}$ \\
$\sigma = (cste)Y$ & $\R^{+*}$ & $Y \mapsto \log(Y)$ \\
$\sigma = (cste)Y^2$ & $\R^{+*}$ & $Y \mapsto Y^{-1}$ \\
$\sigma = (cste)\sqrt{Y(1-Y)}$ & $[0;1]$ & $Y \mapsto  \arcsin{\sqrt{Y}}$ \\
$\sigma = (cste)\sqrt{1-Y}Y^{-1}$ & $[0;1]$ & $Y \mapsto (1-Y)^{\frac{1}{2}}-\frac{1}{3}(1-Y)^{\frac{3}{2}}$ \\
$\sigma = (cste)(1-Y)^{-2}$ & $[-1;1]$ & $Y \mapsto \log(1+Y)-\log(1-Y)$ \\
\hline
\end{tabular}
\caption{Table des changements de variable pour la variable à expliquer afin de stabiliser la variance de $Y$}
\label{TabStabVariance}
\end{center}
\end{table}
-->
<!--%Souvent, ces situations correspondent à des modèles précis. Par exemple la 5ème transformation correspond le plus souvent à des données de comptage. Dans le cas où les effectifs observés sont faibles (de l'ordre de la dizaine), on aura plutôt intérêt à utiliser un modèle plus précis basé sur les lois binomiales. Il s'agit alors d'un modèle linéaire généralisé. D'ailleurs, toutes les situations issues d'une de ces transformations peut être traitées par modèle linéaire généralisé. Notons cependant que pour des grands échantillons, la transformation de $Y$ peut suffire à transformer le modèle en un modèle linéaire classique et est beaucoup plus simple à mettre en oeuvre. Par exemple, dans une étude bactériologique sur des désinfectants dentaires, on a mesuré le degré d'infection d'une racine dentaire en comptant les germes au microscope électronique. Sur les dents infectées, le nombre de germes est élevé et variable. L'écart-type est proportionnel à la racine carrée de la réponse. Une loi ayant cette propriété est la loi de Poisson, qui donne alors lieu à un modèle linéaire généralisé. Toutefois, si les décomptes sont en nombre important, travailler directement avec pour donnée la racine carrée du nombre de germes peut répondre tout aussi bien à la question.
-->
</div>
<div id="pour-vérifier-lhypothèse-h3-indépendance" class="section level3">
<h3><span class="header-section-number">6.6.3</span> Pour vérifier l’hypothèse H3 : indépendance</h3>
<p>Un graphe pertinent pour s’assurer de l’indépendance des erreurs entre elles est celui des résidus <span class="math inline">\(\widehat{\varepsilon_i}\)</span> en fonction de l’ordre des données (lorsque celui-ci a un sens, en particulier s’il représente le temps). Un tel graphique est potentiellement suspect si les résidus ont tendance à rester par paquets lorsqu’ils se trouvent d’un côté ou de l’autre de 0. On pourra confirmer ces doutes en effectuant un test de runs (cf <span class="citation">Draper and Smith (<a href="#ref-Draper" role="doc-biblioref">1998</a>)</span>, p. 157). Ce test est basé sur le nombre de runs, i.e. sur le nombre de paquets de résidus consécutifs de même signe.</p>
<p>Par ailleurs, si les erreurs sont corrélées suivant certaines conditions (par exemple si ce sont des processus ARMA), il est tout d’abord possible d’obtenir encore des résultats quant à l’estimation des paramètres. Mais il existe également des méthodes de correction telles que les estimations par moindres carrés généralisés ou pseudo-généralisés, cf <span class="citation">Guyon (<a href="#ref-Guyon" role="doc-biblioref">2001</a>)</span> ou d’autres.</p>
</div>
<div id="pour-vérifier-lhypothèse-h4-gaussianité" class="section level3">
<h3><span class="header-section-number">6.6.4</span> Pour vérifier l’hypothèse H4 : gaussianité</h3>
<p>%Nous l’avons déjà évoqué et nous le redirons : l’hypothèse H4 de gaussianité des données n’est importante que si l’on dispose de très peu de données (i.e. grossièrement, car tout dépend du modèle et du nombre de variables, moins de quelques dizaines). Dans ce cas,
Notamment pour que les tests de Fisher et de Student aient un sens, il peut être intéressant de vérifier si l’hypothèse de gaussianité est acceptable. Pour cela, nous déconseillons fortement les tests d’adéquation classiques de Kolmogorov-Smirnov, Cramer-Von Mises,…, du fait qu’on les appliquera sur les résidus <span class="math inline">\(\widehat{\varepsilon_i}\)</span>, qui ne sont (quasiment) jamais indépendants. On préfèrera se “contenter” d’une vérification graphique à partir du tracé d’une droite de Henri, dite encore graphique QQ-plot (cf graphiques en haut à droite Figures <a href="regression.html#fig:Figresidussimple">6.14</a> et <a href="regression.html#fig:Figresidus">6.15</a>. Celle-ci relie les points de <span class="math inline">\(\mathbb{R}^2\)</span> formés par les quantiles empiriques des résidus studentisés (i.e. le <span class="math inline">\(\widehat{\varepsilon_i}\)</span> divisés par leur écart-type empirique) en fonction des quantiles théoriques (pour les probabilités <span class="math inline">\(k/(n+1)\)</span> où <span class="math inline">\(k=1,\cdots,n\)</span>, <span class="math inline">\(n\)</span> étant le nombre de données) d’une loi normale centrée réduite. La loi de Student “ressemblant” fortement à une loi gaussienne dès que le paramètre dépasse la dizaine, si les erreurs <span class="math inline">\((\varepsilon_i)\)</span> sont gaussiennes, i.e. sous H4, alors la droite de Henri est une bissectrice du plan. Ce type de tracé permet surtout de voir si une loi à “queue de distribution lourde” ne pourrait pas être plus adéquate (dans ce cas, les points s’éloignent de la droite de Henri en ses extrémités).</p>
</div>
<div id="détection-de-données-aberrantes" class="section level3">
<h3><span class="header-section-number">6.6.5</span> Détection de données aberrantes</h3>
<p>Nous allons ici décrire deux méthodes permettant de détecter des données “aberrantes”.</p>
<div id="effet-levier-avec-la-matrice-h" class="section level4">
<h4><span class="header-section-number">6.6.5.1</span> Effet levier avec la matrice <span class="math inline">\(H\)</span></h4>
<p>On reprend la matrice chapeau <span class="math inline">\(H=X(X&#39;X)^{-1}X&#39;\)</span>. La prédiction pour le ième individu est donné par
<span class="math display">\[
\hat Y_i = (X \hat\theta)_i = (HY)_i = H_{ii} Y_i + \sum_{j\neq i} H_{ij} Y_{j}.
\]</span>
Si <span class="math inline">\(H_{ii}=1\)</span>, <span class="math inline">\(\hat Y_i\)</span> est entièrement déterminée par la ième observation alors que, si <span class="math inline">\(H_{ii}=0\)</span>, la ième observation n’a aucune influence sur <span class="math inline">\(\hat Y_i\)</span>. Ainsi, pour mesurer l’influence d’une observation sur sa propre estimation, on peut examiner le diagramme en batons des termes diagonaux de <span class="math inline">\(H\)</span> (cf Figure <a href="regression.html#fig:diagH">6.18</a>). En pratique, on déclare la ième observation comme <strong>levier</strong> si <span class="math inline">\(H_{ii}\)</span> dépasse <span class="math inline">\(2k/n\)</span> ou <span class="math inline">\(3k/n\)</span>.</p>
</div>
<div id="distances-de-cook" class="section level4">
<h4><span class="header-section-number">6.6.5.2</span> Distances de Cook</h4>
<p>Les points influents sont les points tels que, si on les retire de l’étude, l’estimation des coefficients du modèle sera fortement modifiée. La mesure la plus classique d’influence est la distance de Cook. C’est une distance entre le coefficient estimé avec toutes les observations et celui estimé en enlevant une observation. La distance de Cook pour la ième observation est définie par
<span class="math display">\[
DC_i = (\widehat{\theta}-\widehat{\theta}^{(-i)})&#39;T&#39;T(\widehat{\theta}-\widehat{\theta}^{(-i)})
\]</span></p>
<p>où <span class="math inline">\(T\)</span> est le vecteur des résidus studentisés et <span class="math inline">\(\widehat{\theta}^{(-i)}\)</span> l’EMV sans l’observation <span class="math inline">\(i\)</span>. On peut là encore tracer le diagramme en bâtons des <span class="math inline">\(DC_i\)</span> (cf Figure <a href="regression.html#fig:diagH">6.18</a>). Si une distance se révèle grande par rapport aux autres alors ce point sera considéré comme influent. Il faut alors chercher à comprendre pourquoi il est influent : il est levier, aberrant, les deux, ….</p>
<div class="figure"><span id="fig:diagH"></span>
<img src="Bookdown-poly_files/figure-html/diagH-1.png" alt="\label{diagH} Diagramme en bâtons des termes diagonaux de la matrice chapeau H (à gauche) et des distances de Cook (à droite)." width="672" />
<p class="caption">
Figure 6.18:  Diagramme en bâtons des termes diagonaux de la matrice chapeau H (à gauche) et des distances de Cook (à droite).
</p>
</div>
<!--
\begin{figure}[h]
\centerline{\includegraphics[width=10cm]{Image/ChapRegLin/imagesRegression/DiagHCook-1.pdf}}
\caption{Diagramme en bâtons des termes diagonaux de la matrice chapeau $H$ (à gauche) et des distances de Cook (à droite).}\label{Fig:diagH}
\end{figure}
-->
<!--
%Voici deux méthodes permettant de détecter des données aberrantes :
%\begin{enumerate}
%   \item L'effet levier par les éléments diagonaux de la matrice $H$. En effet, l'estimation des paramètres est très sensible à la présence de points extrêmes pouvant modifier de façon substantielle les résultats. Une observation est influente si l'élément diagonal de la matrice $H$ correspondant à cette observation est grand. L'effet levier apparait principalement pour des observations dont les valeurs prises par les variables explicatives sont éloignées de la moyenne.
%   \item Les mesures d'influence peuvent aussi permettre de déceler des points "atypiques" avec la distance de Cook $D_i$ pour l'individu $i$ : $(\widehat{\theta}-\widehat{\theta}^{(-i)})'T'T(\widehat{\theta}-\widehat{\theta}^{(-i)})$ où $T$ est le vecteur des résidus studentisés. Cette distance conclut à une influence de l'observation $i$ lorsque la valeur de $D_i$ dépasse 1.
%\end{enumerate}

%\subsection{Etude des colinéarités des variables explicatives}
%\subsubsection{Le problème}
%L'estimation des paramètres et de leurs variances nécessite le calcul de l'inverse de la matrice $(X'X)$. On dit que la matrice $(X'X)$ est mal conditionnée si son déterminant est proche de 0. La matrice $(X'X)^{-1}$ sera alors très grande. Cette situation se produit lorsque les variables explicatives sont très corrélées entre elles. On parle alors de \textbf{multi-colinéarité} et cela conduit à des estimations biaisées des paramètres avec des variances importantes.\\
%
%\underline{Remarque :} Dans le cas extrême où certaines variables explicatives sont des constantes ou des combinaisons linéaires des autres, alors les colonnes de la matrice $X$ sont des vecteurs linéairement liées et $X'X$ est singulière. %Dans ce cas, SAS élimine certaines variables en leur affectant d'autorité un coefficient nul. 
%
%\subsubsection{Les critères de diagnostic}
%Il s'agit de diagnostiquer ces situations critiques puis d'y remédier. Une des techniques (la plus simple, mais pas la plus rapide) est de détecter les fortes liaisons entre les variables explicatives en faisant la régression de chaque variable explicative sur les autres variables explicatives et en mesurant les liaisons $R^2$ de chacune de ces régressions. Un autre critère de diagnostic permet de détecter les problèmes de multi-colinéarité entre variables : le facteur d'inflation de la variance (VIF).\\
%
%Soient $\widetilde{X}$ la matrice des données observées centrées (c'est-à-dire la matrice $X$ privée de la colonne $\1_n$ et centrée) et $S$ la matrice diagonale contenant les écart-types empiriques des variables $z^{(j)}$. On peut définir la matrice $R$ des corrélations sous la forme :
%$$R=\frac{1}{n}S^{-1}\widetilde{X}'\widetilde{X}S^{-1}.$$
%On note $\widetilde{\theta}$ le vecteur des paramètres associés aux $p$ variables explicatives centrées. On peut montrer que $\widehat{\widetilde{\theta}}$ et $Var\left(\widehat{\widetilde{\theta}}\right)$ peuvent s'exprimer en fonction de $\widetilde{X}$ :
%$$\widehat{\widetilde{\theta}} = (\widetilde{X}'\widetilde{X})^{-1}\widetilde{X}'Y \mbox{ et } Var\left(\widehat{\widetilde{\theta}}\right) = \sigma^2(\widetilde{X}'\widetilde{X})^{-1}.$$
%On peut en déduire une nouvelle expression de $Var\left(\widehat{\widetilde{\theta}}\right)$ :
%$$Var\left(\widehat{\widetilde{\theta}}\right)=\frac{\sigma^2}{n}S^{-1}R^{-1}S^{-1}.$$
%Si on note $Var\left(\widehat{\widetilde{\theta_j}}\right)$ le $j$ème élément diagonal de la matrice de variance-covariance de $\widehat{\widetilde{\theta}}$ et $V_j$ le $j$ème élément diagonal de $R^{-1}$ alors 
%$$Var\left(\widehat{\widetilde{\theta_j}}\right) = \frac{\sigma^2}{n} \frac{V_j}{Var(X_j)}.$$
%$V_j$ est appelé \textit{facteur d'inflation de la variance} (VIF). Plus $V_j$ est grand, plus la variance de $\widehat{\theta_j}$ est grande. $V_j$ peut s'exprimer comme :
%$$V_j=\frac{1}{1-R_j^2}$$
%où $R_j^2$ est le coefficient de corrélation multiple obtenu en régressant $X_j$ sur les $p-1$ autres variables explicatives. On appelle \textit{tolérance} le coefficient $1-R^2_j$. Une tolérance et un facteur d'inflation de la variance qui tendent vers 1 signifient une absence de multi-colinéarité entres les variables explicatives. En revanche, si la tolérance tend vers 0 et le facteur d'inflation de la variance vers $+\infty$, alors on détecte un problème de multi-colinéarité entre les variables explicatives.
%
%\subsubsection{Une première solution}
%Une façon d'éviter ce problème d'inversibilité  et donc de réduire les inconvénients de variables explicatives fortement corrélées est de remplacer $\widehat{\theta}$ par 
%$$\widetilde{\theta}=(X'X+cI_p)^{-1}X'Y,$$
%où $c$ est un réel choisi par l'utilisateur de la façon suivante. $\widetilde{\theta}$ n'est plus un estimateur sans biais de $\theta$ mais il est de variance plus petite que celle de $\widehat{\theta}$. On calcule alors l'erreur quadratique de $\widetilde{\theta}$ ($variance + biais^2$) et on choisit $c$ de façon à ce que l'erreur quadratique de $\widetilde{\theta}$ soit minimum.
-->
</div>
</div>
</div>
<div id="en-résumé-4" class="section level2">
<h2><span class="header-section-number">6.7</span> En résumé</h2>
<div class="summarybox">
<ul>
<li>Savoir écrire un modèle de régression linéaire (pour chaque individu et matriciellement)</li>
<li>Savoir déterminer les estimateurs de la régression linéaire et leur loi.</li>
<li>Savoir construire en régression linéaire un intervalle de confiance pour un paramètre et un intervalle de prédiction</li>
<li>Savoir construire un test de nullité d’un (ou des) paramètre(s)</li>
<li>Comprendre la problématique de la sélection de variables, savoir proposer des stratégies en pratique et savoir interpréter mes sorties de R</li>
<li>Comprendre le principe des méthodes de régression régularisées et savoir lire les sorties de R associées.</li>
<li>Savoir interpréter les graphiques de contrôle pour valider les hypothèses du modèle linéaire.</li>
</ul>
</div>
</div>
<div id="quelques-codes-python" class="section level2">
<h2><span class="header-section-number">6.8</span> Quelques codes python</h2>
<p>Dans cette partie, on donne quelques lignes de codes en python pour reproduire (partiellement) l’étude faite précédemment en R.</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb39-1"><a href="regression.html#cb39-1"></a><span class="im">import</span> statsmodels.api <span class="im">as</span> sm</span>
<span id="cb39-2"><a href="regression.html#cb39-2"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb39-3"><a href="regression.html#cb39-3"></a><span class="co"># on passe par reticulate pour récupérer le jeu de données ici</span></span>
<span id="cb39-4"><a href="regression.html#cb39-4"></a>fitnesspy<span class="op">=</span>r.fitness</span></code></pre></div>
<div class="sourceCode" id="cb40"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb40-1"><a href="regression.html#cb40-1"></a><span class="co"># régression linéaire simple</span></span>
<span id="cb40-2"><a href="regression.html#cb40-2"></a>x <span class="op">=</span> np.array(fitnesspy.runtime).reshape((<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb40-3"><a href="regression.html#cb40-3"></a>x <span class="op">=</span> sm.add_constant(x)</span>
<span id="cb40-4"><a href="regression.html#cb40-4"></a>y <span class="op">=</span> np.array(fitnesspy.oxy)</span>
<span id="cb40-5"><a href="regression.html#cb40-5"></a>regsimplepy <span class="op">=</span> sm.OLS(y, x)</span>
<span id="cb40-6"><a href="regression.html#cb40-6"></a>resultsregsimple <span class="op">=</span> regsimplepy.fit()</span>
<span id="cb40-7"><a href="regression.html#cb40-7"></a><span class="bu">print</span>(resultsregsimple.summary())</span></code></pre></div>
<pre><code>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.743
Model:                            OLS   Adj. R-squared:                  0.735
Method:                 Least Squares   F-statistic:                     84.01
Date:                Jeu, 28 oct 2021   Prob (F-statistic):           4.59e-10
Time:                        11:08:52   Log-Likelihood:                -74.254
No. Observations:                  31   AIC:                             152.5
Df Residuals:                      29   BIC:                             155.4
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
const         82.4218      3.855     21.379      0.000      74.537      90.307
x1            -3.3106      0.361     -9.166      0.000      -4.049      -2.572
==============================================================================
Omnibus:                        0.032   Durbin-Watson:                   1.924
Prob(Omnibus):                  0.984   Jarque-Bera (JB):                0.072
Skew:                           0.028   Prob(JB):                        0.964
Kurtosis:                       2.770   Cond. No.                         84.2
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.</code></pre>
<div class="sourceCode" id="cb42"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb42-1"><a href="regression.html#cb42-1"></a><span class="co"># régression linéaire multiple</span></span>
<span id="cb42-2"><a href="regression.html#cb42-2"></a>list_var<span class="op">=</span>fitnesspy.columns.drop(<span class="st">&quot;oxy&quot;</span>)</span>
<span id="cb42-3"><a href="regression.html#cb42-3"></a>X<span class="op">=</span>fitnesspy[list_var]</span>
<span id="cb42-4"><a href="regression.html#cb42-4"></a>X <span class="op">=</span> sm.add_constant(X)</span>
<span id="cb42-5"><a href="regression.html#cb42-5"></a>y<span class="op">=</span>np.array(fitnesspy.oxy)</span>
<span id="cb42-6"><a href="regression.html#cb42-6"></a></span>
<span id="cb42-7"><a href="regression.html#cb42-7"></a>regmultipy <span class="op">=</span> sm.OLS(y, X)</span>
<span id="cb42-8"><a href="regression.html#cb42-8"></a>resultsregmulti <span class="op">=</span> regmultipy.fit()</span>
<span id="cb42-9"><a href="regression.html#cb42-9"></a><span class="bu">print</span>(resultsregmulti.summary())</span></code></pre></div>
<pre><code>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.849
Model:                            OLS   Adj. R-squared:                  0.811
Method:                 Least Squares   F-statistic:                     22.43
Date:                Jeu, 28 oct 2021   Prob (F-statistic):           9.72e-09
Time:                        11:08:52   Log-Likelihood:                -66.068
No. Observations:                  31   AIC:                             146.1
Df Residuals:                      24   BIC:                             156.2
Df Model:                           6                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
const        102.9345     12.403      8.299      0.000      77.335     128.534
age           -0.2270      0.100     -2.273      0.032      -0.433      -0.021
weight        -0.0742      0.055     -1.359      0.187      -0.187       0.038
runtime       -2.6287      0.385     -6.835      0.000      -3.422      -1.835
rstpulse      -0.0215      0.066     -0.326      0.747      -0.158       0.115
runpulse      -0.3696      0.120     -3.084      0.005      -0.617      -0.122
maxpulse       0.3032      0.136      2.221      0.036       0.022       0.585
==============================================================================
Omnibus:                        2.609   Durbin-Watson:                   1.711
Prob(Omnibus):                  0.271   Jarque-Bera (JB):                1.465
Skew:                          -0.069   Prob(JB):                        0.481
Kurtosis:                       4.056   Cond. No.                     7.91e+03
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 7.91e+03. This might indicate that there are
strong multicollinearity or other numerical problems.</code></pre>
<div class="sourceCode" id="cb44"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb44-1"><a href="regression.html#cb44-1"></a><span class="co"># Pour SSR, SSE, SST</span></span>
<span id="cb44-2"><a href="regression.html#cb44-2"></a><span class="bu">print</span>(<span class="st">&#39;SSR:&#39;</span>, resultsregsimple.ssr)</span></code></pre></div>
<pre><code>SSR: 218.4814449878273</code></pre>
<div class="sourceCode" id="cb46"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb46-1"><a href="regression.html#cb46-1"></a><span class="bu">print</span>(<span class="st">&#39;SSE:&#39;</span>, resultsregsimple.ess)</span></code></pre></div>
<pre><code>SSE: 632.9000998508823</code></pre>
<div class="sourceCode" id="cb48"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb48-1"><a href="regression.html#cb48-1"></a><span class="bu">print</span>(<span class="st">&#39;SST:&#39;</span>, resultsregsimple.centered_tss)</span></code></pre></div>
<pre><code>SST: 851.3815448387096</code></pre>
<div class="sourceCode" id="cb50"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb50-1"><a href="regression.html#cb50-1"></a><span class="co"># Coefficient de détermination R2</span></span>
<span id="cb50-2"><a href="regression.html#cb50-2"></a><span class="bu">print</span>(<span class="st">&#39;coefficient of determination:&#39;</span>,<span class="bu">round</span>(np.<span class="bu">float</span>(resultsregsimple.rsquared),<span class="dv">3</span>))</span></code></pre></div>
<pre><code>coefficient of determination: 0.743</code></pre>
<div class="sourceCode" id="cb52"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb52-1"><a href="regression.html#cb52-1"></a><span class="bu">print</span>(<span class="st">&#39;coefficient of determination:&#39;</span>, <span class="bu">round</span>(np.<span class="bu">float</span>(resultsregmulti.rsquared),<span class="dv">3</span>))</span></code></pre></div>
<pre><code>coefficient of determination: 0.849</code></pre>
<div class="sourceCode" id="cb54"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb54-1"><a href="regression.html#cb54-1"></a><span class="co"># Exemple de tests de Fisher de sous-modèle</span></span>
<span id="cb54-2"><a href="regression.html#cb54-2"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb54-3"><a href="regression.html#cb54-3"></a><span class="im">from</span> statsmodels.stats.anova <span class="im">import</span> anova_lm</span>
<span id="cb54-4"><a href="regression.html#cb54-4"></a>resregfin <span class="op">=</span> ols(<span class="st">&#39;oxy~age + runtime+runpulse+maxpulse&#39;</span>, data<span class="op">=</span>fitnesspy).fit()</span>
<span id="cb54-5"><a href="regression.html#cb54-5"></a>anovaResults <span class="op">=</span> anova_lm(resregfin,resultsregmulti)</span>
<span id="cb54-6"><a href="regression.html#cb54-6"></a><span class="bu">print</span>(anovaResults)</span></code></pre></div>
<pre><code>   df_resid         ssr  df_diff   ss_diff         F   Pr(&gt;F)
0      26.0  138.930018      0.0       NaN       NaN      NaN
1      24.0  128.837938      2.0  10.09208  0.939979  0.40455</code></pre>
<div class="sourceCode" id="cb56"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb56-1"><a href="regression.html#cb56-1"></a><span class="co"># Intervalles de confiances</span></span>
<span id="cb56-2"><a href="regression.html#cb56-2"></a>resultsregsimple.conf_int(<span class="fl">0.1</span>)</span></code></pre></div>
<pre><code>array([[75.87112183, 88.97242353],
       [-3.9242713 , -2.69683942]])</code></pre>
<div class="sourceCode" id="cb58"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb58-1"><a href="regression.html#cb58-1"></a>resultsregmulti.conf_int(<span class="fl">0.05</span>)</span></code></pre></div>
<pre><code>                  0           1
const     77.335413  128.533546
age       -0.433028   -0.020919
weight    -0.186852    0.038497
runtime   -3.422350   -1.834955
rstpulse  -0.157863    0.114796
runpulse  -0.616992   -0.122263
maxpulse   0.021505    0.584929</code></pre>
<div class="sourceCode" id="cb60"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb60-1"><a href="regression.html#cb60-1"></a><span class="co"># Regression ridge</span></span>
<span id="cb60-2"><a href="regression.html#cb60-2"></a>Xtildepy<span class="op">=</span>r.tildeX</span>
<span id="cb60-3"><a href="regression.html#cb60-3"></a>ytildepy<span class="op">=</span>r.tildeY</span>
<span id="cb60-4"><a href="regression.html#cb60-4"></a></span>
<span id="cb60-5"><a href="regression.html#cb60-5"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> Ridge</span>
<span id="cb60-6"><a href="regression.html#cb60-6"></a>lambdas<span class="op">=</span>np.arange(<span class="fl">0.001</span>,<span class="fl">1.01</span>,<span class="fl">0.01</span>)</span>
<span id="cb60-7"><a href="regression.html#cb60-7"></a>p<span class="op">=</span>Xtildepy.shape[<span class="dv">1</span>]</span>
<span id="cb60-8"><a href="regression.html#cb60-8"></a>coefs<span class="op">=</span>np.empty((<span class="dv">0</span>,p),<span class="bu">float</span>)</span>
<span id="cb60-9"><a href="regression.html#cb60-9"></a><span class="cf">for</span> a <span class="kw">in</span> lambdas:</span>
<span id="cb60-10"><a href="regression.html#cb60-10"></a>  ridge <span class="op">=</span> Ridge(alpha<span class="op">=</span>a, fit_intercept<span class="op">=</span><span class="va">False</span>)<span class="op">;</span></span>
<span id="cb60-11"><a href="regression.html#cb60-11"></a>  ridge.fit(Xtildepy, ytildepy)<span class="op">;</span></span>
<span id="cb60-12"><a href="regression.html#cb60-12"></a>  coefs<span class="op">=</span>np.append(coefs,ridge.coef_,axis<span class="op">=</span><span class="dv">0</span>)<span class="op">;</span></span></code></pre></div>
<div class="sourceCode" id="cb61"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb61-1"><a href="regression.html#cb61-1"></a><span class="co"># Regression Lasso</span></span>
<span id="cb61-2"><a href="regression.html#cb61-2"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> lasso_path</span>
<span id="cb61-3"><a href="regression.html#cb61-3"></a>lambdas<span class="op">=</span>np.arange(<span class="fl">0.01</span>,<span class="fl">1.01</span>,<span class="fl">0.01</span>)</span>
<span id="cb61-4"><a href="regression.html#cb61-4"></a>alphas_lasso, coefs_lasso, _ <span class="op">=</span> lasso_path(Xtildepy, ytildepy, alphas<span class="op">=</span>lambdas, fit_intercept<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb61-5"><a href="regression.html#cb61-5"></a></span>
<span id="cb61-6"><a href="regression.html#cb61-6"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb61-7"><a href="regression.html#cb61-7"></a>plt.figure()<span class="op">;</span></span>
<span id="cb61-8"><a href="regression.html#cb61-8"></a>neg_log_alphas_lasso <span class="op">=</span> <span class="op">-</span>np.log(alphas_lasso)</span>
<span id="cb61-9"><a href="regression.html#cb61-9"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(coefs_lasso.shape[<span class="dv">1</span>]):</span>
<span id="cb61-10"><a href="regression.html#cb61-10"></a>  plt.plot(neg_log_alphas_lasso, coefs_lasso[<span class="dv">0</span>,i,:],<span class="st">&#39;-&#39;</span>)<span class="op">;</span></span>
<span id="cb61-11"><a href="regression.html#cb61-11"></a>plt.xlabel(<span class="st">&#39;-log(lambda)&#39;</span>)<span class="op">;</span></span>
<span id="cb61-12"><a href="regression.html#cb61-12"></a>plt.ylabel(<span class="st">&#39;weights&#39;</span>)<span class="op">;</span></span>
<span id="cb61-13"><a href="regression.html#cb61-13"></a>plt.axis(<span class="st">&#39;tight&#39;</span>)<span class="op">;</span></span>
<span id="cb61-14"><a href="regression.html#cb61-14"></a>plt.show()  </span></code></pre></div>
<p><img src="Bookdown-poly_files/figure-html/unnamed-chunk-34-1.png" width="672" /></p>
<div class="sourceCode" id="cb62"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb62-1"><a href="regression.html#cb62-1"></a><span class="co"># regression Elastic-Net</span></span>
<span id="cb62-2"><a href="regression.html#cb62-2"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> enet_path</span>
<span id="cb62-3"><a href="regression.html#cb62-3"></a>lambdas<span class="op">=</span>np.arange(<span class="fl">0.01</span>,<span class="fl">1.01</span>,<span class="fl">0.01</span>)</span>
<span id="cb62-4"><a href="regression.html#cb62-4"></a>alphas_enet, coefs_enet, _ <span class="op">=</span> enet_path(Xtildepy, ytildepy, alphas<span class="op">=</span>lambdas, l1_ratio<span class="op">=</span><span class="fl">0.3</span>, fit_intercept<span class="op">=</span><span class="va">False</span>)</span></code></pre></div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Akaike98">
<p>Akaike, Hirotogu. 1998. “Information Theory and an Extension of the Maximum Likelihood Principle.” In <em>Selected Papers of Hirotugu Akaike</em>, 199–213. Springer.</p>
</div>
<div id="ref-Akaike78">
<p>Akaike, Hirotugu. 1978. “A Bayesian Analysis of the Minimum Aic Procedure.” <em>Annals of the Institute of Statistical Mathematics</em> 30 (1): 9–14.</p>
</div>
<div id="ref-Azais">
<p>Azaïs, Jean-Marc, and Jean-Marc Bardet. 2005. <em>Le Modèle Linéaire Par L’exemple: Régression, Analyse de La Variance et Plans d’expériences Illustrés Avec R, Sas et Splus</em>. Dunod.</p>
</div>
<div id="ref-Draper">
<p>Draper, Norman R, and Harry Smith. 1998. <em>Applied Regression Analysis</em>. Vol. 326. John Wiley &amp; Sons.</p>
</div>
<div id="ref-Guyon">
<p>Guyon, X. 2001. “Modele Linéaire et économétrie.” <em>Ellipse, Paris</em>.</p>
</div>
<div id="ref-Hoerl">
<p>Hoerl, Arthur E, Robert W Kannard, and Kent F Baldwin. 1975. “Ridge Regression: Some Simulations.” <em>Communications in Statistics-Theory and Methods</em> 4 (2): 105–23.</p>
</div>
<div id="ref-hoerl76">
<p>Hoerl, Arthur E, and Robert W Kennard. 1976. “Ridge Regression Iterative Estimation of the Biasing Parameter.” <em>Communications in Statistics-Theory and Methods</em> 5 (1): 77–88.</p>
</div>
<div id="ref-Mallows">
<p>Mallows, Colin L. 2000. “Some Comments on Cp.” <em>Technometrics</em> 42 (1): 87–94.</p>
</div>
<div id="ref-McDonald">
<p>McDonald, Gary C, and Diane I Galarneau. 1975. “A Monte Carlo Evaluation of Some Ridge-Type Estimators.” <em>Journal of the American Statistical Association</em> 70 (350): 407–16.</p>
</div>
<div id="ref-Schwarz">
<p>Schwarz, Gideon, and others. 1978. “Estimating the Dimension of a Model.” <em>The Annals of Statistics</em> 6 (2): 461–64.</p>
</div>
<div id="ref-tibshirani96">
<p>Tibshirani, Robert. 1996. “Regression Shrinkage and Selection via the Lasso.” <em>Journal of the Royal Statistical Society: Series B (Methodological)</em> 58 (1): 267–88.</p>
</div>
<div id="ref-zou05">
<p>Zou, Hui, and Trevor Hastie. 2005. “Regularization and Variable Selection via the Elastic Net.” <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 67 (2): 301–20.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="singulier.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ANOVA.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Bookdown-poly.pdf", "Bookdown-poly.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
