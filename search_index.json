[["index.html", "Modèle linéaire général et modèle linéaire généralisé Préface", " Modèle linéaire général et modèle linéaire généralisé Cathy Maugis-Rabusseau (INSA Toulouse / IMT) 2021-10-28 Préface Ce polycopié reprend les notions abordées dans la partie modèle linéaire et modèle linéaire généralisé de l’UF “Elements de modélisation statistique” en modIA 4A. J’ai également enseigné ce cours en 4ème année de la spécialité Mathématiques appliquées (dans différentes versions selon l’évolution de la maquette). Je remercie ici les collègues du GMM qui ont permis l’évolution de ce cours (support de prédécesseurs, échanges oraux, …). Organisation des chapitres de ce cours Ce cours va débuter par un chapitre introductif 1. Dans la partie I, nous allons nous intéresser au modèle linéaire général. Les chapitres 2, 3, 4 et 5 sont des chapitres “théoriques” définissant le cadre du modèle linéaire général, traite le problème de l’estimation des paramètres et du test de sous-modèle de Fisher. Les chapitres 6, 7 et 8 sont dédiés à la régression linéaire, l’ANOVA et l’ANCOVA respectivement. Ils permettront d’illustrer les notions vues précédemment dans ces exemples classiques du modèle linéaire général. La partie II est dédiée au modèle linéaire généralisé. Après avoir donné les pincipes dans la chapitre 9, on détaillera l’exemple de la régression logistique dans le chapitre 10 et de la régression loglinéaire (Poisson) dans le chapitre 11. Notations : Les notations suivantes seront utilisées dans ce polycopié : \\(A&#39;\\) est la transposée de la matrice \\(A\\) \\(0_n=(0,\\ldots,0)&#39;\\in \\mathbb{R}^n\\) avec \\(n\\in\\mathbb{N}^\\star\\) \\(\\mathbb{1}_n=(1,\\ldots,1)&#39;\\in \\mathbb{R}^n\\) avec \\(n\\in\\mathbb{N}^\\star\\) \\(I_n\\) désigne la matrice identité de \\(\\mathcal M_n(\\mathbb{R})\\) \\(Var(A)\\) est la variance pour une variable aléatoire \\(A\\) \\(Cov(A,B)\\) est la covariance entre deux variables aléatoires \\(A\\) et \\(B\\) Soit \\(x=(x_1,\\ldots,x_n)\\) une série de mesures. On note \\(\\bar x_n=\\frac 1 n \\sum_{i=1}^{n} x_i\\) la moyenne des mesures et \\(var(x)=\\frac 1 n \\sum_{i=1}^n (x_i-\\bar x_n)^2\\) la variance. Soit \\(x=(x_1,\\ldots,x_n)\\) et \\(y=(y_1,\\ldots,y_n)\\) deux séries de mesures. La covariance est définie par \\(cov(x,y)=\\frac 1 n \\sum_{i=1}^{n} (x_i - \\bar x_n) (y_i-\\bar y_n)\\). On utilisera la même notation pour désigner la matrice et l’application linéaire associée à cette matrice Illustrations du cours : Les illustrations de ce polycopié sont principalement réalisées sous R. Les packages R suivants ont été utilisés. AER, bestglm, boot, corrplot, GGally, ggfortify, ggplot2, gridExtra, ISLR, leaps, MASS, nnet, VGAM Quelques codes python sont également proposés. "],["intro.html", "Chapitre 1 Introduction 1.1 Modélisation d’une réponse quantitative 1.2 Modélisation d’une variable binaire, de comptage, … 1.3 Objectifs du cours", " Chapitre 1 Introduction Documents associés à ce chapitre : Les slides associés à ce chapitre sont disponibles ici: SlidesIntro.pdf Le jeu de données utilisé est ronfletabac.csv Dans ce chapitre nous allons introduire à travers plusieurs exemples les différents types de modèles linéaires (général et généralisé) que nous allons aborder dans ce cours. 1.1 Modélisation d’une réponse quantitative 1.1.1 Jeu de données illustratif On considère le jeu de données ronfletabac.csv. Pour 100 individus, on dispose de leur taille, leur poids, leur âge et leur sexe (75 hommes et 25 femmes). On sait également si ce sont des fumeurs ou non; s’ils ronflent la nuit ou non. On appelle don le jeu de données chargé sous R. Un extrait des données est présenté ci-dessous : On obtient le résumé suivant des données : summary(don) age weight height sex snore tobacco Min. :23.00 Min. : 42.00 Min. :158.0 F:25 N:65 N:36 1st Qu.:43.00 1st Qu.: 75.50 1st Qu.:166.0 H:75 O:35 O:64 Median :52.00 Median : 92.00 Median :186.0 Mean :52.27 Mean : 88.83 Mean :181.1 3rd Qu.:62.25 3rd Qu.:104.25 3rd Qu.:194.0 Max. :74.00 Max. :120.00 Max. :208.0 Ce jeu de données est donc composé de 3 variables quantitatives et 3 variables qualitatives. Pour les variables quantitatives age, weights et height, leur distribution est résumée en Figure 1.1. Figure 1.1: Représentations graphiques de la distribution des variables quantitatives : l’âge, le poids et la taille. Pour les variables qualitatives sex, tobacco et snore, on résume les données sous forme de tableau de fréquences (Table 1.1) et on les présente graphiquement par des diagrammes en bâtons (Figure 1.2). Table 1.1: Tableau des fréquences pour les variables sex, tobacco et snore Variable Modalités Fréquence en % Sex Féminin 25 Masculin 75 Tobacco Oui 64 Non 36 Snore Oui 35 Non 65 Figure 1.2: Diagrammes en bâtons représentant la distribution des variables qualitatives : sexe, tabac et ronfle. 1.1.2 Régression linéaire Pour étudier la relation entre deux variables quantitatives (par exemple, entre le poids et la taille, ou entre le poids et l’âge), on peut tracer un nuage de points (Figure 1.3) et calculer le coefficient de corrélation linéaire entre ces deux variables (Table 1.2). Figure 1.3: Nuage de points représentant la relation entre le poids et la taille (à gauche), entre le poids et l’âge (à droite). Nous remarquons que le coefficient de corrélation linéaire est significativement différent de 0 entre le poids et la taille. Ce n’est pas le cas entre le poids et l’âge. Table 1.2: Coefficient de corrélation linéaire de Pearson et test de nullité de ce coefficient height age weight 0.92 -0.004 pvaleur &lt;2.2e-16 0.9687 1.1.2.1 Régression linéaire simple Le nuage de points peut être résumé par une droite que l’on appellera la droite de régression linéaire simple. C’est le cas le plus simple de modèle linéaire, qui permet d’expliquer une variable quantitative en fonction d’une autre variable quantitative. Par exemple, la droite de régression linéaire résumant la relation entre le poids et la taille a pour équation : \\[\\begin{equation} weight_i= a + b \\times height_i + \\varepsilon_i, \\, i=1,\\cdots, 100 \\label{eq:eqreglin} \\end{equation}\\] où \\(\\varepsilon_i\\) désigne l’erreur associée à chaque observation. Généralement, ces erreurs sont supposées être des variables indépendantes gaussiennes centrées de variance constante \\(\\sigma^2\\) à estimer. Le modèle statistique sous-jacent à l’équation peut aussi être présenté sous une forme matricielle : \\[\\begin{equation} \\underbrace{\\left(\\begin{array}{c}weight_1 \\\\ \\vdots \\\\ weight_{100}\\end{array} \\right)}_{weight} = \\underbrace{\\left( \\begin{array}{cc} 1 &amp; height_1 \\\\ \\vdots &amp; \\vdots\\\\ 1 &amp; height_{100} \\end{array} \\right)}_{X} \\underbrace{\\left( \\begin{array}{c} a \\\\ b \\end{array} \\right)}_{\\theta} + \\underbrace{\\left( \\begin{array}{c} \\varepsilon_1 \\\\ \\vdots \\\\ \\varepsilon_{100} \\end{array} \\right)}_{\\varepsilon} \\tag{1.1} \\end{equation}\\] Dans le modèle (1.1), \\(\\theta=(a,b)&#39;\\) et \\(\\sigma^2\\) sont inconnus. Afin d’estimer les paramètres \\(a\\) et \\(b\\), nous utilisons la méthode des moindres carrés. Nous choisissons ainsi le couple \\((\\hat a, \\hat b)\\) vérifiant : \\[(\\hat a, \\hat b)= \\mathrm{arg} \\underset{(\\alpha,\\beta)}{\\mathrm{min} } \\sum_{i=1}^{100} \\left( weight_i - \\alpha\\ - \\beta\\ height_i \\right)^2=\\mathrm{arg} \\underset{(\\alpha,\\beta)}{\\mathrm{min} }\\ || weight -\\alpha \\mathbb{1}_{100} - \\beta\\ height ||^2.\\] Dans le chapitre dédié à la régression linéaire, nous déterminerons l’expression explicite de ces estimateurs et étudierons leurs propriétés. A l’aide de la fonction lm() sous R, on peut facilement ajuster ce modèle de régression linéaire sur les données : reg1&lt;-lm(weight~height,data=don) summary(reg1) Call: lm(formula = weight ~ height, data = don) Residuals: Min 1Q Median 3Q Max -20.7482 -3.8787 0.6629 4.1182 17.0261 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -146.16586 10.35384 -14.12 &lt;2e-16 *** height 1.29760 0.05702 22.76 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 7.583 on 98 degrees of freedom Multiple R-squared: 0.8409, Adjusted R-squared: 0.8393 F-statistic: 517.9 on 1 and 98 DF, p-value: &lt; 2.2e-16 En pratique nous obtenons les estimations suivantes : \\(\\left(\\widehat{b}\\right)^{obs} =\\) 1.298 : estimation de la pente de la droite de régression = estimation de la variation moyenne du poids par rapport à la taille \\(\\left(\\widehat{a}\\right)^{obs} =\\) -146.166 : estimation de l’ordonnée à l’origine de la droite de régression \\(\\left(\\widehat{\\sigma^2}\\right)^{obs}=\\) (7.583)\\(^2\\) L’estimation de la pente égale à 1.298 est significativement différente de 0, montrant que le poids et la taille sont liés de façon significative. Ces résultats préliminaires ne donnent qu’une approximation du modèle linéaire sous-jacent. Dans bien des situations, il reste à mener une étude approfondie permettant de “valider” le modèle et de l’exploiter (construction de tests, d’intervalles de confiance, …) Nous reviendrons plus en détails sur ces notions dans les chapitres suivants. 1.1.2.2 Régression linéaire multiple Il peut être également intéressant de modéliser une variable en fonction de plusieurs autres variables quantitatives, par un modèle de régression linéaire multiple. Par exemple, on peut modéliser le poids en fonction de la taille et de l’âge, ce qui donne l’équation suivante : \\[weight_i = a_0 + a_1 \\times height_i + a_2 \\times age_i + \\varepsilon_i,\\] où les \\(\\varepsilon_i\\), \\(i=1,\\ldots,100\\) désignent des variables indépendantes gaussiennes centrées de variance constante \\(\\sigma^2\\). Dans ce cas, le modèle statistique peut s’écrire de la façon suivante : \\[\\begin{equation} \\underbrace{\\left( \\begin{array}{c} weight_1 \\\\ \\vdots \\\\ weight_{100} \\end{array} \\right)}_{weight} =\\underbrace{\\left( \\begin{array}{ccc} 1 &amp; height_1 &amp; age_1\\\\ \\vdots &amp; \\vdots &amp; \\vdots \\\\ 1 &amp; height_{100} &amp; age_{100} \\end{array} \\right)}_{X} \\underbrace{\\left( \\begin{array}{c} \\theta_0 \\\\ \\theta_1 \\\\\\theta_2 \\end{array} \\right)}_{\\theta} +\\underbrace{\\left( \\begin{array}{c} \\varepsilon_1 \\\\ \\vdots \\\\ \\varepsilon_{100} \\end{array} \\right)}_{\\varepsilon} \\tag{1.2} \\end{equation}\\] cette formule pouvant à nouveau être écrite de la façon suivante : \\(weight=X\\theta + \\varepsilon\\). On peut remarquer en regardant les équations (1.1) et (1.2) que les deux modèles de régression linéaire considérés précédemment s’écrivent sous une “même forme” matricielle. 1.1.3 Analyse de la variance (ANOVA) Il est possible d’étudier la relation entre une variable quantitative et une variable qualitative, par exemple entre le poids et le sexe, ou entre le poids et le tabac. Cette relation est représentée graphiquement par des violin plots (ou boxplots) parallèles (Figure 1.4). Figure 1.4: Boxplots parallèles représentant la relation entre le poids et le sexe (à gauche); entre le poids et le tabac (à droite). 1.1.3.1 ANOVA à un facteur Intuitivement, pour comparer le poids des hommes et celui des femmes, nous allons calculer le poids moyen pour chaque groupe. Statistiquement, nous modélisons le poids en fonction du sexe en mettant en oeuvre un modèle d’analyse de variance à un facteur qui s’écrit sous la forme : \\[weight_i = \\mu_1 \\mathbb{1}_{sex_i=F} + \\mu_2\\mathbb{1}_{sex_i=H} +\\varepsilon_i,\\] où les \\(\\varepsilon_i, \\, i=1,\\ldots,100\\) désignent des variables indépendantes gaussiennes centrées de variance constante \\(\\sigma^2\\). Dans ce cas, en réordonnant les observations selon le facteur sexe, le modèle peut être écrit sous la forme matricielle suivante : \\[\\underbrace{\\left( \\begin{array}{c} weight_{11} \\\\ \\vdots \\\\ weight_{1n_1} \\\\ weight_{21} \\\\ \\vdots \\\\ weight_{2n_2} \\end{array} \\right)}_{weight} = \\underbrace{\\left( \\begin{array}{cc} 1 &amp; 0 \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; 0 \\\\ 0 &amp; 1 \\\\ \\vdots &amp; \\vdots \\\\ 0 &amp; 1 \\end{array} \\right)}_{X} \\underbrace{\\left( \\begin{array}{c} \\mu_1 \\\\ \\mu_2 \\end{array} \\right) }_{\\theta} +\\underbrace{ \\left( \\begin{array}{c} \\varepsilon_{11} \\\\ \\vdots \\\\ \\varepsilon_{1n_1} \\\\ \\varepsilon_{21} \\\\ \\vdots \\\\ \\varepsilon_{2n_2} \\end{array} \\right)}_{\\varepsilon},\\] où \\(weight_{ij}\\) désigne le poids de l’individu \\(j\\) de sexe \\(i=F\\) ou H avec \\(j\\) variant de \\(1\\) à \\(n_i\\). En pratique, on utilise la méthode des moindres carrés pour estimer les paramètres inconnus. Toujours à l’aide de la fonction lm() de R, on obtient les résultats suivants : anova1&lt;-lm(weight~sex-1,data=don) summary(anova1) Call: lm(formula = weight ~ sex - 1, data = don) Residuals: Min 1Q Median 3Q Max -48.77 -13.44 4.00 16.23 29.23 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) sexF 83.000 3.741 22.19 &lt;2e-16 *** sexH 90.773 2.160 42.03 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 18.7 on 98 degrees of freedom Multiple R-squared: 0.9584, Adjusted R-squared: 0.9576 F-statistic: 1129 on 2 and 98 DF, p-value: &lt; 2.2e-16 Nous obtenons donc \\(\\left(\\widehat{ \\mu_1}\\right)^{obs} =\\) 83 et \\(\\left(\\widehat{ \\mu_2}\\right)^{obs}=\\) 90.773 qui représentent le poids moyen des femmes et celui des hommes respectivement. 1.1.3.2 ANOVA à deux facteurs Il est également possible d’étudier l’effet conjoint du sexe et du tabac sur le poids. Intuitivement, on peut étudier les moyennes par classe, en croisant les deux variables qualitatives. Pour étudier l’effet combiné du sexe et du tabac sur le poids, nous mettons en oeuvre un modèle d’analyse de variance à deux facteurs croisés. Ce modèle s’écrit de la façon suivante : \\[weight_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_{ij} + \\varepsilon_{ijk},\\ \\varepsilon_{ijk}\\underset{i.i.d}{\\sim}\\mathcal N (0,\\sigma^2)\\] où \\(weight_{ijk}\\) désigne le poids de l’individu \\(k\\) tel que \\(sexe = i\\in\\{H,F\\}\\) et \\(tabac=j\\in\\{0,N\\}\\). Les variables \\(\\varepsilon_{ijk}\\) sont supposées être des variables indépendantes gaussiennes centrées et de variance constante \\(\\sigma^2\\). Nous pouvons également écrire ce modèle sous forme matricielle de la forme \\[weight=X\\theta +\\varepsilon.\\] Ce modèle nous permettra d’étudier l’effet de chaque facteur (sexe et tabac) sur le poids, mais aussi de détecter des combinaisons (interactions) entre le sexe et le tabac qui donneraient un poids particulièrement différent des autres classes. 1.1.4 Analyse de covariance (ANCOVA) Sur notre exemple, nous pouvons tenter d’expliquer le poids selon la taille (variable quantitative) et le sexe (variable qualitative). Dans ce cas, nous pouvons représenter deux nuages de points entre le poids et la taille, l’un pour les femmes et l’autre pour les hommes, comme le montre la Figure 1.5. Figure 1.5: Nuages de points représentant la relation entre le poids et la taille selon le sexe. Le modèle d’analyse de covariance s’écrit de la façon suivante : \\[ weight_{ij}=a_i + b_i \\ height_{ij} + \\varepsilon_{ij}, i\\in\\{H,F\\} \\mbox{ et } j=1, \\cdots, n_i \\] où \\(weight_{ij}\\) désigne le poids de l’individu \\(j\\) de sexe \\(i\\) et les erreurs \\(\\varepsilon_{ij}\\) sont supposées gaussiennes centrées indépendantes et de variance \\(\\sigma^2\\). Nous pouvons ainsi comparer l’effet de la taille sur le poids, selon le sexe en mettant en oeuvre un modèle d’analyse de la covariance. En pratique cela correspond à estimer, pour chaque modalité de la variable sexe, une droite de régression du poids en fonction de la taille. En conclusion, dans les divers problèmes évoqués dans ce paragraphe, à savoir la régression linéaire, l’analyse de variance et l’analyse de covariance, nous avons utilisé : le même type de modélisation matricielle, le même type d’hypothèses sur les erreurs, l’estimateur des moindres carrés. En fait, ces différents problèmes ne sont pas si éloignés qu’ils le paraissent a priori car les modèles utilisés font partie d’une même famille de modèles : le modèle linéaire général. 1.2 Modélisation d’une variable binaire, de comptage, … On se place maintenant dans le cas où la variable réponse \\(Y\\) est qualitative (ou des comptages) et on souhaite expliquer cette variable \\(Y\\) en fonction de plusieurs régresseurs \\(z^{(1)},\\dots,z^{(m)}\\). Voici quelques exemples illustratifs : Exemple 1 Une compagnie d’assurance cherche à détecter les dossiers frauduleux. Elle dispose pour cela d’un panel de \\(n\\) dossiers. À chacun de ces dossiers est associé la valeur \\(0\\) (pour dossier frauduleux) ou \\(1\\). Après avoir sélectionné les caractéristiques les plus intéressantes (endettement du foyer, milieu social, lieu de résidence, …), elle cherche à savoir dans quelle mesure ces dernières variables influencent la probabilité d’existence d’une fraude. Elle espère pouvoir ainsi à l’avenir détecter d’éventuels dossiers “sensibles”. On est dans le cas d’une variable réponse \\(Y\\) binaire. Exemple 2 On souhaite expliquer le nombre d’espèces de plantes qui se développent dans différents lieux en fonction de la biomasse de ces différents lieux et du pH du sol. La variable réponse \\(Y\\) prend ici ses valeurs dans \\(\\mathbb{N}\\). Dans le cas d’une variable réponse binaire, on dispose d’un vecteur \\(Y=(Y_1,\\dots,Y_n)&#39;\\) où \\(Y_i \\sim \\mathcal{B}(\\pi_i)\\) pour tout \\(i\\in\\lbrace 1,\\dots, n \\rbrace\\) et de \\(m\\) régresseurs \\(z^{(1)},\\dots,z^{(m)}\\). Il semblerait assez naturel d’utiliser la modélisation suivante : \\[\\mathbb{E}[Y_i] = \\pi_i = a_1 z_i^{(1)} + a_2 z_i^{(2)} + \\dots + a_m z_i^{(m)}, \\ i=1, \\dots ,n.\\] Cependant, comme on cherche à modéliser et prédire des probabilités, cette approche semble peu recommandée dans la mesure où certaines valeurs prédites pourraient ne pas appartenir à l’intervalle \\([0,1]\\). On va donc plutôt chercher à modéliser linéairement une fonction des \\(\\pi\\). Par exemple dans le cadre de la régression logistique, on considère la fonction de lien \\(g: ]0,1[ \\rightarrow \\mathbb{R}\\) définie par \\[ g(t) = \\ln\\left( \\frac{t}{1-t} \\right), \\ \\forall t\\in ]0,1[\\] et on modélise \\[ g(\\pi_i)= a_1 z_i^{(1)} + \\dots + a_m z_i^{(m)}, \\ \\forall i \\in \\lbrace 1,\\dots,n \\rbrace.\\] De manière plus générale, il est possible d’envisager de considérer d’autres distributions pour la variable \\(Y\\) et d’autres fonctions de lien. À ce titre, on pourra remarquer que le modèle de régression abordé au début de ce chapitre correspond à une distribution gaussienne et à une fonction de lien canonique (identité). Nous verrons qu’il est possible d’étudier tous ces modèles à travers un même cheminement : le modèle linéaire généralisé. 1.3 Objectifs du cours Nous verrons que les modèles de régression linéaire (simple ou multiple), d’analyse de variance et d’analyse de covariance peuvent être rassemblés sous un même formalisme : on parle alors de modèle linéaire général. Un cran au-dessus, nous pouvons encore rassembler le modèle linéaire général et par exemple la régression logistique sous une même bannière : le modèle linéaire généralisé. Les possibilités offertes par ces différentes modélisations ne s’arrêtent pas à une simple écriture commune. C’est en fait tout le traitement et l’exploitation des données qui peuvent être abordés de manière unifiée. Les slides pour les chapitres 2, 3, 4 et 5 sont disponibles ici "],["DefML.html", "Chapitre 2 Définitions générales 2.1 Modèle linéaire régulier 2.2 Exemples de modèle linéaire gaussien 2.3 En résumé", " Chapitre 2 Définitions générales 2.1 Modèle linéaire régulier Definition 2.1 Soit \\(Y=(Y_1,\\ldots,Y_n)&#39;\\) une variable réponse. \\(Y\\) suit un modèle linéaire statistique si \\(Y\\) peut être écrite sous la forme : \\[\\begin{equation} Y= X\\theta + \\varepsilon, \\tag{2.1} \\end{equation}\\] où \\(X\\) est une matrice réelle à \\(n\\) lignes et \\(k\\) colonnes avec \\(\\mathbf{k&lt;n}\\), \\(X \\in \\mathcal{M}_{n,k}(\\mathbb{R})\\), \\(\\theta\\) est un vecteur réel inconnu de taille \\(k\\), le vecteur \\(\\varepsilon\\in\\mathbb{R}^n\\) représente l’erreur du modèle. Cette définition est très générale et dépasse largement le cadre de la régression et de l’analyse de variance. L’hypothèse \\(k&lt;n\\) signifie que le nombre d’observations doit être supérieur au nombre de paramètres à estimer. C’est en quelque sorte une hypothèse d’identifiabilité. Definition 2.2 Le modèle linéaire (2.1) est dit régulier si la matrice \\(X\\) est régulière, c’est-à-dire de rang \\(k\\). Dans le cas contraire (\\(X\\) est de rang \\(r&lt;k\\)), on parle de modèle singulier. Proposition 2.1 Soit \\(X \\in \\mathcal{M}_{n,k}(\\mathbb{R})\\). Les propositions suivantes sont équivalentes : \\(X\\) est une matrice de rang \\(k\\). L’application \\(X : \\mathbb{R}^k \\rightarrow \\mathbb{R}^n\\) est injective. La matrice \\(X&#39;X\\) est inversible. Ainsi, si \\(X\\) est régulière alors par injectivité de l’application \\(X\\), \\(X\\theta=0_{n} \\Rightarrow \\theta=0_{k}\\) pour tout \\(\\theta\\in\\mathbb{R}^k\\). Cette propriété assure que les colonnes de \\(X\\) sont linéairement indépendantes dans \\(\\mathbb{R}^n\\) et garantit l’unicité de \\(\\theta\\). Dans certaines situations, la matrice considérée \\(X\\) ne pourra être régulière. Nous verrons cependant (cf section 5.2) qu’il est parfois possible de pallier ce problème en rajoutant des contraintes dites d’identifiabilité sur les paramètres à estimer. À moins que cela ne soit mentionné explicitement, la matrice \\(X\\) sera supposée régulière par la suite. Proposition 2.2 Soit \\(X \\in \\mathcal{M}_{n,k}(\\mathbb{R})\\) une matrice régulière. Alors la matrice de projection sur \\([X]=Im(X)\\) est donnée par \\(P_{[X]}=X(X&#39;X)^{-1}X&#39;\\). Cette matrice \\(P_{[X]}\\), souvent notée \\(H\\), est appelée la matrice chapeau ou Hat Matrix. Proof. Soit \\(P_{[X]}:=X(X&#39;X)^{-1}X&#39;\\) où \\(X \\in \\mathcal{M}_{n,k}(\\mathbb{R})\\) une matrice régulière. Pour tout \\(u\\in\\mathbb{R}^n\\), on a \\(u = P_{[X]} u + u - P_{[X]}u\\) et \\(P_{[X]}u = X (X&#39;X)^{-1}X&#39;u\\in [X]\\). On va montrer que \\(u-P_{[X]}u \\in [X]^{\\perp}\\) : \\[\\begin{eqnarray*} \\forall v\\in\\mathbb{R}^k,\\ \\ (Xv)&#39;(u-P_{[X]}u) &amp;=&amp; v&#39;X&#39;(u - X(X&#39;X)^{-1}X&#39; u) \\\\ &amp;=&amp; v&#39;X&#39;u - v&#39;(X&#39; X)(X&#39;X)^{-1}X&#39;u = 0 \\end{eqnarray*}\\] Afin de pouvoir travailler plus simplement et d’aller plus loin dans l’étude de ce modèle, nous allons maintenant imposer quelques restrictions concernant le vecteur des erreurs \\(\\varepsilon\\) : Hypothèse H1 : Les erreurs sont centrées \\(\\mathbb{E}[\\varepsilon]=0_{n}\\). Cette hypothèse est relativement importante et assure que le modèle est correctement défini. En effet, s’il s’avérait que \\(\\mathbb{E}[\\varepsilon] \\not= 0_{n}\\), cela pourrait signifier qu’une partie de l’information n’a pas été prise en compte dans la modélisation de \\(\\mathbb{E}[Y]\\). En fait cette hypothèse suppose que \\[\\mathbb{E}[Y]=X\\theta = \\sum_{j=1}^k\\theta_j X^{(j)}\\] où \\(X^{(j)}\\) désigne la colonne \\(j\\) de la matrice \\(X\\). En d’autres termes, l’écriture de ce modèle suppose que l’ensemble des variables \\(X^{(j)}\\) est censé expliquer \\(Y\\) par une relation de cause à effet. Ainsi les variables \\(X^{(j)}\\) sont appelées variables explicatives ou prédicteurs. Au final, en moyenne \\(Y\\) s’écrit donc comme une combinaison linéaire des \\(X^{(j)}\\) : la liaison entre les \\(X^{(j)}\\) et \\(Y\\) est de nature linéaire. C’est la raison pour laquelle ce modèle est appelé modèle linéaire. Hypothèse H2 : La variance des erreurs est constante : \\[\\forall i=1,\\dots,n,\\ \\ \\mathbb{E}[\\varepsilon_i^2]=\\sigma^2\\] où \\(\\sigma^2\\) est un paramètre inconnu, à estimer. Cette hypothèse impose que \\(Var(Y_i)=\\sigma^2\\) pour tout \\(i=1,\\ldots,n\\). Il est souvent raisonnable de supposer que H2 est bien vérifiée. Dans le cas contraire, il est possible de mettre en place un traitement statistique du modèle linéaire… mais cela nécessite bien plus de travail. Hypothèse H3 : Les variables \\(\\varepsilon_i\\) sont indépendantes. Nous considèrerons que cette hypothèse est vérifiée lorsque chaque donnée correspond à un échantillonnage indépendant ou à une expérience physique menée dans des conditions indépendantes. Il existe un certain nombre de cas où ce postulat ne peut s’appliquer. On pourra par exemple penser aux séries temporelles : l’erreur du passé peut avoir une influence sur l’erreur future. Ces dernières font appel à un traitement statistique particulier (processus ARMA par exemple). Hypothèse H4 : Les données suivent des lois gaussiennes \\[\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2), \\, \\forall i \\in \\{1,\\cdots,n\\}.\\] L’hypothèse de normalité des erreurs peut se justifier : par un argument théorique : les erreurs sont caractérisables comme des erreurs de mesure. Ce sont une accumulation de petits aléas non-maîtrisables et indépendants. Par exemple, la mesure du poids d’un animal peut être soumise à des fluctuations dues à des erreurs de mesure à la pesée, à l’état de santé de l’animal, à son bagage génétique, à l’effet individuel de l’animal à prendre plus ou moins du poids. D’après le Théorème Central Limite, si tous ces effets sont indépendants de même moyenne nulle et de même “petite” variance, leur somme tend vers une variable gaussienne. La distribution gaussienne modélise assez bien toutes les situations où le hasard est la résultante de plusieurs causes indépendantes les unes des autres ; les erreurs de mesure suivent généralement assez bien la loi gaussienne. par un argument pratique : il est facile de contrôler si une variable aléatoire suit une loi normale. En étudiant a posteriori la distribution des résidus calculés (erreurs estimées) et en la comparant à la distribution théorique (normale), on constate souvent qu’elle peut être considérée comme s’approchant de la loi gaussienne. Il découle des hypothèses H1-H4 la normalité de \\(Y\\) : \\[Y \\sim \\mathcal{N}_n\\left(X\\theta,\\sigma^2 I_n\\right)\\] Dans la littérature statistique, un certain nombre de méthodes, souvent graphiques, sont proposées afin de vérifier la satisfaction des hypothèses H1-H4. Nous les aborderons à la section 6.6. 2.2 Exemples de modèle linéaire gaussien 2.2.1 Le modèle de régression linéaire On cherche à modéliser une variable quantitative \\(Y\\) en fonction de variables explicatives quantitatives \\(x^{(1)}, \\cdots, x^{(p)}\\). Sous l’hypothèse gaussienne, le modèle de régression linéaire s’écrit : \\[Y_i = \\theta_0 + \\theta_1 x^{(1)}_i+\\cdots + \\theta_p x^{(p)}_i+\\varepsilon_i,\\] avec \\(\\theta_0, \\theta_1, \\cdots, \\theta_p\\) paramètres inconnus et \\(\\varepsilon_1, \\cdots, \\varepsilon_n\\) i.i.d de loi \\(\\mathcal{N}(0,\\sigma^2)\\) avec \\(\\sigma^2\\) à estimer. Matriciellement, le modèle peut se réécrire sous la forme \\[ Y = X \\theta + \\varepsilon \\] avec \\(\\theta=(\\theta_0,\\theta_1,\\cdots,\\theta_p)&#39;\\) et \\(X=(\\mathbb{1}_n,x^{(1)},\\ldots,x^{(p)})\\in \\mathcal{M}_{n,(p+1)}(\\mathbb{R})\\). Le modèle de régression linéaire sera étudié en détail dans le chapitre 6. Exercise 2.1 Soit \\(Y_i = \\theta_0 + \\theta_1 x^{(1)}_i+\\cdots + \\theta_p x^{(p)}_i+\\varepsilon_i,\\ \\forall i=1,\\ldots,n\\) avec \\(\\varepsilon_1, \\cdots, \\varepsilon_n\\) i.i.d de loi \\(\\mathcal{N}(0,\\sigma^2)\\). Quelle est la loi de \\(Y_i\\) ? Quelle est la loi de \\(Y\\) ? 2.2.2 Le modèle d’analyse de la variance On cherche à modéliser une variable quantitative \\(Y\\) en fonction d’une (ou de plusieurs) variable(s) explicative(s) qualitative(s) (appelée facteur). Sous l’hypothèse gaussienne, le modèle à un facteur à \\(I\\) modalités s’écrit : \\[\\begin{equation} Y_{ij} = \\mu_i + \\varepsilon_{ij} \\mbox{ pour } i = 1, \\cdots, I \\,;\\, j = 1, \\cdots, n_i, \\tag{2.2} \\end{equation}\\] avec \\(\\mu_1, \\cdots, \\mu_I\\) des paramètres inconnus et \\(\\varepsilon_{11}, \\cdots, \\varepsilon_{In_I}\\) \\(n\\) observations indépendantes de loi \\(\\mathcal{N}(0,\\sigma^2)\\) avec \\(\\sigma^2\\) à estimer. Le modèle d’analyse de la variance sera étudié en détail dans le chapitre 7. Exercise 2.2 Afin d’écrire sous forme matricielle ce modèle, les observations sont rangées par modalité du facteur \\[Y=(Y_{11}, \\cdots, Y_{1,n_1}, Y_{2,1},\\cdots ,Y_{2n_2}, \\cdots,Y_{I1},\\cdots, Y_{In_I})&#39;.\\] Soit \\(\\displaystyle n=\\sum_{i=1}^In_i\\). Ecrivez le modèle (2.2) sous la forme \\(Y = X\\theta + \\varepsilon\\) en précisant la matrice de design \\(X\\in\\mathcal{M}_{nI}(\\mathbb{R})\\) et \\(\\theta\\in\\mathbb{R}^I\\). Quelle est la loi de \\(Y_{ij}\\), de \\(Y_i=(Y_{i1},\\ldots,Y_{in_i})&#39;\\) et de \\(Y\\) ? 2.3 En résumé Modéle linéaire : \\[ Y = X\\theta + \\varepsilon \\textrm{ avec } \\varepsilon\\sim\\mathcal{N}_n(0_n,\\sigma^2 I_n) \\] avec \\(Y\\in\\mathbb{R}^n\\), \\(X\\in\\mathcal{M}_{n,k}(\\mathbb{R})\\), \\(\\theta\\in\\mathbb{R}^k\\), \\(\\varepsilon\\in\\mathbb{R}^n\\) Modèle régulier si \\(\\mbox{rg}(X)=k\\), sinon il est singulier Modèle régulier \\(\\Leftrightarrow\\) \\(X\\) injective \\(\\Leftrightarrow\\) \\(X&#39;X\\) inversible Matrice de projection orthogonale sur \\([X]=Im(X)\\) : \\[P_{[X]}=X(X&#39;X)^{-1}X&#39;\\] "],["EstML.html", "Chapitre 3 Estimation des paramètres 3.1 Estimation de \\(\\theta\\) 3.2 Valeurs ajustées et résidus 3.3 Estimation de \\(\\sigma^2\\) 3.4 Erreurs standards 3.5 Intervalle de confiance de \\(\\theta_j\\), de \\((X\\theta)_i\\) et de \\(X_0\\theta\\) 3.6 Intervalles de prédiction 3.7 Qualité d’ajustement 3.8 En résumé", " Chapitre 3 Estimation des paramètres Dans ce chapitre, nous allons nous intéresser à l’estimation des paramètres dans un modèle linéaire général régulier : \\[ Y = X \\theta +\\varepsilon \\textrm{ avec } \\varepsilon\\sim\\mathcal N(0_n, \\sigma^2 I_n) \\] où \\(X\\in\\mathcal M_{n,k}(\\mathbb{R})\\), \\(rg(X)=k\\). 3.1 Estimation de \\(\\theta\\) Dans cette section, nous nous intéressons à l’estimation du vecteur de paramètres \\(\\theta\\). Pour cela, nous allons utiliser la méthode des moindres carrés. Il s’agit ici de trouver le vecteur \\(\\hat\\theta\\) qui va minimiser la distance entre l’image de la matrice \\(X\\) et les observations \\(Y\\). Autrement dit, l’estimateur de \\(\\theta\\) par la méthode des moindres carrés est défini par : \\[\\begin{eqnarray*} \\widehat{\\theta} &amp;=&amp; \\mathrm{arg} \\, \\underset{\\vartheta}{\\mathrm{min}}\\, \\| Y - X\\vartheta \\|^2 \\\\ &amp;=&amp;\\mathrm{arg} \\ \\underset{\\vartheta}{\\mathrm{min}}\\ SCR(\\vartheta). \\end{eqnarray*}\\] où \\(\\| . \\|\\) est la norme issue du produit scalaire usuel dans \\(\\mathbb{R}^n\\): \\(\\| u\\|^2=\\langle u,u \\rangle= \\sum_{i=1}^n u_i^2=u&#39;u, \\, \\forall u \\in \\mathbb{R}^n.\\) Sous forme matricielle, il est possible d’écrire : \\[\\begin{equation*} \\widehat{\\theta}= \\mathrm{arg} \\ \\underset{\\vartheta}{\\mathrm{min}}\\ (Y - X\\vartheta)&#39; (Y - X\\vartheta) . \\end{equation*}\\] Theorem 3.1 Soit un modèle linéaire régulier \\(Y=X\\theta+\\varepsilon\\). L’estimateur \\(\\widehat{\\theta}\\) obtenu par la méthode des moindres carrés est \\[\\begin{equation} \\tag{3.1} \\widehat{\\theta}= (X&#39; X)^{-1} X&#39; Y. \\end{equation}\\] Proof. On cherche le vecteur \\(X\\hat\\beta\\) appartenant au sous-espace vectoriel de \\(\\mathbb{R}^n\\) engendré par les vecteurs colonnes de la matrice \\(X\\) (c’est à dire l’image de \\(X\\) noté \\([X]\\)). On a: \\[ \\min_{\\vartheta} \\| Y- X\\vartheta \\|^2= \\min_{u \\in [X]} \\| Y- u \\|^2 = \\| Y - P_{[X]} Y \\|^2,\\] où \\(P_{[X]}\\) désigne la projection orthogonale sur \\(Im(X)=[X]\\). Ainsi \\(X \\hat \\theta = P_{[X]}Y = X(X&#39;X)^{-1} X&#39; Y\\). Le modèle étant supposé régulier, \\(X\\) est injective. On en déduit donc que \\(\\hat \\theta = (X&#39;X)^{-1} X&#39; Y\\). Ce premier théorème nous donne donc une formule explicite pour l’estimateur du vecteur \\(\\theta\\) par la méthode des moindres carrés. Il est intéressant de noter que cette dernière est purement géométrique et ne demande aucune connaissance de la loi des erreurs. En effet, l’estimateur \\(\\widehat{\\theta}\\) obtenu par la méthode des moindres carrés vérifie la propriété suivante : \\[X\\widehat{\\theta} = P_{[X]} Y.\\] Remark. Dans le cas particulier où les erreurs sont gaussiennes, l’estimateur des moindres carrés \\(\\widehat{\\theta}\\) correspond exactement à l’estimateur du maximum de vraisemblance. En effet, l’estimation par maximum de vraisemblance est basée sur la vraisemblance du modèle linéaire gaussien : \\[L(\\theta, \\sigma^2; y) = \\prod_{i=1}^nf(y_i; \\theta)\\] où \\(f(y_i; \\theta)\\) est la densité de la loi normale de la variable aléatoire \\(Y_i\\) . Ainsi \\[L(\\theta,\\sigma^2; (Y_1,\\cdots,Y_n))=\\frac{1}{(2\\pi)^{n/2}\\sigma^n}\\exp\\left(-\\frac{\\|Y-X\\theta\\|^2}{2\\sigma^2}\\right).\\] Pour obtenir l’estimateur \\(\\widehat{\\theta}\\) du maximum de vraisemblance, on maximise sa logvraisemblance selon \\(\\theta\\). On remarque que par croissance de la fonction exponentielle cela revient à minimiser \\(\\|Y-X\\theta\\|^2\\). Le résultat suivant explicite les performances de l’estimateur des moindres carrés. Theorem 3.2 Soit un modèle linéaire régulier \\(Y=X\\theta+\\varepsilon\\) et \\(\\hat\\theta\\) l’estimateur de \\(\\theta\\) par la méthode des moindres carrés défini par (3.1). Alors \\[\\mathbb{E}\\left [\\widehat{\\theta}\\right]= \\theta \\quad \\mathrm{et} \\quad \\mathrm{Var}\\left(\\widehat{\\theta}\\right)= \\sigma^2 (X&#39;X)^{-1}.\\] De plus, si les variables \\(\\varepsilon_i\\) sont i.i.d, gaussiennes centrées, \\(\\widehat{\\theta}\\) est le meilleur estimateur parmi tous les estimateurs sans biais de \\(\\theta\\), i.e. \\[ \\mathrm{Var}( C&#39;\\widetilde\\theta) \\geq \\mathrm{Var}( C&#39; \\widehat{\\theta}),\\] pour tout \\(\\widetilde\\theta\\) estimateur sans biais de \\(\\theta\\) et toute combinaison linéaire \\(C&#39; \\theta\\), où \\(C \\in \\mathbb{R}^k\\). Dans ce cas \\(\\widehat{\\theta}\\) est un vecteur gaussien : \\[\\widehat{\\theta} \\sim \\mathcal{N}_k\\left(\\theta, \\sigma^2 (X&#39;X)^{-1}\\right).\\] Exercise 3.1 L’exercice suivant vous guide pour déontrer les points clés du théorème 3.2. Montrez que \\(\\mathbb{E}\\left [\\widehat{\\theta}\\right]= \\theta\\) (rappel : \\(\\mathbb{E}\\left [Y\\right]= X \\theta\\)) Montrez que \\(\\mathrm{Var}\\left(\\widehat{\\theta}\\right)= \\sigma^2 (X&#39;X)^{-1}\\) (rappel : \\(\\mathrm{Var}(AY) = A \\mathrm{Var}(Y) A&#39;\\)) Pourquoi \\(\\widehat\\theta\\) est un vecteur gaussien ? 3.2 Valeurs ajustées et résidus Definition 3.1 Soit un modèle linéaire \\(Y=X\\theta+\\varepsilon\\) et \\(\\widehat{\\theta}\\) l’EMC pour \\(\\theta\\). On appelle valeurs prédites (ou ajustées) \\(\\widehat Y_i\\) par le modèle pour chaque \\(Y_i\\) les valeurs suiavntes : \\[ \\widehat{Y} =(\\widehat Y_1,\\cdots,\\widehat Y_n)&#39; = X \\widehat \\theta = X(X&#39;X)^{-1}X&#39; Y = P_{[X]} Y = H Y. \\] On estime les erreurs \\(\\varepsilon_i\\) par les quantités suivantes appelées les résidus : \\[ \\hat \\varepsilon = (\\hat \\varepsilon_1,\\ldots,\\hat \\varepsilon_n)&#39; = Y - \\widehat Y = (I_n - P_{[X]}) Y = (I_n - H) Y. \\] Ayant des réalisations \\(y_i\\), on obtient alors des valeurs prédites observées \\(\\hat y_i=(\\hat Y_i)^{obs} = (X\\widehat \\theta^{obs})_i\\) et des résidus calculés \\((\\hat \\varepsilon_i)^{obs}=y_i-\\hat y_i\\). Proposition 3.1 Les valeurs ajustées et les résidus vérifient les propriétés suivantes : \\(\\widehat{Y} \\sim \\mathcal{N}_n\\left(X\\theta, \\sigma^2 P_{[X]} \\right)\\) où \\(P_{[X]} = X(X&#39;X)^{-1}X&#39;\\) \\(\\widehat{\\varepsilon} \\sim \\mathcal{N}_n\\left(0_{n}, \\sigma^2(I_n- P_{[X]})\\right)\\) Les variables aléatoires \\(\\widehat{Y}\\) et \\(\\widehat{\\varepsilon}\\) sont indépendantes. Les variables aléatoires \\(\\widehat{\\theta}\\) et \\(\\widehat{\\varepsilon}\\) sont indépendantes. Exercise 3.2 Preuve de la proposition 3.1 Pour démontrer le premier point, utilisez la loi de \\(\\widehat \\theta\\) Pour démontrer le deuxième point, on peut remarquer que \\(\\widehat{\\varepsilon} = (I_n - P_{[X]}) Y\\) et \\(Y\\sim \\mathcal N(X\\theta, \\sigma^2 I_n)\\). Pour démontrer le troisième point, pensez au théorème de Cochran! Pour démontrer le quatrième point, on peut remarquer que \\(\\widehat{\\theta} = (X&#39;X)^{-1} X&#39; X \\widehat{\\theta}\\). 3.3 Estimation de \\(\\sigma^2\\) Dans cette section, on s’intéresse à l’estimation de la variance des erreurs \\(\\sigma^2\\), appelée variance résiduelle. Par définition du modèle linéaire, la variance résiduelle \\(\\sigma^2\\) est également donnée comme la variance de \\(Y\\) pour \\(X\\) fixé. Dans le cadre de la régression linéaire, cela s’interprète comme la variance de \\(Y\\) autour de la droite de régression théorique. Cette définition de \\(\\sigma^2\\) suggère que son estimation est calculée à partir des écarts entre les valeurs observées \\(Y_i\\) et les valeurs ajustées \\(\\widehat{Y}_i\\). Theorem 3.3 Soit \\(\\widehat{\\theta}\\) l’estimateur de \\(\\theta\\) par la méthode des moindres carrés. Sous les hypothèses H1-H4, et si \\(X \\in \\mathcal{M}_{nk}(\\mathbb{R})\\), alors \\[ \\widehat{\\sigma}^2 = \\frac{ \\|\\widehat{\\varepsilon}\\|^2}{n-k} = \\frac{ \\|Y - \\widehat{Y}\\|^2}{n-k}=\\frac{ \\| Y - X\\widehat{\\theta} \\|^2}{n-k} = \\frac{SSR(\\widehat{\\theta})}{n-k}\\] est un estimateur sans biais optimal de \\(\\sigma^2\\), indépendant de \\(\\widehat{\\theta}\\). De plus, \\[ \\widehat{\\sigma}^2 \\sim \\frac{\\sigma^2}{n-k}\\ \\chi^2(n-k).\\] Exercise 3.3 Preuve du théorème 3.3 Montrez que \\(SSR(\\widehat{\\theta}) : = \\|Y - X\\widehat{\\theta}\\|^2 = \\|P_{[X]^{\\perp}}\\varepsilon\\|^2\\) A l’aide du théorème de Cochran, montrez que \\(SSR(\\widehat{\\theta})\\sim \\sigma^2 \\chi^2(n-k)\\). Déduisez-en que \\(\\widehat{\\sigma}^2\\) est un estimateur sans biais de \\(\\sigma^2\\). Comme \\(X\\widehat{\\theta} = X\\theta + P_{[X]}\\varepsilon\\), montrez que \\(X\\widehat{\\theta}\\) et \\(\\widehat{\\sigma}^2\\) sont indépendants. Déduisez-en que \\(\\widehat{\\theta}\\) et \\(\\widehat{\\sigma}^2\\) sont indépendants. L’estimation de \\(\\sigma^2\\) est donc \\[(\\widehat{\\sigma}^2)^{obs}=\\frac{ \\| (\\widehat{\\varepsilon})^{obs} \\|^2}{n-k} =\\frac{ \\| y - \\widehat{y}\\|^2}{n-k}.\\] Le dénominateur \\((n-k)\\) provient du fait que l’on a déjà estimé \\(k\\) paramètres dans le modèle. 3.4 Erreurs standards On va ici s’interesser aux erreurs standard associées à \\(\\hat{\\theta_j}\\), \\(\\hat{Y_i}\\) et \\(\\hat{\\varepsilon_i}\\). La matrice de variance-covariance de \\(\\widehat{ \\theta}\\) notée \\(\\Gamma_{\\widehat{\\theta}}=\\sigma^2(X&#39;X)^{-1}\\) est estimée par \\[\\widehat{\\Gamma_{\\widehat{\\theta}}}=\\widehat{\\sigma}^2(X&#39;X)^{-1}.\\] Ainsi \\(Var\\left(\\widehat{\\theta_j}\\right)\\) est estimée par \\(\\widehat{\\sigma}^2[(X&#39;X)^{-1}]_{jj}\\). Par conséquent, l’erreur standard de \\(\\widehat{ \\theta}_j\\), notée \\(se_j\\), vaut \\[se_j=\\sqrt{\\widehat{\\sigma}^2[(X&#39;X)^{-1}]_{jj}}.\\] La matrice des corrélations de \\(\\widehat{\\theta}\\) a pour élément \\(j,j&#39;\\) : \\[r(\\widehat{\\theta}_j, \\widehat{\\theta}_{j&#39;}) = \\frac{\\widehat{\\sigma}^2[(X&#39;X)^{-1}]_{jj&#39;}}{se_j \\times se_{j&#39;}} = \\frac{ [(X&#39;X)^{-1}]_{jj&#39;}}{\\sqrt{[(X&#39;X)^{-1}]_{jj}[(X&#39;X)^{-1}]_{j&#39;j&#39;}}}.\\] La variance \\(Var(\\widehat{Y}) = \\sigma^2\\ P_{[X]}=\\sigma^2 X(X&#39;X)^{-1}X&#39;\\) est estimée par \\(\\widehat{\\sigma}^2 P_{[X]}\\). Par conséquent, \\(\\sqrt{\\widehat{\\sigma}^2\\ (P_{[X]})_{ii}}\\) est l’erreur standard de \\(\\widehat{Y_i}\\). De même, \\(\\sqrt{\\widehat{\\sigma}^2(1-(P_{[X]})_{ii})}\\) correspond à l’erreur de \\(\\widehat{\\varepsilon_i}\\). Ainsi \\(\\widehat{\\varepsilon_i} / \\sqrt{\\widehat{\\sigma^2}}\\) désigne le résidu standardisé et \\(\\widehat{\\varepsilon_i} / \\sqrt{\\widehat{\\sigma}^2(1-(P_{[X]})_{ii})}\\) désigne le résidu studentisé. 3.5 Intervalle de confiance de \\(\\theta_j\\), de \\((X\\theta)_i\\) et de \\(X_0\\theta\\) 3.5.1 Intervalle de confiance de \\(\\theta_j\\) Sachant que \\(\\widehat{\\theta} \\sim \\mathcal{N}_k(\\theta,\\sigma^2 (X&#39;X)^{-1})\\), on a \\(\\widehat{\\theta_j} \\sim \\mathcal{N}(\\theta_j,\\sigma^2 [(X&#39;X)^{-1}]_{jj})\\). Par conséquent \\[ \\frac{\\widehat{\\theta_j}-\\theta_j}{\\sqrt{\\sigma^2[(X&#39;X)^{-1}]_{jj}}}\\sim\\mathcal{N}(0,1)\\] et la variable aléatoire \\((n-k)\\widehat{\\sigma}^2 / \\sigma^2\\) est distribuée selon une loi \\(\\chi^2(n-k)\\). D’après le théorème de Cochran, \\(\\widehat{\\theta}\\) et \\(\\widehat{\\sigma}^2\\) étant indépendantes, on en déduit que \\[T=\\frac{\\widehat{\\theta_j}-\\theta_j}{\\sqrt{\\sigma^2[(X&#39;X)^{-1}]_{jj}}} \\Big / \\sqrt{\\frac{(n-k)\\widehat{\\sigma}^2}{(n-k)\\sigma^2}} = \\frac{\\widehat{\\theta_j}-\\theta_j}{\\sqrt{\\widehat{\\sigma}^2[(X&#39;X)^{-1}]_{jj}}}\\sim \\mathcal{T}(n-k).\\] Si on note \\(t_{n-k,1-\\frac{\\alpha}{2}}\\) le \\((1-\\alpha/2)\\)-quantile de la loi de Student à \\((n-k)\\) ddl, alors \\[ \\mathbb{P}\\left(\\left|\\frac{\\widehat{\\theta_j}-\\theta_j}{\\sqrt{\\widehat{\\sigma}^2[(X&#39;X)^{-1}]_{jj}}}\\right|\\leq t_{n-k,1-\\frac{\\alpha}{2}}\\right)=1-\\alpha. \\] Ainsi, l’intervalle de confiance du paramètre \\(\\theta_j\\) de sécurité \\(1-\\alpha\\) est défini par : \\[IC_{1-\\alpha}(\\theta_j)=\\left[\\widehat{\\theta_j}\\pm t_{n-k,1-\\frac{\\alpha}{2}}\\sqrt{\\widehat{\\sigma}^2[(X&#39;X)^{-1}]_{jj}}\\right] = \\left[\\widehat{\\theta_j}\\pm t_{n-k,1-\\frac{\\alpha}{2}}\\ se_j\\right].\\] 3.5.2 Intervalle de confiance de \\((X\\theta)_i\\) Soit \\(\\mathbb{E}[Y_i] =(X\\theta)_i\\) la réponse moyenne de \\(Y_i\\). On l’estime par \\(\\displaystyle \\widehat{Y_i}=(X\\widehat{\\theta})_i\\). Puisque \\(\\widehat{\\theta} \\sim \\mathcal{N}_{k}(\\theta,\\sigma^2 (X&#39;X)^{-1})\\), d’après les propriétés des vecteurs gaussiens (Corollaire A.1), la loi de \\(\\widehat{Y_i}\\) est \\(\\mathcal N\\left((X\\theta)_i, \\sigma^2 [X(X&#39;X)^{-1}X&#39;]_{ii}\\right)\\). De plus, \\((n-k) \\hat \\sigma^2 \\sim \\sigma^2 \\chi^2(n-k)\\) et \\(\\hat \\theta\\) et \\(\\hat \\sigma^2\\) sont indépendants. On obtient donc que \\[ \\frac{\\widehat Y_i - (X\\theta)_i}{\\sqrt{ \\widehat\\sigma^2 [X(X&#39;X)^{-1}X&#39;]_{ii}}} \\sim \\mathcal T(n-k). \\] L’intervalle de confiance de \\((X\\theta)_i\\) au niveau de confiance de \\(1-\\alpha\\) est donc donné par : \\[ IC_{1-\\alpha}((X\\theta)_i) = \\left[\\widehat{Y_i}\\pm t_{n-k,1-\\alpha/2} \\times \\sqrt{ \\widehat\\sigma^2 [X(X&#39;X)^{-1}X&#39;]_{ii}}\\right]. \\] 3.5.3 Intervalle de confiance de \\(X_0\\theta\\) On considère des nouvelles valeurs pour les variables explicatives, rassemblées dans le vecteur ligne \\(X_{0} \\in \\mathcal{M}_{1k}(\\mathbb{R})\\). \\(X_0\\theta\\) représente alors la réponse moyenne de ce nouvel individu. \\(X_0\\theta\\) est estimé par \\(\\widehat{Y_0}=X_0\\widehat{\\theta}\\). Puisque \\(\\widehat{\\theta} \\sim \\mathcal{N}_{k}(\\theta,\\sigma^2 (X&#39;X)^{-1})\\), d’après les propriétés des vecteurs gaussiens (Théorème ??), la loi de cet estimateur est \\[\\widehat{Y_0}=X_0\\widehat{\\theta}\\sim \\mathcal{N}(X_0\\theta,\\sigma^2 X_0(X&#39;X)^{-1}X&#39;_0).\\] De plus, \\((n-k) \\hat\\sigma^2 \\sim \\sigma^2 \\chi^2(n-k)\\) et \\(\\hat \\theta\\) et \\(\\hat \\sigma^2\\) sont indépendants par le théorème de Cochran. Ainsi l’intervalle de confiance de \\(X_0\\theta\\) au niveau de confiance de \\(1-\\alpha\\) s’écrit : \\[ IC_{1-\\alpha}(X_0\\theta) = \\left[\\widehat{Y_0}\\pm t_{n-k,1-\\alpha/2} \\times \\sqrt{\\widehat{\\sigma}^2X_0(X&#39;X)^{-1}X&#39;_0}\\right]. \\] 3.6 Intervalles de prédiction Avant toute chose, il est important de comprendre la différence entre l’intervalle de confiance de \\(X_0\\theta\\) et l’intervalle de prédiction. Dans les deux cas, on suppose un nouveau jeu de valeurs pour les variables explicatives donnant le vecteur ligne \\(X_0\\). Dans le premier cas, on veut prédire une réponse moyenne correspondant à ces variables explicatives alors que dans le second cas, on cherche à prédire une nouvelle valeur “individuelle”. Par exemple, si on étudie la liaison entre le poids et l’âge d’un animal, on peut prédire la valeur du poids à 20 jours soit comme le poids moyen d’animaux à 20 jours, soit comme le poids à 20 jours d’un nouvel animal. Pour le nouvel animal, on doit prendre en compte la variabilité individuelle, ce qui augmente la variance de l’estimateur et donc la largeur de l’intervalle. Si on veut prédire dans quel intervalle se trouvera le résultat d’un nouvel essai \\(X_0\\in \\mathcal{M}_{1k}(\\mathbb{R})\\), on doit tenir compte de deux facteurs d’incertitude : l’incertitude sur l’estimation du résultat moyen\\(X_0\\theta\\), l’incertitude sur le terme d’erreur \\(\\varepsilon_0\\) du nouvel individu. Le vecteur de paramètres \\(\\theta\\) est estimé par \\[\\widehat{\\theta}=(X&#39;X)^{-1}X&#39;Y\\] où \\(Y=(Y_1,\\ldots,Y_n)&#39;\\). Une nouvelle réponse \\(Y_0\\), correspondant à \\(X_0\\), s’écrit : \\[ Y_0=X_0\\theta+\\varepsilon_0, \\] où \\(\\varepsilon_0\\) est supposé indépendant des \\(\\varepsilon_i, \\, 1 \\leq i \\leq n\\) et \\(\\varepsilon_0 \\sim \\mathcal{N}(0,\\sigma^2)\\). Le modèle linéaire prédit la valeur \\[ \\widehat{Y_0}=X_0\\widehat{\\theta} \\sim \\mathcal{N}(X_0\\theta, \\sigma^2X_0(X&#39;X)^{-1}X&#39;_0). \\] D’après les hypothèses sur \\(\\varepsilon_0\\), on a que \\(Y_0\\sim\\mathcal{N}(X_0\\theta,\\sigma^2)\\) et \\(Y_0\\) est indépendant de \\(\\hat Y_0\\). On a donc \\[Y_0-\\widehat{Y_0} \\sim \\mathcal{N}(0,\\sigma^2\\left(1+X_0(X&#39;X)^{-1}X&#39;_0\\right)).\\] Par ailleurs, d’après le Théorème 3.3 \\[ \\widehat{\\sigma}^2= \\frac{1}{n-k}\\sum_{i=1}^n (Y_i-X\\widehat{\\theta})^2 \\sim \\frac{\\sigma^2}{n-k}\\chi^2(n-k) \\] et comme \\(\\widehat\\sigma^2\\) est indépendant de \\(\\widehat{\\theta}\\) et de \\(\\varepsilon_0\\) (car \\(\\varepsilon_0\\) indépendant des \\(\\varepsilon_i\\)), la variable aléatoire \\[ \\frac{ Y_0-\\widehat{Y_0}}{\\widehat\\sigma \\sqrt{1+X_0(X&#39;X)^{-1}X&#39;_0}} \\sim \\mathcal{T}(n-k). \\] Au final, en notant \\(t_{n-k,1-\\alpha/2}\\) le \\(1-\\alpha/2\\) quantile d’une loi de Student à \\(n-k\\) degrés de liberté, on obtient \\[ \\mathbb{P}\\left(Y_0\\in \\left[ \\widehat{Y_0} \\pm t_{n-k,1-\\alpha/2}\\widehat\\sigma \\sqrt{1+X_0(X&#39;X)^{-1}X&#39;_0} \\right]\\right) = 1-\\alpha. \\] Par conséquent, l’intervalle de prédiction pour une nouvelle observation au point \\(X_0\\) est défini par \\[IC_{1-\\alpha}(Y_0)=\\left[\\ \\widehat{Y_0} \\pm t_{n-k,1-\\alpha/2}\\times\\widehat\\sigma \\sqrt{1+X_0(X&#39;X)^{-1}X&#39;_0}\\ \\right].\\] Notez bien la différence entre \\(IC_{1-\\alpha}(Y_0)\\) et \\[ IC_{1-\\alpha}(X_0\\theta) = \\left[\\widehat{Y_0}\\pm t_{n-k,1-\\alpha/2} \\times \\widehat{\\sigma} \\sqrt{X_0(X&#39;X)^{-1}X&#39;_0}\\right]. \\] 3.7 Qualité d’ajustement La mise en oeuvre d’un modèle linéaire a pour objectif d’expliquer la variabilité d’une variable \\(Y\\) par d’autres variables. On note : \\(\\displaystyle SST=\\|Y - \\bar Y \\mathbb{1}_n\\|^2 = \\sum_{i=1}^n(Y_i-\\overline Y)^2= n\\ var(Y)\\) la variabilité totale de \\(Y\\) (Total sum of squares). \\(\\displaystyle SSE = \\|\\widehat{Y} - \\bar Y \\mathbb{1}_n\\|^2 =\\sum_{i=1}^n(\\widehat{Y_i}-\\overline Y)^2=n\\ var(\\widehat{Y})\\) la variabilité expliquée par le modèle, c’est-à-dire par les prédicteurs (Explained sum of squares). \\(\\displaystyle SSR= \\|Y - \\widehat{Y}\\|^2 = \\sum_{i=1}^n(\\widehat{\\varepsilon_i})^2=\\sum_{i=1}^n (Y_i-\\widehat{Y_i})^2 = n\\ var(\\widehat{\\varepsilon})\\) la variabilité résiduelle non expliquée par le modèle (Residual sum of squares). La variance totale de \\(Y\\) admet alors la décomposition suivante : \\[var(Y)=var(\\widehat{Y})+ var(\\widehat{\\varepsilon})\\] c’est-à-dire \\[SST= SSE + SSR.\\] Exercise 3.4 Démontrez ce résultat avec Pythagore. On verra par la suite que selon le modèle étudié, cette décomposition amène à des définitions spécifiques à chaque modèle. D’après le critère des moindres carrés utilisé pour estimer les paramètres, on cherche à minimiser \\(SSR\\) et donc à maximiser \\(SSE\\). Pour juger de la qualité d’ajustement du modèle aux données, on définit donc le critère \\(R^2\\) suivant. Definition 3.2 On appelle coefficient \\(R^2\\) la quantité suivante \\[R^2=\\frac{SCE}{SCT}=\\frac{var(\\widehat{Y})}{var(Y)}\\in[0,1]\\] Il représente la part de variance de \\(Y\\) expliquée par le modèle : Plus \\(R^2\\) est proche de \\(1\\), plus le modèle s’ajuste aux données. Nous discuterons de l’efficacité de ce critère dans les chapitres suivants. 3.8 En résumé Dans le cadre d’un modèle linéaire régulier, \\(\\widehat{\\theta} = (X&#39;X)^{-1} X&#39; Y \\sim \\mathcal{N}_k(\\theta,\\sigma^2 (X&#39;X)^{-1})\\) \\(\\widehat{\\sigma^2} = \\frac{\\|Y - X \\widehat{\\theta}\\|^2}{n-k} \\sim \\frac{\\sigma^2}{n-k}\\chi^2(n-k)\\) \\(\\widehat{\\theta}\\) et \\(\\widehat{\\sigma}^2\\) sont indépendants Connaitre les définitions de valeurs ajustées \\(\\widehat{Y} = X\\widehat{\\theta}=P_{[X]}Y\\) et de résidus \\(\\widehat{\\varepsilon}=Y-\\widehat{Y}\\) Savoir refaire la construction d’un IC pour un paramètre d’un IC pour une réponse moyenne d’un intervalle de prédiction pour une nouvelle réponse Surtout, ne pas apprendre par coeur les formules ! - Décomposition de la variance \\[ \\underbrace{\\|Y - \\bar Y \\mathbb{1}_n\\|^2}_{SST} = \\underbrace{\\|Y - \\widehat{Y}\\|^2}_{SSR} + \\underbrace{\\|\\widehat{Y} - \\bar Y \\mathbb{1}_n\\|^2}_{SSE} \\] et \\(R^2 = \\frac{SSE}{SST}\\). "],["Test.html", "Chapitre 4 Test de Fisher-Snedecor 4.1 Hypothèses testées 4.2 Le test de Fisher-Snedecor 4.3 Intervalle (région) de confiance pour \\(C\\theta\\) 4.4 En résumé", " Chapitre 4 Test de Fisher-Snedecor Nous allons nous intéresser dans ce chapitre à un certain nombre de tests pouvant être mis en oeuvre sur le modèle linéaire. Nous supposerons pendant toute cette partie que les hypothèses H1-H4 sont vérifiées. Les tests présentés ci-dessous ne peuvent être utilisés si ces hypothèses ne sont pas satisfaites. 4.1 Hypothèses testées On considère un modèle linéaire gaussien \\[\\begin{equation} \\tag{4.1} Y=X\\theta +\\varepsilon \\, \\mbox{ avec } \\varepsilon \\sim \\mathcal{N}_n\\left(0_n,\\sigma^2 I_n\\right) \\end{equation}\\] et on s’intéresse à examiner la nullité de certaines composantes du paramètre \\(\\theta\\) ou de certaines combinaisons linéaires des composantes de \\(\\theta\\), par exemple : \\(\\theta_j=0 \\, ; \\, \\theta_j=\\theta_k=0\\) ou \\(\\theta_j=\\theta_k\\). Ces hypothèses reposent sur la notion de modèles emboîtés : deux modèles sont dits emboîtés si l’un peut être considéré comme un cas particulier de l’autre. Cela revient à comparer un modèle de référence à un modèle réduit ou contraint. Cette approche vise donc à déterminer si le modèle utilisé peut être oui ou non simplifié. Voici deux exemples de sous-modèles : \\[ \\begin{array}{ll} \\mbox{Modèle général de la régression linéaire simple : } &amp; Y_i=a+bX_i+\\varepsilon_i\\\\ \\mbox{Sous-modèle avec nullité de la pente : } &amp; Y_i=a+\\varepsilon_i\\\\ &amp; \\\\ \\mbox{Modèle général de l&#39;analyse de variance à 1 facteur : } &amp; Y_{ij}=\\mu_i+\\varepsilon_{ij}\\\\ \\mbox{Sous-modèle avec égalité des groupes : } &amp; Y_{ij}=\\mu+\\varepsilon_{ij} \\end{array} \\] Par la suite, nous allons considérer deux écritures équivalentes de l’hypothèse nulle \\(\\mathcal{H}_0\\). 4.1.1 Première écriture Pour spécifier la nullité de certaines composantes du paramètre \\(\\theta\\), on introduit la matrice \\(C \\in \\mathcal{M}_{qk}(\\mathbb{R})\\) où \\(k\\) désigne le nombre de paramètres du modèle de référence et \\(q\\) le nombre de contraintes testées \\((1 \\leq q \\leq k)\\) telle que \\[\\mathcal{H}_0 : C\\theta = 0_{q}.\\] La matrice \\(C\\) sera supposée être de rang \\(q\\). Example 4.1 On suppose un modèle à \\(k=3\\) paramètres. Voici trois exemples : Tester l’hypothèse \\(\\mathcal{H}_0 : \\theta_2=0\\) revient à poser \\(\\mathcal{H}_0 : C&#39;\\theta=0\\) avec \\(C&#39;=\\left(\\begin{array}{ccc} 0 &amp; 1 &amp; 0 \\end{array} \\right)\\) et \\(q=1\\). Tester l’hypothèse \\(\\mathcal{H}_0 : \\theta_3=\\theta_2\\) revient à poser \\(\\mathcal{H}_0 : C&#39;\\theta= 0\\) avec \\(C&#39;= \\left(\\begin{array}{ccc} 0 &amp; -1 &amp; 1 \\end{array} \\right)\\) ou \\(C&#39;=\\left(\\begin{array}{ccc} 0 &amp; 1 &amp; -1 \\end{array} \\right)\\) et \\(q=1\\). Tester l’hypothèse \\(\\mathcal{H}_0 : \\theta_3=\\theta_2=0\\) revient à poser \\(\\mathcal{H}_0 : C&#39;\\theta=0_{2}\\) avec \\(\\displaystyle C&#39;=\\left(\\begin{array}{ccc} 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{array} \\right)\\) et \\(q=2\\). 4.1.2 Seconde écriture Plaçons-nous dans le cadre général du modèle linéaire. Soit le modèle (4.1) et soit \\(Z\\) une matrice telle que \\(Im(Z) \\subset Im(X)\\) et \\(k_0=dim( Im(Z) ) &lt;k=dim( Im (X))\\). Le modèle défini par \\[\\begin{equation} Y= Z \\beta +\\varepsilon, \\tag{4.2} \\end{equation}\\] est appelé sous-modèle issu du modèle linéaire défini en (4.1). Le plus souvent, \\(Z\\) est la matrice constituée de \\(k_0\\) vecteurs colonnes de \\(X\\) avec \\(k_0&lt;k\\) et \\(\\beta\\) est un vecteur de longueur \\(k_0\\). Nous notons alors \\(SSR_0\\) la somme des carrés des résidus de ce sous-modèle, associée à \\(n-k_0\\) degrés de liberté et définie de la façon suivante \\[SSR_0=\\|Y-Z\\widehat{\\beta}\\|^2,\\] où \\(\\widehat{\\beta}\\) est l’estimateur des moindres carrés issus du modèle (4.2) pour \\(\\beta\\). Dans la mesure où \\(Im(Z) \\subset Im(X)\\) et par définition des estimateurs des moindre carrés, nous pouvons remarquer que \\(SSR_0 \\geq SSR\\). Il peut être parfois intéressant d’essayer de savoir si les observations sont issues du modèle (4.1) ou (4.2). Soit le modèle défini par : \\[Y= R+ \\varepsilon.\\] Tester la présence d’un sous-modèle revient donc à tester : \\[ \\mathcal{H}_0 : R \\in Im (Z) \\textrm{ contre } \\mathcal{H}_1 : R \\in Im(X) \\backslash Im(Z). \\] 4.2 Le test de Fisher-Snedecor 4.2.1 Principe Le test de Fisher-Snedecor est la règle de décision qui permet de décider si on rejette ou ne rejette pas \\(\\displaystyle \\mathcal{H}_0 : C\\theta =0_{q}\\), c’est-à-dire \\(\\mathcal{H}_0: R \\in Im(Z)\\) : Rejeter \\(\\mathcal{H}_0\\), c’est décider que \\(C\\theta \\neq 0_{q}\\), c’est-à-dire que certaines composantes de \\(C\\theta\\) ne sont pas nulles. Nous n’avons donc pas confiance dans le sous-modèle et nous préfèrerons continuer à travailler avec le modèle de référence. Ne pas rejeter \\(\\mathcal{H}_0\\), c’est ne pas exclure que toutes les composantes de \\(C\\theta\\) sont nulles. Dans ce cas, il n’est pas nécessaire de conserver un modèle trop compliqué et nous préfèrerons conserver le modèle contraint pour expliquer les données. 4.2.2 La statistique de test Theorem 4.1 Dans le cadre du modèle linéaire général (4.1) avec les hypothèses H1-H4 et les notations précédentes, sous l’hypothèse nulle \\(\\mathcal{H}_0\\) (le sous-modèle (4.2) est vrai), la variable \\[F =\\frac{(SSR_0- SSR)/(k-k_0)}{SSR/(n-k)} = \\frac{\\|X\\widehat{\\theta} - Z\\widehat{\\beta}\\|^2 / (k-k_0)}{\\|Y - X\\widehat \\theta\\|^2 / (n-k)} \\underset{\\mathcal{H}_0}{\\sim} \\mathcal{F}(k-k_0,n-k)\\] (loi de Fisher de paramètres \\((k-k_0,n-k)\\)). De plus, \\(F\\) est indépendante de \\(Z\\widehat{\\beta}\\) (calculé sous l’hypothèse \\(\\mathcal{H}_0\\)). Exercise 4.1 Le but de l’exercice est de démontrer le théorème 4.1. Montrez que \\(SSR = \\|P_{[X]^\\perp}\\varepsilon\\|^2 \\sim \\sigma^2 \\chi^2(n-k)\\) Soit \\(A\\) un sous-espace vectoriel de \\(Im(X)=[X]\\) tel que \\(A \\stackrel{\\perp}{\\oplus} Im(Z) = Im(X)\\), \\(dim(A)=k-k_0\\). Montrez que \\(SSR_0-SSR = \\| P_{A}\\varepsilon\\|^2 \\underset{\\mathcal{H}_0}{\\sim} \\sigma^2 \\chi^2(k-k_0)\\) Déduisez-en que \\(F\\underset{\\mathcal{H}_0}{\\sim} \\mathcal{F}(k-k_0,n-k)\\). Montrez que \\(F\\) est indépendante de \\(Z \\widehat{\\beta}\\) et \\(\\widehat{\\beta}\\). Cette statistique de test peut s’écrire sous une autre forme donnée dans la proposition suivante : Proposition 4.1 En suivante la première écriture, la statistique de test de Fisher-Snedecor peut également s’écrire sous la forme suivante : \\[ F=\\frac{ [C\\widehat{\\theta}]&#39; \\left[C(X&#39;X)^{-1}C&#39;\\right]^{-1} [C\\widehat{\\theta}] }{q\\widehat{\\sigma^2}} \\textrm{ avec } q=k-k_0. \\] Proof. La preuve de cette proposition est donnée en annexe B.1. Cette dernière expression a l’avantage de ne pas nécessiter l’estimation du modèle contraint pour tester \\(\\mathcal{H}_0 : C\\theta=0_{q}\\) contre \\(\\mathcal{H}_1 : C\\theta \\neq 0_{q}.\\) Par la suite, on notera \\(F^{obs}\\) la valeur observée de la statistique de test \\(F\\). 4.2.3 Règle de décision La quantité d’importance dans notre construction du test de Fisher est \\(SSR_0- SSR\\). Intuitivement, si la valeur observée de \\(SSR_0- SSR\\) est très grande, il y a peu de chance que les observations \\(Y\\) soient “issues” du sous-modèle. À l’opposé, si la valeur observée \\(SSR_0- SSR\\) est petite, il est fort possible que le modèle initial puisse être simplifié : le sous-modèle explique aussi bien les observations dans la mesure où \\(SSR_0\\) est comparable à \\(SSR\\). Par conséquent, la zone de rejet avec un risque de première espèce \\(\\alpha\\) s’écrit \\[ \\mathcal{R}_\\alpha = \\{ F &gt; f_{q,n-k,1-\\alpha}\\} \\] où \\(f_{q,n-k,1-\\alpha}\\) est le \\((1-\\alpha)\\)-quantile de la distribution de Fisher de degrés de liberté \\(q=k-k_0\\) et \\(n-k\\). 4.2.4 Cas particulier où \\(q=1\\) : Test de Student Dans le cas particulier où l’on teste la nullité d’une seule combinaison linéaire des composantes du paramètre \\(\\theta\\), i.e. \\(q=1\\) et \\(C \\in \\mathcal{M}_{1,k}(\\mathbb{R})\\), alors l’hypothèse nulle s’écrit : \\[\\mathcal{H}_0 : C\\theta=0.\\] On a donc \\(C(X&#39;X)^{-1}C&#39; \\in \\mathbb{R}\\) et la variable aléatoire \\(F\\) s’écrit alors de la façon suivante : \\[ F=\\frac{(C\\widehat{\\theta})^2}{\\widehat{\\sigma}^2 C(X&#39;X)^{-1}C&#39;}. \\] \\(F\\) suit une loi de Fisher à 1 et \\(n-k\\) degrés de liberté. Or une propriété de la distribution de Fisher-Snedecor est qu’une distribution de Fisher-Snedecor à 1 et \\(m_2\\) degrés de liberté est le carré d’une distribution de Student à \\(m_2\\) degrés de liberté. Par conséquent, on obtient l’égalité suivante : si \\(A\\sim \\mathcal{F}(1,n-k)\\) et \\(T\\sim \\mathcal{T}(n-k)\\), \\[\\mathbb{P}\\left(A \\geq f_{1,n-k,1-\\alpha}\\right)=\\alpha=\\mathbb{P}\\left(T^2\\geq f_{1,n-k,1-\\alpha}\\right).\\] On en déduit donc la propriété suivante sur les quantiles : \\[f_{1,n-k,1-\\alpha}=t_{n-k,1-\\alpha/2}^2.\\] Selon le test de Fisher, on rejette l’hypothèse \\(\\mathcal{H}_0\\) si \\(F \\geq f_{1,n-k,1-\\alpha}\\). Or on a les équivalences suivantes : \\[\\begin{eqnarray*} F \\leq f_{1,n-k,1-\\alpha} &amp;\\Longleftrightarrow&amp; |C\\widehat{\\theta}| \\leq t_{n-k,1-\\alpha/2} \\sqrt{\\widehat{\\sigma}^2C(X&#39;X)^{-1}C&#39;} \\\\ &amp;\\Longleftrightarrow&amp; -t_{n-k,1-\\alpha/2} \\sqrt{\\widehat{\\sigma}^2C(X&#39;X)^{-1}C&#39;} \\leq C \\widehat{\\theta} \\leq t_{n-k,1-\\alpha/2} \\sqrt{\\widehat{\\sigma}^2C(X&#39;X)^{-1}C&#39;}. \\end{eqnarray*}\\] Donc au final, on rejette \\(\\mathcal{H}_0\\) si \\(\\left| \\frac{C\\widehat{\\theta}} {\\sqrt{\\widehat{\\sigma}^2C(X&#39;X)^{-1}C&#39;}}\\right| &gt; t_{n-k,1-\\alpha/2}\\). On retrouve le classique test de Student de nullité. Exercise 4.2 Construisez directement le test de Student de nullité du paramètre \\(\\theta_j\\) au niveau \\(\\alpha\\). 4.3 Intervalle (région) de confiance pour \\(C\\theta\\) 4.3.1 IC pour \\(C\\theta \\in \\mathbb{R}\\) Commençons par l’intervalle de confiance pour une combinaison linéaire \\(C\\theta \\in \\mathbb{R}\\). Nous reprenons les notations de la section 4.2.4. Comme \\(\\widehat{\\theta}\\sim\\mathcal{N}_k(\\theta,\\sigma^2 (X&#39;X)^{-1})\\), on a \\(C\\widehat{\\theta} \\sim\\mathcal{N}(C \\theta,\\sigma^2 \\Delta)\\) avec \\(\\Delta=C(X&#39;X)^{-1} C&#39; \\in\\mathbb{R}\\). De plus \\((n-k)\\widehat{\\sigma}^2 / \\sigma^2 \\sim \\chi^2(n-k)\\) et \\(\\widehat{\\theta}\\) et \\(\\widehat{\\sigma}^2\\) sont indépendantes. Ainsi, \\[ \\frac{C\\widehat{\\theta} - C\\theta}{\\widehat{\\sigma} \\sqrt{\\Delta}} \\sim \\mathcal{T}(n-k). \\] On obtient ainsi l’intervalle de confiance suivant au niveau de confiance \\(1-\\alpha\\) : \\[ IC_{1-\\alpha}(C\\theta)=\\left[C\\widehat{\\theta} \\pm t_{n-k,1-\\alpha/2}\\sqrt{\\widehat{\\sigma}^2 C (X&#39;X)^{-1} C&#39;}\\right]. \\] Rappelons le lien entre test et intervalle de confiance : l’ensemble des \\(c_0\\) acceptés pour un test \\[ \\mathcal{H}_0 : C\\theta = c_0 \\textrm{ contre } \\mathcal{H}_1 : C\\theta \\neq c_0 \\] au niveau \\(\\alpha\\), définit un intervalle de confiance au niveau de confiance \\(1-\\alpha\\). 4.3.2 Région de confiance pour \\(C\\theta \\in \\mathbb{R}^q\\) Si maintenant, comme dans la partie 4.2.2, \\(C\\theta\\) est de dimension \\(q&gt;1\\) et si \\(c_0\\) est une valeur particulière appartenant à \\(\\mathbb{R}^q\\), nous pouvons généraliser la construction de l’intervalle de confiance. Dans ce cas, \\(C\\widehat{\\theta} - C\\theta \\sim \\mathcal{N}_q(0_q,\\sigma^2 \\Delta)\\) avec \\(\\Delta = C(X&#39;X)^{-1} C&#39; \\in \\mathcal{M}_q(\\mathbb{R})\\). Ainsi \\[ \\frac{[C\\widehat{\\theta} - C\\theta] &#39; \\Delta^{-1} [C\\widehat{\\theta} - C\\theta]}{\\sigma^2} \\sim \\chi^2(q). \\] On a aussi \\((n-k)\\widehat{\\sigma}^2 / \\sigma^2 \\sim \\chi^2(n-k)\\) et les deux statistiques sont indépendantes. On en déduit donc que \\[ A:= \\frac{[C\\widehat{\\theta} - C\\theta] &#39; \\Delta^{-1} [C\\widehat{\\theta} - C\\theta]}{q\\ \\widehat{\\sigma}^2} \\sim \\mathcal{F}(q,n-k). \\] Finalement, \\[\\begin{eqnarray*} &amp; &amp;\\mathbb{P}(A\\leq f_{q,n-k,1-\\alpha} ) = 1-\\alpha\\\\ \\Leftrightarrow&amp; &amp;\\mathbb{P}([C\\widehat{\\theta} - C\\theta] &#39; \\Delta^{-1} [C\\widehat{\\theta} - C\\theta] \\leq q \\widehat{\\sigma}^2 f_{q,n-k,1-\\alpha} ) = 1-\\alpha\\\\ \\Leftrightarrow&amp; &amp; \\mathbb{P}(C\\theta \\in RC) = 1 - \\alpha \\end{eqnarray*}\\] où \\(RC\\) est l’ellipsoïde de confiance défini par : \\[ RC= \\left\\{u \\in \\mathbb{R}^q; \\, (C \\widehat{\\theta} -u )&#39; [C (X&#39; X)^{-1} C&#39;]^{-1} (C \\widehat{\\theta}- u) \\leq q \\widehat{\\sigma}^2 f_{q,n-k,1-\\alpha}\\right\\}.\\] L’ensemble des \\(c_0\\in\\mathbb{R}^q\\) acceptés par le test \\[ \\mathcal{H}_0 : C \\theta=c_0 \\textrm{ contre } \\mathcal{H}_1 : C \\theta \\neq c_0 \\] au niveau \\(\\alpha\\) forme l’ellipsoïde de confiance \\(RC\\) défini ci-dessus. 4.4 En résumé Savoir écrire les hypothèses d’un test de Fisher de sous-modèle Savoir justifier qu’un modèle est sous-modèle d’un autre Connaitre la forme de la statistique du test de Fisher, sa loi sous \\(\\mathcal{H}_0\\) et savoir définir les quantités qui la composent selon le contexte (Théorème 4.1) Savoir mener la construction d’un test de Fisher de sous-modèle Savoir mener la construction d’un test de Student quand \\(q=1\\) Savoir mener la construction d’un intervalle de confiance pour \\(C\\theta\\). Ne pas apprendre la formule ! "],["singulier.html", "Chapitre 5 Modèles singuliers, orthogonalité et importance des hypothèses sur les erreurs 5.1 Quand H1-H4 ne sont pas respectées… 5.2 Modèles singuliers 5.3 Orthogonalité 5.4 En résumé", " Chapitre 5 Modèles singuliers, orthogonalité et importance des hypothèses sur les erreurs 5.1 Quand H1-H4 ne sont pas respectées… L’hypothèse de gaussianité des erreurs est la plus difficile à vérifier en pratique. Les tests classiques de normalité (test de Kolmogorov-Smirnov, Cramer-Von Mises, Anderson-Darling ou de Shapiro-Wilks) demanderaient l’observation des erreurs \\(\\varepsilon_i\\) elles-mêmes ; ils perdent beaucoup de leur puissance quand ils sont appliqués sur les résidus \\(\\widehat{\\varepsilon_i}=Y_i-\\widehat{Y_i}\\), notamment en raison du fait que ces résidus ne sont pas indépendants. Nous pouvons cependant toujours faire des droites de Henry ou des QQ-plots pour mettre en évidence des écarts évidents. Il n’en reste pas moins que l’hypothèse de gaussianité sera le plus souvent un credo que nous ne pourrons pas vraiment vérifier expérimentalement. Fort heureusement, il existe une théorie asymptotique (donc de grands échantillons) du modèle linéaire qui n’a pas besoin de cette hypothèse. Comme il est dit dans la section 2.1, c’est dans cette optique là qu’il faut réellement penser le modèle linéaire. 5.1.1 Propriétés de l’estimateur des moindres carrés \\(\\widehat{\\theta}\\) Proposition 5.1 Soit \\(\\widehat{\\theta} =(X&#39;X)^{-1}X&#39;Y.\\) \\(\\widehat{\\theta}\\) reste sans biais, \\(\\mathbb{E}[\\widehat{\\theta}]=\\theta\\), sous l’hypothèse H1. la matrice de variance-covariance de \\(\\widehat{\\theta}\\) reste égale à \\(\\sigma^2 (X&#39;X)^{-1}\\) sous les hypothèses H2 et H3, mais si H1 n’est pas vraie cette propriété a peu d’intérêt. \\(\\widehat{\\theta}\\) n’est plus un estimateur optimal parmi les estimateurs sans biais, mais il le reste parmi les estimateurs linéaires sans biais sous H1-H3. \\(\\widehat{\\theta}\\) est gaussien sous H3 et H4. Si H4 n’est pas vraie, alors il tend à être gaussien pour de grands échantillons. On dit qu’il est asymptotiquement gaussien. 5.1.2 Propriétés de l’estimateur des moindres carrés \\(\\widehat{\\sigma}^2\\) Cette étude n’a bien sûr d’intérêt que si \\(\\sigma^2\\) est bien définie ce qui nécessite l’hypothèse H2. Nous considérons \\[\\widehat{\\sigma^2} = \\frac{1}{n-k}\\|Y-X\\widehat{\\theta}\\|^2 \\textrm{ avec } \\widehat{\\theta} =(X&#39;X)^{-1}X&#39;Y.\\] Sous les hypothèses H1-H3, \\(\\widehat{\\sigma}^2\\) reste un estimateur sans biais de \\(\\sigma^2\\) même si l’hypothèse H4 n’est pas vérifiée : \\(\\mathbb{E}[\\widehat{\\sigma}^2]=\\sigma^2.\\) Il est clair que \\((n-k)\\widehat{\\sigma}^2\\) ne suit plus une loi \\(\\sigma^2 \\chi^2(n-k)\\) dès que l’hypothèse H4 n’est pas vérifiée. Nous montrons facilement que sous les hypothèses H1-H3, \\(\\widehat{\\sigma}^2\\) converge en probabilité vers \\(\\sigma^2\\) quand le nombre d’observations devient grand, même si l’hypothèse H4 n’est pas vérifiée. Enfin, sous les seules hypothèses H1-H3, dès que la loi de \\(\\varepsilon_i\\) admet un moment d’ordre 4, alors \\(\\widehat{\\sigma}^2\\) converge à la vitesse \\(\\sqrt{n}\\) vers \\(\\sigma^2\\) mais sa vitesse exacte de convergence dépend du type de loi, plus précisément du coefficient de Kurtosis (Azaïs and Bardet 2005). 5.1.3 Modèles avec corrélations Il est possible de modéliser des corrélations entre erreurs, par exemple en supposant que ces erreurs sont issues d’un processus ARMA, ce qui permet de ne plus avoir besoin de l’hypothèse H3, voir Guyon (2001). Il est également possible de modéliser les liaisons par des modèles à effets aléatoires et poser un modèle mixte, voir Pinheiro et Bates (Pinheiro and Bates 2006). 5.2 Modèles singuliers Nous nous sommes jusqu’à présent cantonnés à l’étude des modèles linéaires réguliers. Or certains modèles ne peuvent être paramétrés de façon régulière : ils sont naturellement sur-paramétrés. Un exemple simple est celui du modèle additif en analyse de la variance à 2 facteurs. Considérons le cas où les 2 facteurs ont chacun 2 niveaux et que les 4 combinaisons sont observées une fois et une seule. On a donc, avec les notations vues précédemment : \\[ Y_{i,j} = \\mu + a-i + b_j + \\varepsilon_{i,j},\\ i\\in\\{1,2\\},\\ j\\in\\{1,2\\}. \\] Le vecteur \\(\\theta=(\\mu,a_1,a_2,b_1,b_2)&#39;\\) et la matrice \\(X\\) du modèle vaut : \\[ X=\\left( \\begin{array}{ccccc} 1 &amp; 1 &amp; 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 &amp; 0 &amp; 1 \\end{array} \\right). \\] Nous remarquons que tout vecteur de la forme \\((\\alpha+\\beta,-\\alpha,-\\alpha, -\\beta,-\\beta)\\) donne le vecteur nul lorsqu’il est multiplié par la matrice \\(X\\). Les valeurs \\(\\mu, \\, a_i, \\, b_i\\) pour \\(i=1\\) ou 2 ne sont donc pas identifiables de manière unique. Le modèle est en fait sur-paramétré : nous avons 5 paramètres inconnus pour seulement 4 observations. Definition 5.1 Le modèle est dit singulier quand la matrice \\(X\\) est non injective, c’est-à-dire s’il existe \\(\\theta\\neq 0_k\\) tel que \\(X\\theta =0_n\\). Nous rappelons que \\(Ker(X)=\\{u \\in \\mathbb{R}^k; \\, Xu=0_{n}\\}\\) désigne le noyau de \\(X\\). Nous pouvons faire deux remarques : \\(X\\widehat{\\theta}\\) reste unique, puisque c’est la projection de \\(Y\\) sur \\(Im(X)\\). \\(\\widehat{\\theta}\\) ne peut être unique puisque si \\(\\widehat{\\theta}\\) est solution et si \\(u \\in Ker(X)\\) alors \\(\\widehat{\\theta} + u\\) est encore solution. Si \\(X\\) n’est pas régulière, alors la matrice \\(X&#39;X\\) n’est pas inversible. Pour contourner ce problème, nous définissons alors un inverse généralisé de \\((X&#39;X)\\). Definition 5.2 Soit \\(M\\) une matrice. Alors la matrice \\(M^-\\) est une matrice inverse généralisée de \\(M\\) si \\(MM^-M=M\\). Cette construction est toujours possible. En effet, \\((X&#39;X)\\) définit une application bijective de \\(Ker(X)^{\\perp}\\) sur lui-même. Il suffit donc simplement de négliger la partie contenue dans le noyau : on prend l’inverse sur \\(Ker(X)^{\\perp}\\), complété arbitrairement sur \\(Ker(X)\\). La définition de \\((X&#39;X)^-\\) est donc loin d’être unique ! Il est alors possible de généraliser les résultats du cas régulier. Proposition 5.2 Si \\((X&#39;X)^-\\) est une matrice inverse généralisée de \\(X&#39;X\\) alors \\(\\widehat{\\theta}=(X&#39;X)^-X&#39;Y\\) est une solution des équations normales : \\[(X&#39;X)\\widehat{\\theta} = X&#39;Y.\\] Proof. On commence par remarquer que \\[\\forall \\omega\\in\\mathbb{R}^k,\\, &lt;X\\omega,P_{[X]^\\perp} Y&gt; = &lt;\\omega, X&#39;P_{[X]^\\perp} Y&gt; = 0\\] donc \\[ X&#39;Y = X&#39;P_{[X]}Y + X&#39; P_{[X]^{\\perp}} Y = X&#39;P_{[X]}Y. \\] Ainsi, \\(\\exists u\\in\\mathbb{R}^k,\\ X&#39;Y = X&#39;Xu\\). Finalement, \\[ (X&#39;X)\\widehat\\theta = (X&#39;X) (X&#39;X)^{-}X&#39;Y = (X&#39;X) (X&#39;X)^{-}X&#39;Xu = X&#39;Xu = X&#39;Y. \\] Remark. Cet estimateur n’est pas unique et dépend de la définition choisie pour \\((X&#39;X)^-\\). Par contre, le vecteur \\(X\\widehat{\\theta}\\) reste unique, même si la matrice \\(X\\) est singulière. Ce vecteur correspond en effet à la projection orthogonale de \\(Y\\) sur \\(Im(X)\\). En règle générale, nous préférons lever l’indétermination sur \\(\\hat\\theta\\) en fixant des contraintes, souvent afin de donner un sens plus intuitif à \\(\\theta\\). 5.2.1 Contraintes d’identifiabilité Proposition 5.3 Supposons la matrice \\(X\\) singulière de rang \\(r&lt;k\\) de sorte qu’il y ait \\(k-r\\) paramètres redondants. Soit \\(M\\) une matrice à \\(k-r\\) lignes et \\(k\\) colonnes, supposée de rang \\(k-r\\) et telle que : \\[Ker(M) \\cap Ker(X)= \\lbrace 0_{k} \\rbrace.\\] Alors, la matrice \\((X&#39;X+M&#39;M)\\) est inversible et son inverse est une matrice inverse généralisée de \\(X&#39;X\\) le vecteur \\(\\widehat{\\theta} = (X&#39;X+M&#39;M)^{-1}X&#39;Y\\) est l’unique solution du système \\(\\displaystyle \\left\\lbrace \\begin{array}{c} X&#39;X\\alpha = X&#39;Y \\\\ M\\alpha= 0_{k-r}. \\end{array} \\right.\\) Exercise 5.1 L’objectif est de démontrer la proposition 5.3. Pour montrer que \\(X&#39;X+M&#39;M\\) est inversible : montrez que la matrice \\[ A=\\left(\\begin{array}{c} X\\\\ M \\end{array}\\right)\\in\\mathcal M_{n+k-r,k}(\\mathbb{R}) \\] est injective et donc \\(A&#39;A\\) est inversible. Considérez le problème de minimisation suivant : \\[ g:\\alpha\\mapsto \\|Y-X\\alpha\\|^2 + \\|M\\alpha\\|^2 . \\] Ecrivez \\(g(\\alpha)\\) sous la forme \\(g(\\alpha)=\\|\\tilde{Y} - A \\alpha\\|^2\\) avec \\(\\tilde {Y}\\) à préciser. Déduisez-en que \\(\\widehat{\\theta}\\) est solution du système \\(\\displaystyle \\left\\lbrace \\begin{array}{c} X&#39;X\\alpha = X&#39;Y \\\\ M\\alpha= 0_{k-r}. \\end{array} \\right.\\) Montrez l’unicité de la solution Le choix de la contrainte n’est pas toujours évident. Par ailleurs, pour chaque contrainte \\(M\\), nous aurons un estimateur correspondant ce qui est parfois gênant. Example 5.1 Prenons l’exemple de l’analyse de variance à un facteur avec effet différentiel : nous supposons pour simplifier que \\(I=4\\). Le modèle s’écrit donc de la façon suivante : \\[Y_{i,j}=\\mu + \\alpha_i +\\varepsilon_{ij} \\mbox{ pour } i=1,\\cdots,4 \\mbox{ et } j=1.\\] La matrice \\(X\\) associée au modèle est : \\[ X= \\left( \\begin{array}{ccccc} 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\end{array} \\right).\\] Il nous faut poser une contrainte (dite d’identifiabilité) sur le vecteur \\(\\theta\\) au travers du choix d’une matrice à \\(1\\) ligne et \\(k\\) colonnes. Si on considère \\(M= (0 \\ 1 \\ 1 \\ 1 \\ 1)\\), la contrainte correspondante est \\[ M\\theta = 0 \\Leftrightarrow \\alpha_1 + \\alpha_2 + \\alpha_3 + \\alpha_4= 0.\\] On impose donc que la somme des effets différentiels est nulle. Si on considère \\(M=(0,1,0,0,0)\\), la contrainte correspondante est \\(M\\theta = \\alpha_1=0\\). On impose donc que la première modalité est la référence. 5.2.2 Fonctions estimables et contrastes En présence d’une matrice singulière, il est donc toujours possible de construire un estimateur. Qu’en est-il des tests ? En particulier, ces contraintes sont-elles systématiquement nécessaires ? La plupart des quantités que nous avons voulu tester sont des fonctions de \\(\\theta\\) qui ne dépendent pas de la solution particulière des équations normales, c’est-à-dire du type de contraintes d’identifiabilité choisi. Ces fonctions sont appelées estimables car elles sont intrinsèques. Definition 5.3 Une combinaison linéaire \\(C\\theta\\) est dite fonction estimable (de paramètre \\(\\theta\\)) si elle ne dépend pas du choix particulier d’une solution des équations normales. On caractérise ces fonctions comme étant celles qui s’écrivent \\(C\\theta=DX\\theta\\) où \\(D\\) est une matrice de plein rang. Definition 5.4 On appelle contraste une fonction estimable \\(C\\theta\\) telle que \\(C \\mathbb{1}=0\\), où \\(\\un\\) désigne le vecteur unité. En analyse de variance, la plupart des combinaisons linéaires que l’on teste sont en fait des contrastes (cf chapitre 7). Dans l’exemple précédent, \\(\\alpha_1 - \\alpha_2\\) est un contraste. 5.3 Orthogonalité 5.3.1 Orthogonalité pour les modèles réguliers L’orthogonalité est une notion qui peut notablement simplifier la résolution et la compréhension d’un modèle linéaire. Un modèle linéaire admet le plus souvent une décomposition naturelle des paramètres \\(\\theta\\) (cf exemple ci-dessous) et conséquemment une décomposition de la matrice \\(X\\) associée au modèle. On va s’intéresser ici à l’orthogonalité éventuelle des différents espaces associés à cette décomposition (l’orthogonalité sera toujours comprise par la suite au sens d’orthogonalité liée au produit scalaire euclidien usuel). Le problème sera plus ou moins délicat suivant que le modèle est régulier ou non. En premier lieu, illustrons par deux exemples ce que l’on entend par décomposition des paramètres. Example 5.2 Soit le modèle de régression linéaire multiple sur trois variables \\(x^{(1)}, \\, x^{(2)}\\) et \\(x^{(3)}\\) : \\[Y_i=\\mu + \\theta_1x_i^{(1)}+\\theta_2 x^{(2)}_i +\\theta_3 x^{(3)}_i + \\varepsilon_i, i=1, \\cdots, n&gt;4.\\] Le vecteur \\(\\theta\\) comprend 4 coordonnées : \\(\\mu, \\, \\theta_1, \\, \\theta_2, \\theta_3\\) et la matrice \\(X\\) quatre colonnes. Assez naturellement ici, on peut considérer la décomposition, plus précisément on parlera par la suite de partition en quatre éléments. La partition de la matrice revient alors à l’écrire comme concaténation de 4 vecteurs colonnes. L’orthogonalité de la partition correspondra alors strictement à l’orthogonalité des 4 droites vectorielles : \\([\\mathbb{1}], \\, [x^{(1)}], \\, [x^{(2)}]\\) et \\([x^{(3)}]\\). Example 5.3 Soit le modèle de régression quadratique sur \\(x^{(1)}\\) et \\(x^{(2)}\\) : \\[Y_i=\\mu + \\theta_1x^{(1)}_i+\\theta_2 x^{(2)}_i +\\gamma_1 \\left(x^{(1)}_i\\right)^2 +\\gamma_2\\left(x^{(2)}_i\\right)^2+ \\delta x^{(1)}_i x^{(2)}_i + \\varepsilon_i, i=1, \\cdots, n&gt;6.\\] Ici plutôt que de demander comme précédemment l’orthogonalité de chacun des régresseurs (ce qui serait beaucoup demander), on peut définir la partition naturelle correspondant à : la constante \\(\\mu\\) ; les effets linéaires \\(\\theta_1, \\, \\theta_2\\) ; les effets carrés \\(\\gamma_1, \\, \\gamma_2\\) ; l’effet produit \\(\\delta\\). L’orthogonalité de la partition est alors définie comme l’orthogonalité des sous-espaces vectoriels : \\([\\mathbb{1}], \\, [(x^{(1)}, \\, x^{(2)})], \\, \\left[\\left((x^{(1)})^2, \\, (x^{(2)})^2\\right)\\right]\\) et \\([x^{(1)} x^{(2)}]\\). En conséquence, on voit bien, à partir de ces deux exemples, qu’il faudra parler de modèle avec partition orthogonale plutôt que de modèle orthogonal. Formalisons ces exemples dans une définition. Definition 5.5 Soit un modèle linéaire général régulier \\(Y=X\\theta + \\varepsilon.\\) Considérons une partition en \\(m\\) termes de \\(X\\) et de \\(\\theta\\), soit \\[Y= X_1\\theta_1 + \\cdots +X_m\\theta_m +\\varepsilon,\\] où la matrice \\(X_j\\) est une matrice de taille \\((n,k_j)\\) et \\(\\theta_j \\in \\mathbb{R}^{k_j}\\) avec \\(k_j \\in \\{1,\\cdots,k\\}\\) pour \\(j=1,\\cdots,m\\) et avec \\(\\sum_{j=1}^{m} k_j=k\\). On dit que cette partition est orthogonale si les sous-espaces vectoriels de \\(\\mathbb{R}^n\\), \\([X_1], \\cdots, [X_m],\\) sont orthogonaux. Une conséquence simple de l’orthogonalité d’un modèle linéaire est que la matrice d’information \\(X&#39;X\\) a une structure bloc diagonale, chaque bloc étant associé à chaque élément de la partition. Le plus souvent, la partition du vecteur de paramètres \\(\\theta\\) en différents effets vient en régression, des différentes variables ; en analyse de la variance, des décompositions en interactions. L’orthogonalité donne aux modèles statistiques les deux propriétés suivantes : Proposition 5.4 Soit un modèle linéaire régulier muni d’une partition orthogonale : \\[Y= X_1\\theta_1 + \\cdots +X_m\\theta_m +\\varepsilon.\\] Alors les estimateurs des moindres carrés des différents effets \\(\\widehat{\\theta}_1, \\dots, \\widehat\\theta_m\\) sont non-corrélés et indépendants sous l’hypothèse gaussienne. pour \\(l=1,\\cdots, m\\), l’expression de l’estimateur \\(\\widehat{\\theta}_l\\) ne dépend pas de la présence ou non des autres termes \\(\\theta_j\\) dans le modèle. L’orthogonalité apporte une simplification des calculs : elle permet d’obtenir facilement une expression explicite des estimateurs. Par ailleurs, elle donne une indépendance approximative entre les tests des différents effets. Les tests portant sur des effets orthogonaux ne sont liés que par l’estimation du \\(\\sigma^2\\). 5.3.2 Orthogonalité pour les modèles non-réguliers Lorsque le modèle est singulier, il est nécessaire de rajouter des contraintes. Il est alors raisonnable d’effectuer cette démarche en tenant compte de la partition, i.e. \\(C_j \\theta_j =0\\) où \\(X_j|_{Ker(C_j)}\\) sont injectives. Definition 5.6 Soit la partition suivante d’un modèle linéaire \\[Y=X_1\\theta_1+\\cdots+X_m\\theta_m +\\varepsilon.\\] Soit un système de contraintes \\(C_1\\theta_1=0, \\cdots, C_m \\theta_m=0\\) qui rendent le modèle identifiable. On dit que ces contraintes rendent la partition orthogonale si les sous-espaces vectoriels \\[V_j=\\left\\{X_j \\theta_j; \\, \\theta_j \\in Ker(C_j)\\right\\}, \\, j=1, \\cdots, m\\] sont orthogonaux. Cette notion est proche du cas régulier. Cependant, la notion d’orthogonalité dépend des contraintes choisies. L’idée sera en général de choisir des contraintes qui rendent le modèle orthogonal. On verra que cette définition prend tout son sens avec l’exemple incontournable du modèle d’analyse de la variance à deux facteurs croisés (cf chapitre 7). 5.4 En résumé Dans ce chapitre, il est attendu que vous ayez compris la problématique de l’estimation des paramètres pour un modèle linéaire singulier l’intérêt d’avoir l’orthogonalité dans un modèle linéaire Les résultats énoncés dans ce chapitre ne sont pas à connaitre. Il faudra savoir les mettre en application dans le cadre de l’ANOVA (voir Chapitre 7) et de l’ANCOVA (voir Chapitre 8). References "],["regression.html", "Chapitre 6 La régression linéaire 6.1 Introduction 6.2 Estimation 6.3 Tests et intervalles de confiance 6.4 Sélection des variables explicatives 6.5 Régression linéaire régularisée 6.6 Validation du modèle 6.7 En résumé 6.8 Quelques codes python", " Chapitre 6 La régression linéaire Les slides associés à la régression linéaire sont disponibles ici Le jeu de données utilisé dans ce chapitre est disponible ici fitness.txt 6.1 Introduction 6.1.1 Exemple illustratif Pour illustrer les notions abordées dans ce chapitre, nous allons considérer l’exemple suivant : on mesure pour 31 personnes lors de séances d’aérobic les \\(7\\) variables suivantes : age (a): age weight (w): poids oxy (oxy): consommation d’oxygène runtime (run): temps de l’effort rstpulse (rst): mesure de pulsation cardiaque 1 runpulse (rp): mesure de pulsation cardiaque 2 maxpulse (maxp): mesure de pulsation cardiaque 3 fitness=read.table(&quot;Data/fitness.txt&quot;,header=T) head(fitness) age weight oxy runtime rstpulse runpulse maxpulse 1 44 89.47 44.609 11.37 62 178 182 2 40 75.07 45.313 10.07 62 185 185 3 44 85.84 54.297 8.65 45 156 168 4 42 68.15 59.571 8.17 40 166 172 5 38 89.02 49.874 9.22 55 178 180 6 47 77.45 44.811 11.63 58 176 176 L’objectif est d’étudier si la consommation d’oxygène (variable réponse \\(Y\\)=oxy) peut être expliquée linéairement par les \\(6\\) autres variables quantitatives. Quelques statistiques descriptives sont données ci-dessous. Les boxplots et les corrélations deux à deux entre les variables quantitatives sont représentés sur la Figure 6.1 Figure 6.1: Description des données. A gauche, boxplot des différentes variables quantitatives. A droite, représentation graphique des corrélations deux à deux des variables quantitatives. 6.1.2 Problématique La régression est une des méthodes les plus connues et les plus appliquées en statistique pour l’analyse des données quantitatives. Elle est utilisée pour établir une liaison entre une variable quantitative, et une ou plusieurs autres variables quantitatives, sous la forme d’un modèle. Si on s’intéresse à la relation entre deux variables (par exemple, la consommation d’oxygène oxy en fonction du temps de l’effort runtime), on parlera de régression simple en exprimant une variable en fonction de l’autre. Si la relation porte entre une variable et plusieurs autres variables (par exemple, la variable oxy fonction de toutes les autres variables quantitatives), on parlera de régression multiple. La mise en oeuvre d’une régression impose l’existence d’une relation de cause à effet entre les variables prises en compte dans le modèle. Cette méthode peut être mise en place sur des données quantitatives observées sur \\(n\\) individus et présentées sous la forme : une variable quantitative \\(Y\\) prenant la valeur \\(Y_i\\) pour l’individu \\(i, i=1,\\cdots, n\\) appelée variable à expliquer ou variable réponse, \\(p\\) variables quantitatives \\(z^{(1)}, z^{(2)}, \\cdots, z^{(p)}\\) prenant respectivement les valeurs \\(z^{(1)}_i, z^{(2)}_i, \\cdots, z^{(p)}_i\\) pour l’individu \\(i\\), appelées variables explicatives ou prédicteurs. Si \\(p=1\\), on est dans le cas de la régression simple. Dans notre exemple, \\(n=31\\), \\(Y\\) est la variable oxy et \\(p=6\\). Considérons un couple de variables quantitatives \\((Y,Z)\\). S’il existe une liaison entre ces deux variables, la connaissance de la valeur prise par \\(Z\\) change notre incertitude concernant la réalisation de \\(Y\\). Si l’on admet qu’il existe une relation de cause à effet entre \\(Z\\) et \\(Y\\), le phénomène représenté par \\(Z\\) peut donc servir à prédire celui représenté par \\(Y\\) et la liaison s’écrit sous la forme \\(y=f(z)\\). On dit que l’on fait de la régression de \\(Y\\) sur \\(Z\\). Dans le cas les plus fréquents, on choisit l’ensemble des fonctions affines (du type \\(f(z)=\\theta_0 + \\theta_1 z\\) ou \\(f(z^{(1)},z^{(2)},\\cdots,z^{(p)})=\\theta_0+\\theta_1z^{(1)}+\\theta_2z^{(2)}+\\cdots+\\theta_p z^{(p)}\\)) et on parle de régression linéaire. 6.1.3 Le modèle de régression linéaire simple Soit un échantillon de \\(n\\) individus. Pour un individu \\(i\\) \\((i=1,\\cdots,n)\\), on a observé \\(Y_i\\) la valeur de la variable quantitative \\(Y\\) (ex: la consommation d’oxygène oxy), \\(z_i\\) la valeur de la variable quantitative \\(z\\) (ex: le temps d’effort runtime) On veut étudier la relation entre ces deux variables, et en particulier, l’effet de \\(z\\) (variable explicative) sur \\(Y\\) (variable réponse). Dans un premier temps, on peut représenter graphiquement cette relation en traçant le nuage des \\(n\\) points de coordonnées \\((z_i,Y_i)_{1\\leq i \\leq n}\\) (cf Figure 6.2). Dans le cas où le nuage de points est de forme “linéaire”, on cherchera à ajuster ce nuage de points par une droite. La relation entre \\(Y_i\\) et \\(z_i\\) s’écrit alors sous la forme d’un modèle de régression linéaire simple : \\[\\begin{equation} \\left\\{ \\begin{array}{l} Y_i=\\theta_0+\\theta_1z_i +\\varepsilon_i, \\, \\forall i =1,\\cdots, n, \\\\ \\\\ \\varepsilon_1,\\ldots,\\varepsilon_n \\textrm{ i.i.d de loi } \\mathcal{N}(0,\\sigma^2) \\end{array}\\right. \\tag{6.1} \\end{equation}\\] La première partie du modèle \\(\\theta_0+\\theta_1z_i\\) représente la moyenne de \\(Y_i\\) sachant \\(z_i\\) et la seconde partie \\(\\varepsilon_i\\), la différence entre cette moyenne et la valeur \\(Y_i\\). Le nuage de points est résumé par la droite d’équation \\(y=\\theta_0+\\theta_1z\\). Figure 6.2: Représentation du oxy en fonction de runtime. En rouge, la droite de régression linéaire simple ajustée. 6.1.4 Le modèle de régression linéaire multiple On dispose d’un échantillon de \\(n\\) individus pour lesquels on a observé \\(Y_i\\) la valeur de la variable réponse \\(Y\\) quantitative (ex: variable oxy), \\(z^{(1)}_i, \\cdots, z^{(p)}_i\\) les valeurs de \\(p\\) autres variables quantitatives \\(z^{(1)},\\cdots,z^{(p)}\\). On veut expliquer la variable quantitative \\(Y\\) par les \\(p\\) variables quantitatives \\(z^{(1)},\\cdots,z^{(p)}\\). Le modèle s’écrit \\[\\begin{equation} \\left\\{\\begin{array}{l} Y_i=\\theta_0+\\theta_1z^{(1)}_i+\\cdots, +\\theta_pz^{(p)}_i+\\varepsilon_i, \\forall i=1,\\cdots, n,\\\\ \\\\ \\varepsilon_1,\\ldots,\\varepsilon_n \\textrm{ i.i.d de loi } \\mathcal{N}(0,\\sigma^2) \\end{array}\\right. \\tag{6.2} \\end{equation}\\] 6.2 Estimation 6.2.1 Résultats généraux Le modèle (6.2) peut se réécrire sous la forme matricielle \\[ \\underbrace{\\left(\\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{array}\\right)}_{Y} = \\underbrace{\\left(\\begin{array}{c c c c c } 1&amp; z_1^{(1)}&amp; z_1^{(2)}&amp; \\ldots&amp; z_1^{(p)}\\\\ 1&amp; z_2^{(1)}&amp; z_2^{(2)}&amp; \\ldots&amp; z_2^{(p)}\\\\ \\vdots&amp;\\vdots &amp;\\vdots &amp;\\vdots &amp; \\vdots\\\\ 1&amp; z_n^{(1)}&amp; z_n^{(2)}&amp; \\ldots&amp; z_n^{(p)} \\end{array}\\right)}_{X} \\underbrace{\\left(\\begin{array}{c} \\theta_0\\\\ \\theta_1\\\\ \\vdots \\\\ \\theta_p\\end{array}\\right)}_{\\theta} + \\underbrace{\\left(\\begin{array}{c} \\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n \\end{array}\\right)}_{\\varepsilon} \\] où \\(X \\in \\mathcal{M}_{n,p+1}(\\mathbb{R})\\) (ici, \\(k=p+1\\)). Si le modèle est régulier, on peut alors estimer le vecteur des paramètres \\(\\theta\\) par la méthode des moindres carrés d’où \\[ \\widehat{\\theta}=(X&#39;X)^{-1}X&#39;Y \\sim \\mathcal{N}_{p+1}(\\theta,\\sigma^2(X&#39;X)^{-1}). \\] On en déduit alors \\(\\widehat{Y}_i=(X \\widehat{\\theta})_i = \\widehat{\\theta}_0+\\sum_{j=1}^p\\widehat{\\theta}_j z^{(j)}_i\\) la valeur ajustée de \\(Y_i\\) et le résidu \\(\\widehat{\\varepsilon_i}=Y_i-\\widehat{Y_i}\\). La variance \\(\\sigma^2\\) est estimée par \\[\\widehat{\\sigma}^2=\\frac{\\|Y - X \\widehat{\\theta}\\|^2}{n-(p+1)}= \\frac{1}{n-(p+1)}\\sum_{i=1}^n \\left(\\widehat{\\varepsilon_i}\\right)^2.\\] Les erreurs standards des estimateurs \\(\\widehat{\\theta_0}, \\cdots, \\widehat{\\theta_p}\\), des valeurs ajustées et des résidus calculés valent: erreur standard de \\(\\widehat{\\theta_j}\\) vaut \\(se(\\widehat{\\theta_j}) = \\sqrt{\\widehat{\\sigma^2}[(X&#39;X)^{-1}]_{j+1,j+1}}\\) erreur standard de \\(\\widehat{Y_i}\\) vaut \\(se(\\widehat{Y_i})= \\sqrt{\\widehat{\\sigma^2}[X(X&#39;X)^{-1}X&#39;]_{ii}}=\\sqrt{\\widehat{\\sigma^2}H_{ii}}\\) erreur standard de \\(\\widehat{\\varepsilon_i}\\) vaut \\(se(\\widehat{\\varepsilon_i}) = \\sqrt{\\widehat{\\sigma^2}(1-H_{ii})}.\\) Exercise 6.1 On se place dans le cadre de la régression linéaire simple d’équation (6.1). Montrez que les estimateurs de \\(\\theta_0\\) et \\(\\theta_1\\) par la méthode des moindres carrés sont donnés par : \\[\\left\\lbrace \\begin{array}{l} \\widehat{\\theta}_1 = \\displaystyle \\frac{cov(Y,z)}{ var(z)}=\\frac{\\strut \\sum\\limits_{i=1}^n(z_i - \\overline{z}) (Y_i - \\overline{Y})}{\\sum\\limits_{i=1}^n (z_i - \\overline{z})^2 \\strut},\\\\ \\widehat{\\theta}_0 = \\overline{Y} - \\widehat{\\theta}_1 \\, \\overline{z}, \\end{array} \\right.\\] où \\(\\overline{z}=\\frac{1}{n}\\sum_{i=1}^n z_i\\) et \\(\\overline{Y}=\\frac{1}{n}\\sum_{i=1}^n Y_i\\). Pour cela, on cherche à minimiser la fonction des moindres carrés \\[ (a,b) \\mapsto \\sum_{i=1}^n (Y_i - a - b z_i)^2. \\] 6.2.1.1 Exemple en régression linéaire simple Sous R, on peut ajuster le modèle de régression linéaire à l’aide de la fonction lm()avec la syntaxe suivante : Call: lm(formula = oxy ~ runtime, data = fitness) Residuals: Min 1Q Median 3Q Max -5.3352 -1.8424 -0.0569 1.5342 6.2033 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 82.4218 3.8553 21.379 &lt; 2e-16 *** runtime -3.3106 0.3612 -9.166 4.59e-10 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 2.745 on 29 degrees of freedom Multiple R-squared: 0.7434, Adjusted R-squared: 0.7345 F-statistic: 84.01 on 1 and 29 DF, p-value: 4.585e-10 On a en particulier \\((\\widehat{\\theta}_0)^{obs} =\\) 82.422 et \\((\\widehat{\\theta}_1)^{obs} =\\) -3.311 ainsi que leur erreur standard dans la colonne suivante. Sur le nuage de point, on obtient donc l’ajustement de la droite de régression linéaire en rouge 6.2.1.2 Exemple en régression linéaire multiple Les résultats obtenus avec la commande lm() pour l’exemple de la régression linéaire multiple sont donnés ci-dessous. Les deux premières colonnes correspondent aux estimations et aux erreurs standards respectivement pour chaque paramètre. reg.multi&lt;-lm(oxy~.,data=fitness) summary(reg.multi) Call: lm(formula = oxy ~ ., data = fitness) Residuals: Min 1Q Median 3Q Max -5.4026 -0.8991 0.0706 1.0496 5.3847 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 102.93448 12.40326 8.299 1.64e-08 *** age -0.22697 0.09984 -2.273 0.03224 * weight -0.07418 0.05459 -1.359 0.18687 runtime -2.62865 0.38456 -6.835 4.54e-07 *** rstpulse -0.02153 0.06605 -0.326 0.74725 runpulse -0.36963 0.11985 -3.084 0.00508 ** maxpulse 0.30322 0.13650 2.221 0.03601 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 2.317 on 24 degrees of freedom Multiple R-squared: 0.8487, Adjusted R-squared: 0.8108 F-statistic: 22.43 on 6 and 24 DF, p-value: 9.715e-09 6.2.2 Propriétés en régression linéaire simple On se place dans cette section dans le cadre de la régression linéaire simple (cf Equation (6.1)). La proposition suivante donne des propriétés entre les résidus et les valeurs prédites par le modèle. Proposition 6.1 Les résidus et les valeurs prédites vérifient les propriétés suivantes \\(\\sum_{i=1}^n \\widehat{\\varepsilon_i } = 0\\), \\(\\sum_{i=1}^n \\widehat{Y_i } = \\sum_{i=1}^n Y_i\\). La droite de régression passe par le point de coordonnées \\((\\overline z, \\overline Y)\\). Le vecteur des résidus n’est pas corrélé avec la variable explicative : \\(cov(z, \\widehat{\\varepsilon})=0\\). Le vecteur des résidus n’est pas corrélé avec la variable ajustée : \\(cov(\\widehat{Y},\\widehat{\\varepsilon})=0\\). La variance de \\(Y\\) admet la décomposition : \\[\\begin{equation} \\tag{6.3} var(Y)=var(\\widehat{Y})+ var(\\widehat{\\varepsilon}). \\end{equation}\\] Le carré du coefficient de corrélation de \\(z\\) et de \\(Y\\) s’écrit sous les formes suivantes : \\[r^2(z,Y)=\\frac{var(\\widehat{Y})}{var(Y)}= 1- \\frac{var(\\widehat{\\varepsilon})}{var(Y)}.\\] On en déduit que la variance empirique de \\(Y\\) se décompose en somme d’une part de variance expliquée \\((var(\\widehat{Y}))\\) et d’une variance résiduelle \\((var(\\widehat{\\varepsilon}))\\), et que \\(r^2(z,Y)\\) est le rapport de la variance expliquée sur la variance de la variable à expliquer. Proof. En utilisant que \\(\\widehat\\varepsilon_i = Y_i - \\widehat{Y}_i\\), \\(\\widehat{Y_i } = \\widehat{\\theta_0} + \\widehat{\\theta_1} z_i\\) et \\(\\widehat{\\theta_0} = \\bar Y - \\widehat{\\theta_1} \\bar z\\), on a \\(\\frac 1 n \\sum_{i=1}^n \\widehat{\\varepsilon_i } = \\frac 1 n \\sum_{i=1}^n \\left\\{Y_i - [\\widehat\\theta_0 + \\widehat\\theta_1 z_i] \\right\\} = \\bar Y - \\widehat\\theta_0 - \\widehat\\theta_1 \\bar z = 0\\) par définition de \\(\\widehat\\theta_0\\). \\(\\widehat\\theta_0 + \\widehat\\theta_1 \\bar z = \\left[\\bar Y - \\widehat\\theta_1 \\bar z \\right] + \\widehat\\theta_1 \\bar z =\\bar Y\\) On a \\[\\begin{eqnarray*} n\\ cov(z,\\widehat\\varepsilon) &amp;=&amp; \\sum_{i=1}^n \\widehat\\varepsilon_i (z_i - \\bar z)\\\\ &amp;=&amp; \\sum_{i=1}^n [Y_i - \\bar Y - \\widehat{\\theta}_1(z_i - \\bar z)][z_i - \\bar z]\\\\ &amp;=&amp; n\\, \\left\\{cov(Y,z) - \\widehat{\\theta}_1var(z)\\right\\} = 0 \\end{eqnarray*}\\] par définition de \\(\\widehat\\theta_1\\). \\(n\\ cov(\\widehat Y, \\widehat\\varepsilon) = \\sum_{i=1}^n \\widehat\\varepsilon_i (\\widehat Y_i -\\bar Y) = \\sum_{i=1}^n \\widehat\\varepsilon_i \\widehat\\theta_1 (z_i - \\bar z) = n \\widehat\\theta_1 cov(z,\\widehat\\varepsilon) = 0.\\) \\(n\\, var(Y) = \\sum_{i=1}^n (Y_i - \\widehat Y_i + \\widehat Y_i - \\bar Y)^2 = n\\, var(\\widehat\\varepsilon) + n\\, var(\\widehat Y) + 2 n\\, cov(\\widehat\\varepsilon,\\widehat Y).\\) On a \\(r^2(z,Y) = \\frac{cov(z,Y)^2}{var(z) var(Y)}\\) et \\[ n\\, cov(z,Y) = \\sum_{i=1}^n (Y_i -\\widehat Y_i + \\widehat Y_i - \\bar Y) (z_i - \\bar z) = n\\,cov(\\widehat\\varepsilon,z) + n\\, cov(\\widehat Y,z) = n\\, cov(\\widehat Y,z). \\] Ainsi, \\[r^2(z,Y) = \\frac{cov(\\widehat Y, z)^2}{var(z) var(\\widehat Y)} \\frac{var(\\widehat Y)}{var(Y)} = cor(\\widehat Y,z)^2 \\frac{var(\\widehat Y)}{var(Y)} = \\frac{var(\\widehat Y)}{var(Y)}\\] car \\(\\widehat Y_i = \\widehat\\theta_0 + \\widehat \\theta_1 z_i, \\forall i\\) (relation linéaire). 6.2.3 Le coefficient \\(R^2\\) 6.2.3.1 Définition Le coefficient \\(R^2\\), défini comme le carré du coefficient de corrélation de \\(z\\) et \\(Y\\) est une mesure de qualité de l’ajustement, égale au rapport de la variance effectivement expliquée sur la variance à expliquer : \\[R^2= r^2(z,Y)=\\frac{var(\\widehat{Y})}{var(Y)}.\\] Ainsi \\(R^2 \\in [0,1]\\) et s’interprète comme la proportion de variance expliquée par la régression. La plupart des logiciels n’utilise pas la décomposition (6.3), mais plutôt la décomposition obtenue en multipliant cette expression par \\(n\\) : \\[SST = SSE + SSR\\] où \\(\\displaystyle SST = \\|Y - \\overline Y \\mathbb{1}_n\\|^2=\\sum_{i=1}^n (Y_i-\\overline Y)^2\\) est la somme totale des carrés corrigés de \\(Y\\), \\(\\displaystyle SSE =\\|\\widehat{Y}-\\overline Y \\mathbb{1}_n\\|^2= \\sum_{i=1}^n (\\widehat{Y_i}-\\overline Y)^2\\) est la somme des carrés expliquée par le modèle, \\(\\displaystyle SSR =\\|Y - \\widehat{Y}\\|^2= \\sum_{i=1}^n (\\widehat{\\varepsilon_i})^2\\) est la somme des carrés des résidus. Ainsi, pour calculer le \\(R^2\\), on utilise également l’expression \\[R^2= \\frac{SSE}{SST}= 1- \\frac{SSR}{SST}.\\] Dans l’exemple de la régression linéaire simple, la valeur du \\(R^2\\) vaut \\(0.8078\\). Pour retrouver les valeurs de \\(SST\\), \\(SSR\\) et \\(SSE\\), on peut utiliser la commande anova(). anova(reg.simple) Analysis of Variance Table Response: oxy Df Sum Sq Mean Sq F value Pr(&gt;F) runtime 1 632.90 632.90 84.008 4.585e-10 *** Residuals 29 218.48 7.53 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Dans le cas d’une régression multiple de \\(Y\\) par \\(z^{(1)}, \\cdots, z^{(p)}\\), le coefficient de corrélation multiple noté \\(r(Y,z^{(1)},\\cdots,z^{(p)})\\) est défini comme le coefficient de corrélation linéaire empirique de \\(Y\\) par \\(\\widehat{Y}\\) : \\[r(Y,z^{(1)}, \\cdots, z^{(p)})=r(Y,\\widehat{Y}).\\] Ainsi le coefficient \\(R^2\\) de la régression multiple est égal au carré du coefficient de corrélation linéaire multiple empirique \\(r(Y,z^{(1)}, \\cdots, z^{(p)})\\). Dans l’exemple de la régression linéaire multiple, la valeur du \\(R^2\\) vaut 0.849. 6.2.3.2 Augmentation mécanique du \\(R^2\\) Lorsque l’on ajoute une variable explicative à un modèle, la somme des carrés des résidus diminue ou au moins reste stable. En effet, si on considère un modèle à \\(p-1\\) variables : \\[Y_i=\\theta_0+\\theta_1z^{(1)}_i + \\cdots + \\theta_{p-1}z^{(p-1)}_i + \\varepsilon_i\\] alors les coefficients \\((\\widehat{\\theta}_0, \\widehat{\\theta}_1, \\cdots, \\widehat{\\theta}_{p-1})\\) estimés minimisent \\[\\phi(\\theta_0,\\theta_1, \\cdots, \\theta_{p-1})=\\sum_{i=1}^n\\left[Y_i-(\\theta_0+\\theta_1z^{(1)}_i + \\cdots + \\theta_{p-1}z^{(p-1)}_i)\\right]^2.\\] Si on rajoute une nouvelle variable explicative \\(z^{(p)}\\) au modèle, on obtient \\[Y_i=\\theta_0+\\theta_1z^{(1)}_i + \\cdots + \\theta_{p-1}z^{(p-1)}_i + \\theta_pz^{(p)}_i+ \\varepsilon_i,\\] et les coefficients estimés, notés \\((\\widetilde{\\theta}_0, \\widetilde{\\theta}_1, \\cdots, \\widetilde{\\theta}_p)\\) minimisent la fonction : \\[\\tilde\\psi(\\theta_0,\\theta_1, \\cdots, \\theta_p)=\\sum_{i=1}^n\\left[Y_i-(\\theta_0+\\theta_1z^{(1)}_i + \\cdots + \\theta_pz^{(p)}_i)\\right]^2\\] qui, par construction, vérifie l’égalité : \\[\\tilde\\psi(\\theta_0,\\theta_1, \\cdots, \\theta_{p-1},0)=\\phi(\\theta_0,\\theta_1, \\cdots, \\theta_{p-1}).\\] D’où l’inégalité : \\[\\tilde\\psi(\\widetilde{\\theta}_0,\\widetilde{\\theta}_1, \\cdots, \\widetilde{\\theta}_p) \\leq \\tilde\\psi(\\widehat{\\theta}_0,\\widehat{\\theta}_1, \\cdots, \\widehat{\\theta}_{p-1},0) = \\phi(\\widehat{\\theta}_0,\\widehat{\\theta}_1, \\cdots, \\widehat{\\theta}_{p-1}).\\] Ceci prouve l’augmentation “mécanique” du \\(R^2\\) sans pour autant améliorer le modèle, comme nous le verrons par la suite. 6.3 Tests et intervalles de confiance 6.3.1 Test de nullité d’un paramètre du modèle En testant l’hypothèse nulle \\(\\mathcal{H}_0^{(j)} : \\theta_j=0\\) où \\(\\theta_j\\) est le paramètre associé à la variable explicative \\(z^{(j)}\\), on étudie l’effet de la présence de la variable explicative \\(z^{(j)}\\). Pour tester \\(\\mathcal{H}_0^{(j)} : \\theta_j = 0\\) contre \\(\\mathcal{H}_1^{(j)} : \\theta_j\\neq 0\\), on met en place un test classique de Student. On estime \\(\\theta\\) par l’EMC \\(\\widehat{\\theta}\\) donc \\(\\theta_j\\) est estimé par \\(\\widehat{\\theta}_j\\) Comme \\(\\widehat{\\theta}\\sim\\mathcal{N}_k(\\theta,\\sigma^2 (X&#39;X)^{-1})\\), on a \\(\\widehat{\\theta}_j\\underset{\\mathcal{H}_0}{\\sim} \\mathcal{N}(0,\\sigma^2 [(X&#39;X)^{-1}]_{jj})\\) On estime \\(\\sigma^2\\) par \\(\\widehat{\\sigma}^2= \\frac{\\|Y - X \\widehat\\theta\\|^2}{n-k}\\) D’après Cochran, \\(\\frac{(n-k)\\widehat{\\sigma}^2}{\\sigma^2}\\sim \\chi^2(n-k)\\) et \\(\\widehat{\\theta}_j\\) et \\(\\widehat{\\sigma}^2\\) sont indépendants. Ainsi la statistique de test \\[ T_j := \\frac{\\widehat\\theta_j}{\\sqrt{\\widehat{\\sigma}^2 [(X&#39;X)^{-1}]_{jj}}}\\underset{\\mathcal{H}_0}{\\sim} \\mathcal{T}(n-k) \\] La zone de rejet est de la forme \\[ \\mathcal{R}_\\alpha = \\left\\{|T_j|&gt; t_{1-\\alpha/2,n-k}\\right\\} \\] où \\(t_{1-\\alpha/2,n-k}\\) est le \\(1-\\alpha/2\\) quantile de la loi de Student \\(\\mathcal{T}(n-k)\\). Dans les exemples de la régression linéaire simple (section 6.2.1.1) et la régression linéaire multiple (section 6.2.1.2), la pvaleur associée au test de nullité de chacun des coefficients \\(\\theta_j\\) est donnée dans la dernière colonne (la valeur de la statistique de test est donnée dans l’avant dernière colonne). D’après les résultats dans l’exemple de la régression simple, on rejette fortement la nullité de chacun des coefficients au niveau \\(5\\%\\). Dans l’exemple de la régression multiple, on rejette la nullité des coefficients \\(\\theta_0\\), \\(\\theta_1\\), \\(\\theta_3\\), \\(\\theta_6\\) et \\(\\theta_7\\) au niveau \\(5\\%\\) chacun. Chaque test de nullité est fait séparément, attention aux conclusions trop rapides! 6.3.2 Test de nullité de quelques paramètres du modèle Soit un modèle de référence à \\(p\\) variables explicatives. On veut étudier l’influence de \\(q\\) variables explicatives (avec \\(q \\leq p)\\) sur la variable à expliquer. Cela revient à tester l’hypothèse de nullité de \\(q\\) paramètres du modèle : \\[\\mathcal{H}_0 : \\theta_1 = \\theta_2 = \\cdots = \\theta_q=0, \\mbox{ avec } q \\leq p.\\] Sous l’hypothèse alternative, au moins un des paramètres \\(\\theta_1, \\cdots, \\theta_q\\) est non nul. Ce test peut être formulé comme la comparaison de deux modèles emboîtés, l’un à \\(p+1\\) paramètres et l’autre à \\(p+1-q\\) paramètres : \\[ \\begin{array}{c l l} (M1) &amp; Y_i=\\theta_0 + \\theta_1 z^{(1)}_i + \\cdots + \\theta_p z^{(p)}_i + \\varepsilon_i &amp; \\textrm{ sous } \\mathcal{H}_1\\\\ \\textrm{versus} &amp; &amp;\\\\ (M0) &amp; Y_i=\\theta_0 + \\theta_{q+1} z^{(q+1)}_i + \\cdots + \\theta_p z^{(p)}_i + \\varepsilon_i &amp; \\textrm{ sous } \\mathcal{H}_0. \\end{array} \\] Dans la suite, on note \\(Y=Z\\beta+\\varepsilon\\) et \\(Y=X\\theta+\\varepsilon\\) les formes matricielles des modèles \\((M0)\\) et \\((M1)\\) respectivement. Pour ce test, on considère la statistique de Fisher : \\[F=\\frac{(SSR_0-SSR_1) / q}{SSR_1 / (n-(p+1))} \\underset{\\mathcal{H}_0}{\\sim} \\mathcal F(q,n-(p+1))\\] où \\(SSR_0=\\|Y - Z \\widehat{\\beta}\\|^2\\) désigne la somme des carrés des résidus du modèle “réduit” sous \\(\\mathcal{H}_0\\) et \\(SSR_1=\\|Y - X \\widehat{\\theta}\\|^2\\) correspond à la somme des carrés des résidus du modèle de référence. La zone de rejet est de la forme \\(\\mathcal{R}_\\alpha=\\left\\{F\\geq f_{q,n-p-1,1-\\alpha}\\right\\}\\) où \\(f_{q,n-p-1,1-\\alpha}\\) est le \\(1-\\alpha\\) quantile de la loi de Fosher \\(\\mathcal{F}(q,n-(p+1))\\). On remarque que dans le cas où \\(q=1\\), on teste la nullité d’un seul paramètre du modèle et on retrouve les mêmes conclusions qu’avec le test précédent de Student. Dans notre exemple en régression linéaire multiple, on souhaite tester le sous-modèle composé uniquement des variables age, runtime,runpulse et maxpulse. A l’aide de la fonction anova(), on va faire un test de Fisher entre ce sous-modèle et le modèle complet : regfin&lt;-lm(oxy~age + runtime+runpulse+maxpulse,data=fitness) anova(regfin,reg.multi) Analysis of Variance Table Model 1: oxy ~ age + runtime + runpulse + maxpulse Model 2: oxy ~ age + weight + runtime + rstpulse + runpulse + maxpulse Res.Df RSS Df Sum of Sq F Pr(&gt;F) 1 26 138.93 2 24 128.84 2 10.092 0.94 0.4045 La pvaleur valant \\(0.4045\\), on ne rejette pas le sous-modèle \\(M_0\\) au risque \\(5\\%\\). Exercise 6.2 Dans la sortie R de anova(regfin,reg.multi) ci-dessus, à quoi corresponde chacune des valeurs numériques ? 6.3.3 Test de nullité de tous les paramètres du modèle Dans cette section, on souhaite tester l’hypothèse de nullité de tous les paramètres du modèle (associés aux variables explicatives) : \\[\\mathcal{H}_0 : \\theta_1 = \\cdots = \\theta_p=0.\\] Ce test revient à comparer la qualité d’ajustement du modèle de référence à celle du “modèle blanc”. Cette hypothèse composée de \\(p\\) contraintes signifie que les \\(p\\) paramètres associés aux \\(p\\) variables explicatives sont nuls, c’est-à-dire qu’aucune variable explicative présente dans le modèle ne permet d’expliquer la variable \\(Y\\). Sous \\(\\mathcal{H}_0\\), le modèle s’écrit : \\[Y_i=\\theta_0+\\varepsilon_i \\mbox{ avec } \\widehat{\\theta}_0 = \\overline Y\\] et la somme des carrés des résidus (\\(SRR_0\\)) est égale à la somme des carrés totales \\((SST)\\): \\[ SSR_0=\\|Y - \\widehat{\\theta}_0 \\mathbb{1}_n\\|^2=\\|Y - \\overline Y \\mathbb{1}_n\\|^2=SST. \\] La statistique de test de Fisher dans ce cas s’écrit : \\[ \\begin{eqnarray*} F&amp;=&amp; \\frac{SSR_0 - SSR_1 / (P+1-1)}{SSR_1/n-(P+1)}\\\\ &amp;=&amp;\\frac{SSE_1 / p}{SCR_1 / n-(p+1)}\\\\ &amp;=&amp;\\frac{R^2}{1-R^2}\\times\\frac{n-p-1}{p} \\underset{\\mathcal{H}_0}{\\sim} \\mathcal F(p,n-p-1) \\end{eqnarray*} \\] où \\(SSE_1\\) désigne la somme des carrés du modèle de référence avec \\(SST = SSE_1+SSR_1\\) et \\(R^2\\) est le critère d’ajustement du modèle de référence. La zone de rejet est de la forme \\[ \\mathcal{R}_\\alpha = \\left\\{ F \\geq f_{p,n-(p+1),1-\\alpha} \\right\\} \\] où \\(f_{p,n-(p+1),1-\\alpha}\\) est le \\(1-\\alpha\\) quantile de la loi de Fisher \\(\\mathcal{F}(p,n-(p+1))\\). Dans l’exemple de régression linéaire multiple, on peut mettre en place ce test avec la fonction anova(). On peut aussi remarquer que le résultat de ce test est donné directement dans summary(reg.multi) (voir section 6.2.1.2). regblanc&lt;-lm(oxy~1,data=fitness) anova(regblanc,reg.multi) Analysis of Variance Table Model 1: oxy ~ 1 Model 2: oxy ~ age + weight + runtime + rstpulse + runpulse + maxpulse Res.Df RSS Df Sum of Sq F Pr(&gt;F) 1 30 851.38 2 24 128.84 6 722.54 22.433 9.715e-09 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Ici, la pvaleur vaut \\(9.715e^{-091}\\), on rejette donc l’hypothèse que tous les coefficients sont nuls. 6.3.4 Intervalle de confiance de \\(\\theta_j\\), de \\((X\\theta)_i\\) et de \\(X_0\\theta\\) 6.3.4.1 Intervalle de confiance de \\(\\theta_j\\) On reprend ici la construction générale faite en section 3.5.1, ici \\(k=1+p\\). En utilisant que \\(\\widehat \\theta_j \\sim \\mathcal N(\\theta_j, \\sigma^2 [(X&#39;X)^{-1}]_{j+1,j+1})\\) \\(\\displaystyle \\frac{(n-(p+1)) \\widehat\\sigma^2}{\\sigma^2} \\sim \\chi^2(n-(p+1))\\) \\(\\widehat \\theta_j\\) et \\(\\widehat \\sigma^2\\) indépendants on obtient que \\[\\frac{\\widehat{\\theta_j} - \\theta_j}{ \\sqrt{\\widehat \\sigma^2 [(X&#39;X)^{-1}]_{j+1,j+1}}} \\sim \\mathcal{T}(n-(p+1)).\\] On construit alors l’intervalle de confiance suivant pour le paramètre \\(\\theta_j\\) au niveau de confiance \\(1-\\alpha\\) : \\[ IC_{1-\\alpha}(\\theta_j) = \\left[\\widehat{\\theta_j}\\pm t_{n-(p+1),1-\\alpha/2} \\times \\sqrt{\\widehat \\sigma^2 [(X&#39;X)^{-1}]_{j+1,j+1}}\\right]. \\] Dans les deux exemples de ce chapitre, on peut facilement obtenir les intervalles de confiance pour les coefficient \\(\\theta_j\\) à l’aide de la fonction confint(). confint(reg.simple,level=0.9) 5 % 95 % (Intercept) 75.871122 88.972424 runtime -3.924271 -2.696839 confint(reg.multi) 2.5 % 97.5 % (Intercept) 77.33541293 128.53354604 age -0.43302821 -0.02091938 weight -0.18685216 0.03849733 runtime -3.42235018 -1.83495545 rstpulse -0.15786297 0.11479569 runpulse -0.61699207 -0.12226345 maxpulse 0.02150491 0.58492935 6.3.4.2 Intervalle de confiance de \\((X\\theta)_i\\) En reprenant la construction faite en section 3.5.2, l’intervalle de confiance de \\((X\\theta)_i\\) au niveau de confiance de \\(1-\\alpha\\) est donc donné par : \\[ IC_{1-\\alpha}((X\\theta)_i) = \\left[\\widehat{Y_i}\\pm t_{n-(p+1),1-\\alpha/2} \\times \\sqrt{ \\widehat \\sigma^2 [X(X&#39;X)^{-1}X&#39;]_{ii}}\\right]. \\] Pour l’exemple de la régression linéaire simple, ces intervalles de confiance sont représentés en Figure 6.3. Figure 6.3: Intervalle de confiance la réponse moyenne. 6.3.4.3 Intervalle de confiance de \\(X_0\\theta\\) Pour des nouvelles données \\(z_0^{(1)}, \\cdots, z_0^{(p)}\\) des variables explicatives, on définit \\(X_0=(1,\\, z^{(1)}_0, \\, \\cdots, \\,z_0^{(p)}) \\in \\mathcal{M}_{1,(p+1)}(\\mathbb{R})\\). La réponse moyenne est alors : \\[ X_0\\theta=\\theta_0+\\sum_{j=1}^p \\theta_j z^{(j)}_0 . \\] En reprenant la construction faite en section 3.5.3, on obtient que l’intervalle de confiance de \\(X_0\\theta\\) au niveau de confiance de \\(1-\\alpha\\) s’écrit : \\[ IC_{1-\\alpha}(X_0\\theta) = \\left[X_0\\widehat{\\theta}\\pm t_{n-(p+1),1-\\alpha/2} \\times \\sqrt{\\widehat{\\sigma}^2X_0(X&#39;X)^{-1}X&#39;_0}\\right]. \\] Dans l’exemple de la régression linéaire simple, voir Figure 6.4. Figure 6.4: Intervalle de confiance pour la réponse moyenne d’un nouvel individu. 6.3.5 Intervalle de prédiction On veut prédire dans quel intervalle se trouvera le résultat d’un nouvel essai \\((z^{(1)}_0, \\cdots ,z_0^{(p)})\\). On veut donc construire un intervalle de prédiction pour une nouvelle observation \\(Y_0\\), correspondant à \\(X_0=(1, z_0^{(1)}, z_0^{(2)} , \\cdots, z_0^{(p)})\\) : \\[ Y_0=X_0\\theta+\\varepsilon_0, \\] où \\(\\varepsilon_0\\) est indépendant des \\(\\varepsilon_i, \\, 1 \\leq i \\leq n\\) et où \\(\\varepsilon_0 \\sim \\mathcal{N}(0,\\sigma^2)\\). En reprenant la construction faite en section 3.6, on obtient que l’intervalle de prédiction de la variable \\(Y\\) pour une nouvelle observation au point \\(X_0\\) est défini par \\[IC_{1-\\alpha}(Y_0)=\\left[X_0\\widehat{\\theta}\\pm t_{n-(p+1),1-\\alpha/2}\\widehat\\sigma \\sqrt{1+X_0(X&#39;X)^{-1}X&#39;_0} \\right].\\] Notez bien la différence entre \\(IC_{1-\\alpha}(Y_0)\\) et \\[ IC_{1-\\alpha}(X_0\\theta) = \\left[X_0\\widehat{\\theta}\\pm t_{n-(p+1),1-\\alpha/2} \\times \\widehat{\\sigma} \\sqrt{X_0(X&#39;X)^{-1}X&#39;_0}\\right]. \\] Dans l’exemple de la régression linéaire simple, les intervalles de confiance \\(IC_{1-\\alpha}(X_0\\theta)\\) et \\(IC_{1-\\alpha}(Y_0)\\) sont représentés sur la Figure 6.5. Figure 6.5: Intervalle de confiance pour la réponse moyenne (en gris foncé)et intervalle de prédiction (pointillés rouge) pour un nouvel individu. Remark. Pour faire de la prédiction à l’aide de ce modèle de régression linéaire, il est recommandé de n’utiliser ce modèle que dans le domaine couvert par les données. En effet, le phénomène étudié peut être linéaire dans le domaine observé et avoir un comportement différent dans un autre domaine. 6.4 Sélection des variables explicatives En présence de \\(p\\) variables explicatives dont on ignore celles qui sont réellement influentes, on doit rechercher un modèle d’explication de \\(Y\\) à la fois performant (résidus les plus petits possibles) et économique (le moins possible de variables explicatives). Nous allons donc maintenant nous concentrer sur l’étude de la matrice \\(X\\) autrement dit sur les variables explicatives elles-mêmes. Dans cette partie, nous allons voir comment choisir le modèle le plus en adéquation avec nos données et éliminer certaines variables peu explicatives pour gagner en interprétation. Ce problème de sélection de variables est en fait un problème de sélection de modèles. 6.4.1 Cadre général de sélection de modèles Par soucis de simplicité, on présente ce problème dans le cadre de la régression linéaire multiple. Les outils présentés ici peuvent être bien sûr utilisés dans un cadre plus général (bien souvent sans travail supplémentaire). On se donne une famille de modèles \\(\\mathcal{M}\\) représentant formellement une famille de sous-ensembles de \\(\\lbrace 1, \\dots, p \\rbrace\\). Ce choix est fait a priori et peut ne pas être exhaustif. Par exemple, on peut considérer famille exhaustive : \\(\\mathcal{M}=\\mathcal{P}(\\lbrace 1,\\dots, p \\rbrace )\\) i.e. la famille de tous les sous-ensembles de \\(\\lbrace 1,\\dots, p\\rbrace\\), famille croissante : \\(\\mathcal{M}= \\left( \\lbrace 1,\\dots, m \\rbrace \\right)_{m=1,\\ldots ,p}\\). Par la suite, pour \\(m \\in \\mathcal{M}\\), on notera \\(|m|\\) le cardinal de \\(m\\) et \\(X_{(m)}\\) représente la matrice constituée des vecteurs \\(\\textbf{z}^{(j)}\\) pour \\(j\\in m\\). On supposera également que pour tout \\(m \\in \\mathcal{M}\\), la matrice \\(X_{(m)}\\) est régulière, i.e. de rang \\(|m|+1\\). Il faut noter que le “\\(+1\\)” vient de la constante (de l’intercept) qui est supposée être présente systématiquement dans tous les modèles. Hypothèses sur le vrai modèle : On suppose qu’il existe \\(m^{\\star} \\in \\mathcal{M}\\), inconnu, tel que le vrai modèle s’écrit : \\[Y=\\mu^{\\star}+\\varepsilon^{\\star} = X_{(m^{\\star})}\\theta_{(m^{\\star})} + \\varepsilon^{\\star}, \\mbox{ avec } \\varepsilon^{\\star} \\sim \\mathcal{N}(0_{n},\\sigma^{\\star\\, 2}I_n),\\] le vecteur \\(\\theta_{(m^{\\star})} \\in \\mathbb{R}^{|m^{\\star}|+1}\\) ayant toutes ses coordonnées non nulles. Modèles d’analyse : Pour modéliser l’expérience et essayer d’identifier le vrai modèle on utilise la famille de modèles suivante, qui est en correspondance avec \\(\\mathcal{M}\\), i.e. \\[Y=\\mu+\\varepsilon = X_{(m)}\\theta_{(m)} + \\varepsilon, \\mbox{ avec } \\varepsilon \\sim \\mathcal{N}(0_{n},\\sigma^2 I_n).\\] Pour préciser la modélisation, nous utiliserons le vocabulaire suivant : Definition 6.1 On suppose que le modèle d’analyse est \\(m \\in \\mathcal{M}\\). Alors si \\(m=m_p=\\{1,\\cdots,p\\}\\), on dit que le modèle est complet, i.e. que toutes les variables explicatives disponibles sont significatives si \\(m^{\\star} \\subset m\\) avec \\(m \\neq m^{\\star}\\), on dit que le modèle est sur-ajusté si \\(|m \\cap m^{\\star}| &lt; |m^{\\star}|\\), on dit que le modèle est faux si \\(m \\subset m^{\\star}\\) avec \\(m \\neq m^{\\star}\\), on dit que le modèle est sous-ajusté. Rappelons que chaque modèle correspond à un choix parmi l’ensemble des variables explicatives, et qu’il y a donc potentiellement des variables explicatives superflues. En cas de sur-ajustement, i.e. s’il y a des variables superflues, un modèle sur-ajusté est un modèle contenant toutes les variables du vrai modèle plus un certain nombre de variables superflues. Un faux modèle est typiquement un modèle où les variables du vrai modèle n’ont pas toutes été choisies et où certaines variables superflues ont pu être choisies. Un cas particulier est celui du sous-ajustement correspondant à un faux modèle ne contenant aucune variable superflue. Nous allons voir dans la suite diverses approches permettant, non pas de retrouver \\(m^{\\star}\\), mais au moins de s’en approcher. Ceci correspond aux bases de la sélection de modèle. 6.4.2 Quelques critères pour sélectionner un modèle 6.4.2.1 Les coefficients d’ajustement Dans la situation où seul un petit nombre de régresseurs est en jeu, il existe déjà un certain nombre d’approches s’inspirant plus ou moins directement des outils étudiés précédemment. Pour “tester” la validité d’un sous-modèle \\(m\\) par rapport à un modèle plus grand, il existe deux indices (ou coefficients) dont le calcul et l’interprétation sont assez immédiats. Une première possibilité consiste à s’intéresser au coefficient de détermination : \\[ R_m^2= \\frac{SST-SSR(m)}{SST} = 1-\\frac{\\| Y- X_{(m)} \\hat\\theta_{(m)} \\|^2}{\\| Y - \\overline Y \\mathbb{1}_n\\|^2}. \\] Cet indice compare donc les valeurs prédites de \\(Y\\) aux valeurs observées par l’intermédiaire de \\(\\| \\widehat{Y}_{(m)} - Y \\|^2\\), le dénominateur correspondant en quelque sorte à une renormalisation. Plus le coefficient \\(R^2_m\\) sera proche de \\(1\\), plus l’adéquation du modèle retenu aux données sera importante. Si on est amené à choisir entre deux modèles explicatifs, on est donc facilement tenté de retenir celui possédant le coefficient de détermination le plus important. Il est cependant important d’apporter un petit bémol à ce type de raisonnement. En effet la maximisation de ce critère \\(R_m^2\\) revenant à maximiser \\(\\|Y-\\widehat{Y}_{(m)}\\|^2\\), il est clair que la quantité \\(\\|Y-\\widehat{Y}_{(m)}\\|^2=\\|P_{[X_{(m)}]^{\\perp}}Y\\|^2\\) décroît pour une suite emboîtée de modèles. Par conséquent, la maximisation de \\(R_m^2\\) conduit à coup sûr à choisir le modèle complet \\(m_k\\). Utiliser ce type de critère favorise ainsi la sélection de modèles très paramétrés. En revanche, pour des modèles de même cardinal \\(|m|\\), ce coefficient peut être utilisé pour choisir un modèle optimal. Il est possible d’améliorer le coefficient \\(R^2\\) pour permettre de sélectionner des modèles comportant un nombre différent de variables explicatives en définissant le coefficient de détermination ajusté \\(\\widetilde R^2_m\\). Ce coefficient permet de tenir compte du nombre de régresseurs retenus et propose donc un compromis entre l’adéquation et le paramétrage du modèle. Cet indice est défini par : \\[ \\widetilde{R^2}_m = 1- \\frac{n-1}{n-|m|-1}.\\frac{SSR(m)}{SST}= 1- \\frac{n-1}{n-|m|-1}.\\frac{\\| Y - X_m \\hat\\theta_{(m)} \\|^2}{\\| Y - \\overline Y \\mathbb{1}_n\\|^2}.\\] L’interprétation est similaire à celle du \\(R^2\\). 6.4.2.2 Les stratégies de sélections ascendantes et descendantes par le test de Fisher Le coefficient d’ajustement peut être utilisé en présence d’un petit nombre de modèles. Dans le cas contraire, on peut utiliser une stratégie dite de régression descendante faisant appel au test de Fisher sur la présence d’un sous-modèle. La méthodologie est la suivante: on part du modèle utilisant tous les régresseurs possibles. À chaque étape, on calcule la statistique de Fisher correspondant au retrait de chacune des variables encore présentes. On retire alors la variable possédant la plus petite valeur, i.e. la plus grande \\(p\\)-valeur. En fait à chaque étape, on retire la variable la moins significative au sens du test de Fisher. On réitère ensuite ce processus jusqu’à ce que toutes les statistiques soient supérieures à un seuil pré-déterminé, i.e. lorsque toutes les \\(p\\)-valeurs sont toutes plus petites qu’un seuil fixé au préalable, par exemple \\(5\\%\\). Attention, cette stratégie peut être extrêmement lourde à mettre en place suivant le nombre de variables en question (on peut aller jusqu’à \\(|m|!\\) tests de Fisher). Initialisation : on se donne un seuil \\(s\\) et \\(m_{[0]} =\\{1,\\ldots,p\\}\\) Itération \\(t\\) : Etape 1 : Pour tout \\(j\\in m_{[t]}\\), on calcule la p-valeur \\(p_j\\) du test de Fisher de sous-modèle de \\[ (M_0) : m_{[t]}\\setminus \\{j\\} \\textrm{ contre } (M_1) : m_{[t]} \\] Etape 2 : \\(\\hat{\\jmath} = \\arg\\underset{j\\in m_{[t]}}{\\max}\\ p_j\\) Etape 3 : Si \\(p_{\\hat{\\jmath}} &gt;s\\), \\(m_{[t+1]} = m_{[t]} \\setminus \\{\\hat{\\jmath}\\}\\) et on retourne à l’étape 1 Sinon stop. La sélection de modèle par régression ascendante reprend exactement les mêmes arguments, sauf que l’on part du modèle vide (sans régresseur, uniquement l’intercept) et l’on rajoute au fur et à mesure les variables les plus significatives (au sens du test de Fisher), jusqu’au dépassement par les p-valeurs d’un seuil fixé préalablement. 6.4.2.3 Le critère \\(C_p\\) de Mallows Le risque quadratique est un critère usuel pour mesurer l’écart entre le vrai modèle \\(m^\\star\\) et un modèle d’analyse \\(m\\in\\mathcal M\\). Definition 6.2 Soit \\(m\\in \\mathcal{M}\\). Le risque quadratique entre les modèles \\(m\\) et \\(m^{\\star}\\) est défini par : \\[\\mathcal R(m,m^{\\star})=\\mathbb{E}\\left[\\left\\|\\mu^{\\star}-\\widehat{Y}_{(m)}\\right\\|^2\\right]= \\mathbb{E}\\left[ \\left\\| X_{(m^{\\star})} \\theta_{(m^{\\star})} - X_{(m)} \\hat\\theta_{(m)} \\right\\|^2 \\right],\\] où \\(\\mu^{\\star}=X_{(m^{\\star})} \\theta_{(m^{\\star})}\\) et ${(m)}=X{(m)} _{(m)} $. Par la suite, pour tout \\(m\\in \\mathcal{M}\\), on définit \\(\\mu_{(m)}^{\\star}=P_{[X_{(m)}]} \\mu^{\\star}\\), le projeté orthogonal de \\(\\mu^{\\star}\\) sur l’espace vectoriel \\(Im(X_{(m)})\\). Il est alors possible de calculer explicitement ce risque quadratique. Proposition 6.2 Pour tout \\(m\\in \\mathcal{M}\\), on a : \\[\\begin{equation} \\mathcal R(m,m^{\\star}) = \\sigma^{\\star\\,2} (|m|+1) + \\| \\mu_{(m)}^{\\star} - \\mu^{\\star} \\|^2. \\tag{6.4} \\end{equation}\\] La preuve de la proposition 6.2 est donnée en annexe B.3. Afin de minimiser la distance entre \\(m\\) et \\(m^{\\star}\\), il y a donc un compromis à trouver. Si \\(|m|\\) est petit, il en sera de même pour le terme de variance \\(\\sigma^{\\star\\,2} (|m|+1)\\), au dépend du terme de biais \\(\\| \\mu_{(m)}^{\\star} - \\mu^{\\star} \\|^2\\). Au contraire, pour de grandes valeurs de \\(|m|\\), on peut espérer avoir un petit biais, mais au risque d’avoir une erreur plus importante, ce qui se traduit par une augmentation du terme \\(\\sigma^{\\star\\,2} (|m|+1)\\). Ce compromis biais-variance est très classique dans ce cadre de sélection de modèle et se retrouve dans un grand nombre de thématiques. Remark. À partir du moment où \\(m^{\\star} \\subset m\\), on a \\(\\| \\mu_{(m)}^{\\star} - \\mu^{\\star} \\|^2=0\\), puisque \\(\\mu_{(m)}^{\\star}\\) correspond au projeté orthogonal de \\(\\mu^{\\star}\\) sur \\([X_{(m)}]\\). La question qui se pose à présent est : comment approcher le modèle qui va minimiser le risque quadratique ? Clairement, trouver le meilleur modèle possible nécessite la connaissance de \\(\\mu^{\\star}\\)… que l’on cherche justement à estimer ! L’idée proposée par Mallows (Mallows 2000) consiste à estimer le risque quadratique à partir des données elles-mêmes et de prendre ensuite une décision à partir de cette estimation. Le modèle \\(\\hat m_{CP}\\) retenu vérifie : \\[\\hat m_{CP} = \\mathrm{arg} \\min_{m\\in \\mathcal{M}} C_p(m)\\] où le critère \\(C_p\\) de Mallows est défini par \\[C_p(m)= \\| Y - \\widehat{Y}_{(m)} \\|^2 + 2|m| \\sigma^2\\] si la variance est connue. Dans le cas où la variance est inconnue, on utilisera l’estimateur \\(\\widehat{\\sigma}^2 = \\widehat{\\sigma}^2_{(m_p)}\\) où \\(m_p=\\lbrace 1,\\dots, p \\rbrace\\) est le modèle prenant en compte tous les régresseurs. La construction de ce critère est présentée en annexe B.5. 6.4.2.4 Les critères AIC et BIC Le critère \\(C_p\\) de Mallows est basé sur une volonté de minimiser la distance entre \\(m\\) et le vrai modèle au sens du risque quadratique. Les critères AIC (Akaike Information Criterion) (Akaike (1978),Akaike (1998)) et BIC (Bayesian Information Criterion) (Schwarz and others 1978) sont eux construits pour minimiser la dissemblance de Kullback entre les 2 modèles. À chaque modèle d’analyse qui, en général, est un faux modèle, on peut faire correspondre la mesure de probabilité de \\(Y\\) en procédant comme si ce modèle d’analyse était réellement le vrai modèle. On fait donc correspondre la loi de \\(X_{(m)}\\hat\\theta_{(m)} +\\hat \\varepsilon\\). On peut ainsi mesurer l’écart entre la loi du vrai modèle (loi paramétrée par des paramètres inconnus) et la loi engendrée par le modèle d’analyse. Pour mesurer cet écart, un outil souvent utilisé est la dissemblance de Kullback-Leibler. Definition 6.3 Soient \\(\\mathbb{P}\\) et \\(\\mathbb{P}^{\\star}\\) deux mesures de probabilité dominées par une même mesure (dans notre cas la mesure de Lebesgue). La dissemblance de Kullback entre ces deux mesures est donnée par: \\[KL(\\mathbb{P}^\\star,\\mathbb{P})= \\mathbb{E}_{\\mathbb{P}^{\\star}} \\left[ \\log \\frac{d \\mathbb{P}^{\\star}}{d \\mathbb{P}} \\right].\\] Si \\(\\displaystyle f =\\frac{d\\mathbb{P}}{d\\nu}\\) et si \\(\\displaystyle f^{\\star} =\\frac{d\\mathbb{P}^{\\star}}{d\\nu}\\), alors \\(\\displaystyle KL(\\mathbb{P}^{\\star},\\P)=\\left\\{ \\begin{array}{l} \\int f^{\\star}\\log \\frac{f^{\\star}}{f}d\\nu \\mbox{ si } \\mathbb{P}^{\\star} \\ll \\P,\\\\ +\\infty \\mbox{ sinon.} \\end{array} \\right.\\) Remarquons, en premier lieu la non symétrie de \\(KL(.,.)\\). C’est pour cette raison que l’on préfèrera parler de dissemblance plutôt que de distance. Cependant, cette dissemblance vérifie comme toute distance “classique” les propriétés suivantes : Ces propriétés peuvent être démontrées par des arguments de convexité. Dans le cas où les erreurs sont gaussiennes, ce que nous avons supposé jusqu’à présent, il est possible d’obtenir une expression relativement simple de la dissemblance de Kullback. Proposition 6.3 Soit \\(m\\in \\mathcal{M}\\) fixé. On a alors : \\[KL(m^{\\star},m) = \\frac{n}{2} \\left[ \\log\\left( \\frac{\\sigma_{(m)}^2}{\\sigma^{\\star\\, 2}} \\right) + \\frac{\\sigma^{\\star\\,2}}{\\sigma_{(m)}^2} - 1 \\right] + \\frac{1}{2\\sigma_{(m)}^2} \\|\\mu^{\\star}- \\mu_{(m)}^\\star\\|^2,\\] où \\(KL(m^{\\star},m)\\) désigne la dissemblance de Kullback entre les deux modèles \\(m^{\\star}\\) et \\(m\\). La preuve de la proposition 6.3 en annexe B.4. Proposition 6.4 Le critère AIC consiste à sélection le modèle vérifiant \\[ \\hat m = \\mathrm{arg} \\min_{m\\in \\mathcal{M}} \\mbox{AIC}(m) \\] avec \\[ \\mbox{AIC}(m) = - 2 \\textrm{logvraisemblance au maximum de vraisemblance} + 2 D_m \\] où \\(D_m\\) est la dimension du modèle \\(m\\) (i.e le nombre de paramètres pour le modèle \\(m\\)). Nous n’allons pas ici présenter la construction théorique de ce critère AIC. Une preuve est disponible dans (Azaïs and Bardet 2005). Dans le cas gaussien, la logvraisemblance au maximum de vraisemblance vaut \\[\\ln\\left[ (2\\pi \\tilde \\sigma^2_{(m)})^{-n/2} \\exp\\left(-\\frac{1}{2 \\tilde \\sigma^2_{(m)}} \\|Y - \\hat Y_{(m)}\\|^2\\right)\\right] = -\\frac n 2 \\ln(2\\pi) -\\frac n 2 \\ln(\\tilde \\sigma^2_{(m)}) -\\frac{n}{2}\\] car \\(\\tilde \\sigma^2_{(m)} = \\frac 1 n \\|Y - \\hat Y_{(m)}\\|^2.\\) Ainsi la sélection de modèle par le critère AIC peut se réécrire sous la forme \\[ \\hat m = \\mathrm{arg} \\min_{m\\in \\mathcal{M}} n \\ln(\\tilde \\sigma^2_{(m)}) + 2 (|m|+2). \\] Ce type de critère fonctionne plutôt bien pour de petites collections de modèles. Des simulations numériques montrent toutefois que la qualité d’estimation a tendance à se dégrader lorsque \\(m\\) augmente. Afin de pallier ce problème, il est possible d’utiliser le critère \\(AIC\\) corrigé : \\[ \\mbox{AIC}_c(m) = n \\ln \\left( \\tilde \\sigma_{(m)}^2 \\right)+ n \\frac{n+|m|-1}{n-|m| -3}. \\] Le critère BIC (Bayesian Information Criterion) introduit en 1978 par Schwarz (Schwarz and others 1978), est une extension de l’écriture générale du critère d’AIC et utilise le point de vue bayésien. On ne considère plus le paramètre inconnu \\(\\theta\\) comme un vecteur de \\(\\mathbb{R}^{p+1}\\) mais plutôt comme une variable aléatoire à valeurs dans \\(\\mathbb{R}^{p+1}\\). Une loi a priori est alors placée sur le ‘paramètre’ à estimer. La démarche consiste ensuite à essayer d’exploiter cette information pour l’estimation. Ce type d’approche apporte en théorie plus de richesse puisque l’on étend l’éventail des solutions possibles. Cette approche conduit au critère BIC défini par : \\[\\begin{equation} BIC(m)= n \\log (\\hat\\sigma^2_{(m)} ) + \\log n \\times |m|. \\tag{6.5} \\end{equation}\\] Le modèle correspondant \\(\\hat m_{BIC}\\) est obtenu en posant : \\[\\begin{equation} \\hat m_{BIC} = \\mathrm{arg} \\min_{m \\in \\mathcal{M}} BIC(m). \\tag{6.6} \\end{equation}\\] Nous ne nous étendrons pas sur les détails permettant d’arriver à la construction de ce critère. 6.4.3 Algorithmes de sélection de variables En pratique, une fois un critère de sélection de modèles choisi, la détermination du “meilleur” modèle par une recherche exhaustive est impossible en raison du nombre de modèles à explorer. On a donc recourt à des méthodes pas à pas : Les méthodes descendantes : On part du modèle en utilisant les \\(p\\) variables explicatives et on cherche, à chaque étape de l’algorithme, la variable la plus pertinente à retirer selon le critère choisi. On itère ainsi l’algorithme jusqu’à atteindre l’ensemble vide. Parmi les ensembles de variables visités pendant l’algorithme, on retient le meilleur au vu du critère. Certains algorithmes s’arrêtent dès lors qu’un seuil donné est atteint. Initialisation : \\(m_{[0]} = \\{1,\\ldots,p\\}\\) Itération \\(t\\) : Etape 1 Pour tout \\(j\\in m_{[t]}\\), on calcule \\(c_j = \\mbox{CRIT}(m_{[t]} \\setminus \\{j\\} )\\). Etape 2] \\(\\hat{\\jmath} = \\arg\\underset{j\\in m_{[t]}}{\\max}\\ c_j\\) Etape 3 \\(m_{[t+1]} = m_{[t]} \\setminus \\{\\hat{\\jmath}\\}\\) Si \\(m_{[t+1]} \\neq \\emptyset\\), on retourne à l’étape 1 Sinon stop. Les méthodes ascendantes : On part de l’ensemble vide de variables et on cherche, à chaque étape de l’algorithme, la variable la plus pertinente à ajouter selon le critère choisi. On itère ainsi l’algorithme jusqu’à intégrer toutes les variables. Parmi les ensembles de variables visités pendant l’algorithme, on retient le meilleur au vu du critère. Certains algorithmes s’arrêtent dès lors qu’un seuil donné est atteint. Initialisation : \\(m_{[0]} = \\emptyset\\) Itération \\(t\\) : Etape 1 Pour tout \\(j\\in \\{1,\\ldots,p\\} \\setminus m_{[t]}\\),\\ on calcule \\(c_j = \\mbox{CRIT}(m_{[t]} \\cup \\{j\\} )\\). Etape 2 \\(\\hat{\\jmath} = \\arg\\underset{j}{\\min}\\ c_j\\) Etape 3 \\(m_{[t+1]} = m_{[t]} \\cup \\{\\hat{\\jmath}\\}\\) Si \\(m_{[t+1]} \\neq \\{1,\\ldots,p\\}\\), on retourne à l’étape 1 Sinon stop. Les méthodes stepwise : Partant d’un modèle donné, on opère une sélection d’une nouvelle variable (comme avec une méthode ascendante), puis on cherche si on peut éliminer une des variables du modèle (comme pour une méthode descendante) et ainsi de suite. Il faut définir pour une telle méthode un critère d’entrée et un critère de sortie. On peut citer la méthode des “\\(s\\) best subsets” ( ou “\\(s\\) meilleurs sous-ensembles”) : On cherche de façon exhaustive parmi tous les sous-ensembles de \\(s\\) variables, les \\(s\\) meilleures, au sens du critère considéré. 6.4.4 Illustration sur l’exemple Dans cette section, nous allons illustrer sur notre exemple quelques stratégies de sélection de variables. Grâce à la fonction regsubsets(), on peut mettre en place une méthode ascendante, descendante ou séquentielle. On peut également choisir un critère parmi le Cp de Mallows, le \\(R^2\\) ajusté et le critère BIC. library(leaps) choixb&lt;-regsubsets(oxy~.,data=fitness,nbest=1,nvmax=10,method=&quot;backward&quot;) choixf&lt;-regsubsets(oxy~.,data=fitness,nbest=1,nvmax=10,method=&quot;forward&quot;) plot(choixb,scale=&quot;Cp&quot;) Figure 6.6: Sélection de variables avec le critère Cp de Mallows plot(choixb,scale=&quot;adjr2&quot;) Figure 6.7: Sélection de variables avec le R2 ajusté. plot(choixb,scale=&quot;bic&quot;) Figure 6.8: Sélection de variables avec le critère BIC Par exemple, le Cp de Mallows et le critère BIC (Figures 6.6 et 6.8), on retient le modèle composé des variables age, runtime, maxpulse et runpulse. Avec le \\(R^2\\) ajusté, on conserve en plus la variable weight (Figure 6.7). On valide ensuite cette proposition de sous-modèle par un test de Fisher adapté : reg.fin&lt;-lm(oxy~age+runtime+maxpulse+runpulse,data=fitness) anova(reg.fin,reg.multi) Analysis of Variance Table Model 1: oxy ~ age + runtime + maxpulse + runpulse Model 2: oxy ~ age + weight + runtime + rstpulse + runpulse + maxpulse Res.Df RSS Df Sum of Sq F Pr(&gt;F) 1 26 138.93 2 24 128.84 2 10.092 0.94 0.4045 On peut aussi utiliser la fonction stepAIC() pour faire de la sélection de variable avec les critère AIC (\\(k=2\\) par défaut) ou le critère BIC (mettre l’option \\(k=log(nrow(Data))\\)). library(MASS) modselect_aic=stepAIC(reg.multi,trace=F,direction=&quot;backward&quot;) modselect_bic=stepAIC(reg.multi,trace=T,direction=&quot;backward&quot;,k=log(nrow(fitness))) Start: AIC=68.2 oxy ~ age + weight + runtime + rstpulse + runpulse + maxpulse Df Sum of Sq RSS AIC - rstpulse 1 0.571 129.41 64.903 - weight 1 9.911 138.75 67.063 &lt;none&gt; 128.84 68.200 - maxpulse 1 26.491 155.33 70.562 - age 1 27.746 156.58 70.812 - runpulse 1 51.058 179.90 75.114 - runtime 1 250.822 379.66 98.268 Step: AIC=64.9 oxy ~ age + weight + runtime + runpulse + maxpulse Df Sum of Sq RSS AIC - weight 1 9.52 138.93 63.669 &lt;none&gt; 129.41 64.903 - maxpulse 1 26.83 156.23 67.309 - age 1 27.37 156.78 67.417 - runpulse 1 52.60 182.00 72.041 - runtime 1 320.36 449.77 100.087 Step: AIC=63.67 oxy ~ age + runtime + runpulse + maxpulse Df Sum of Sq RSS AIC &lt;none&gt; 138.93 63.669 - maxpulse 1 21.90 160.83 64.773 - age 1 22.84 161.77 64.954 - runpulse 1 46.90 185.83 69.252 - runtime 1 352.94 491.87 99.427 Avec une procédure descendante et le critère AIC, on retient le même sous-modèle avec les variables explicatives age, runtime, maxpulse et runpulse. 6.5 Régression linéaire régularisée Quand on se retrouve avec un modèle singulier, \\(\\mbox{rg}(X) &lt; k\\), la matrice \\(X&#39;X\\) n’est plus inversible. Ce cas se présente quand le nombre de variables explicatives est supérieur au nombre d’observations (\\(n&lt;p\\)) \\(n&gt;p\\) mais des variables sont linéairement redondantes (la famille \\(\\{X^{(1)},\\ldots,X^{(p)}\\}\\) est liée) Dans cette situation, on a vu précédemment que l’estimateur des moindres carrés \\(\\widehat{\\theta}\\) n’existe pas. La projection \\(\\widehat{Y}=P_{[X]}Y\\) de la réponse \\(Y\\) sur \\(Im(X)=[X]\\) n’a pas une décomposition unique sur les colonnes de X (le modèle est non identifiable, voir Chapitre 5). De plus, comme la matrice de variance-covariance de \\(\\widehat{\\theta}\\) vaut \\(\\sigma^2 (X&#39;X)^{-1}\\), la précision de l’estimateur \\(\\widehat{\\theta}\\) diminue quand \\(X&#39;X\\) se rapproche d’une matrice non inversible. Du point de vue de la prédiction, si \\(x^\\star\\) est un nouveau vecteur de valeurs des variables explicatives, on sait que la qualité (au sens écart quadratique) de la prédiction \\(\\hat Y^\\star\\) de la vraie réponse \\(Y^\\star\\) se décompose en le biais\\(^2\\) + variance. Donc pour améliorer la prédiction, on peut préférer une augmentation légère du biais pour avoir une diminution de la variance. On va donc chercher dans ce contexte à utiliser des méthodes de régression dites régularisées pour pallier ces difficultés. Elles ont pour formalisme commun l’optimisation d’un critère de la forme \\[\\underset{\\theta\\in\\mathbb{R}^k}{\\mbox{argmin}}\\ \\|Y - X \\theta\\|^2 + \\lambda\\ \\mbox{pen}(\\theta)\\] où \\(\\lambda&gt;0\\) est une quantité à choisir. Elles se distinguent par la forme de la fonction de pénalité \\(\\mbox{pen}(\\theta)\\) qui fera intervenir le contrôle d’une norme de \\(\\theta\\). En pratique on commence par centrer et réduire les variables explicatives \\(z^{(j)}\\) pour ne pas pénaliser ou favoriser un coefficient de \\(\\theta\\) car les pénalisations que nous allons considérer portent sur une norme de \\(\\theta\\). Il est donc préférable que chaque coefficient soit affecté de façon “semblable”. La matrice des variables explicatives centrées-réduites est notée \\(\\tilde X\\). De plus, l’intercept \\(\\theta_0\\) étant un coefficient qui a un rôle particulier assurant au modèle de se positionner autour du comportement moyen de \\(Y\\), il n’a pas à intervenir dans la contrainte sur la norme de \\(\\theta\\). Aussi, on centre le vecteur réponse \\(Y\\), \\(\\tilde Y = Y - \\bar Y \\mathbb{1}_n\\), et on peut potentiellement le réduire. A noter que le modèle est alors de la forme \\(\\tilde Y = \\tilde X \\theta + \\varepsilon\\) avec \\(\\theta=(\\theta_1,\\ldots,\\theta_p)&#39;\\) (donc \\(k=p\\) et sans intercept). Ainsi, après transformation initiale des données, nous allons ici nous intéresser à des méthodes de régression régularisées qui cherchent à minimiser le risque empirique régularisé (pour la perte quadratique) : \\[ \\underset{\\theta\\in\\mathbb{R}^k}{\\mbox{argmin}}\\ \\left\\{\\|\\tilde Y - \\tilde X \\theta\\|^2 + \\lambda \\|\\theta\\|_q^q\\right\\} \\textrm{ où } \\|\\theta\\|_q^q = \\sum_{j=1}^p (\\theta_j)^q. \\] On parle de régression ridge quand \\(q=2\\), de régression Lasso quand \\(q=1\\). Nous allons détailler ces deux méthodes et la régression Elasticnet qui combine les deux premières. Pour illustrer cette section, nous reprenons le jeu de données fitness auquel on a ajouté \\(5\\) variables de bruit (simulation selon une loi \\(\\mathcal{N}(0,1)\\)). set.seed(1234) fitnessplus = cbind(fitness$oxy,fitness[,-3],matrix(rnorm(n=nrow(fitness)*5,0,1),nrow=nrow(fitness))) colnames(fitnessplus)=c(colnames(fitness)[3],colnames(fitness[,-3]),paste(&quot;Z&quot;,1:5,sep=&quot;&quot;)) y_var=fitnessplus[,1] x_var=fitnessplus[,-1] tildeY=scale(y_var,center=T,scale=T) tildeX=scale(x_var,center=T,scale=T) corrplot(cor(fitnessplus),method=&quot;ellipse&quot;,tl.cex=0.5) 6.5.1 Régression ridge Dans le contexte présenté précédemment, la difficulté vient de l’inversibilité de \\(\\tilde X&#39; \\tilde X\\in\\mathcal{M}_p(\\mathbb{R})\\). Cette matrice \\(\\tilde X&#39; \\tilde X\\) est une matrice semi-définie positive donc ses valeurs propres sont positives et on les ordonne \\(\\tau_1\\geq\\tau_2\\geq\\ldots\\geq \\tau_p\\). Si \\(\\tilde X&#39; \\tilde X\\) n’est pas inversible, c’est qu’au moins l’une de ses valeurs propres est nulle. Proposition 6.5 Soit \\(\\lambda&gt;0\\). Les matrices \\(\\tilde X&#39; \\tilde X\\) et \\(\\tilde X&#39; \\tilde X + \\lambda I_p\\) ont les mêmes vecteurs propres mais leur valeurs propres sont \\(\\{\\tau_j\\}_{j\\in[|1,p|]}\\) et \\(\\{\\tau_j + \\lambda\\}_{j\\in[|1,p|]}\\) respectivement. Ainsi, \\(det( \\tilde X&#39; \\tilde X + \\lambda I_p) &gt; det( \\tilde X&#39; \\tilde X)\\), donc \\(\\tilde X&#39; \\tilde X + \\lambda I_p\\) a “plus de chance” d’être inversible que \\(\\tilde X&#39; \\tilde X\\). En exploitant la Proposition 6.5, l’idée consiste à remplacer \\((\\tilde X&#39; \\tilde X)^{-1}\\) dans l’expression de l’estimateur des moindres carrés \\(\\widehat{\\theta}\\) par \\(( \\tilde X&#39; \\tilde X+ \\lambda I_p)^{-1}\\). Ainsi l’estimateur ridge est donné par \\[ \\widehat{\\theta}_{\\scriptsize ridge} (\\lambda) = (\\tilde X&#39;\\tilde X+ \\lambda I_p)^{-1} \\tilde X&#39; \\tilde Y. \\] Cet estimateur ridge est solution du problème optimisation suivant \\[ \\widehat{\\theta}_{\\scriptsize ridge} (\\lambda) \\in \\underset{\\theta\\in\\mathbb{R}^p}{\\mbox{argmin}}\\ \\|\\tilde Y - \\tilde X \\theta\\|_2^2 + \\lambda \\|\\theta\\|_2^2, \\] qui peut être reformulé en le problème de minimisation sous contrainte suivant : \\[\\|\\tilde Y - \\tilde X \\theta\\|_2^2 \\textrm{ sous la contrainte } \\|\\theta\\|_2^2\\leq r(\\lambda)\\] où \\(r(.)\\) est bijective. La régression ridge conserve toutes les variables mais avec la contrainte \\(\\|\\theta\\|_2^2\\leq r(\\lambda)\\), elle empêche les estimateurs de prendre de trop grandes valeurs et limite ainsi la variance des prédictions. On parle de “shrinkage” car on rétrécit l’étendue des valeurs possibles des paramètres estimés. Proposition 6.6 Soit l’estimateur ridge \\(\\widehat{\\theta}_{\\scriptsize ridge} (\\lambda) = (\\tilde X&#39;\\tilde X+ \\lambda I_p)^{-1} \\tilde X&#39; \\tilde Y\\). On a \\(\\mathbb{E}[\\widehat{\\theta}_{\\scriptsize ridge} (\\lambda) ] = \\theta - \\lambda(\\tilde X&#39;\\tilde X+ \\tau I_p)^{-1} \\theta\\) donc il est biaisé. \\(\\mbox{Var}(\\widehat{\\theta}_{\\scriptsize ridge} (\\lambda) ) = \\sigma^2 (\\tilde X&#39;\\tilde X+ \\lambda I_p)^{-1} (\\tilde X&#39;\\tilde X) (\\tilde X&#39;\\tilde X+ \\lambda I_p)^{-1} \\leq \\sigma^2 (\\tilde X&#39;\\tilde X)^{-1} = \\mbox{Var}(\\widehat{\\theta})\\). Les valeurs ajustées pour \\(Y\\) sont \\[ \\widehat{Y}_{\\scriptsize ridge}(\\lambda) = \\tilde X \\widehat{\\theta}_{\\scriptsize ridge} (\\lambda) + \\bar Y \\mathbb{1}_n \\] Quand \\(\\lambda \\rightarrow +\\infty\\), \\(\\widehat{\\theta}_{\\scriptsize ridge} (\\lambda)\\rightarrow 0\\) Quand \\(\\lambda\\rightarrow 0\\), \\(\\widehat{\\theta}_{\\scriptsize ridge} (\\lambda)\\rightarrow \\widehat{\\theta}\\) L’estimateur \\(\\widehat{\\theta}_{\\scriptsize ridge} (\\lambda)\\) dépend du choix de \\(\\lambda\\) qui est un point délicat. C’est pratiquement impossible de pouvoir faire ce choix a priori. On peut tracer le chemin de régularisation de la régression ridge qui est l’ensemble des fonctions \\(\\tau\\mapsto (\\widehat{\\theta}_{\\scriptsize ridge} (\\lambda))_j\\) pour \\(j=1,\\ldots,p\\) (voir Figure 6.9). On constate que le chemin de régularisation de la régression ridge est continu, ne permettant pas un ajustement aisé de \\(\\tau\\). On peut également suivre les recommandations proposées dans la littérature, voir par exemple (Hoerl, Kannard, and Baldwin 1975), (Hoerl and Kennard 1976), (Mallows 2000) et (McDonald and Galarneau 1975). En pratique, on passe par une procédure de validation croisée pour calibrer \\(\\tau\\) (Figure 6.10): On commence par séparer les données en un jeu d’apprentissage \\((Y_a,X_a)\\) et un jeu de test \\((Y_v,X_v)\\). On estime alors la régression ridge sur le jeu d’apprentissage pour chaque valeur de \\(\\tau\\) dans une grille de valeurs choisie et on prédit la réponse sur le jeu de test pour chaque valeur de \\(\\lambda\\): \\(\\widehat{Y}_{{\\scriptsize ridge}, v}(\\lambda)\\). La qualité du modèle est alors obtenue en comparant les vraies données \\(Y_v\\) et les valeurs prédites \\(\\widehat{Y}_{{\\scriptsize ridge}, v}(\\lambda)\\). Par exemple, on peut utiliser le critère PRESS \\[ PRESS(\\lambda) = \\|Y_v - \\widehat{Y}_{{\\scriptsize ridge}, v}(\\lambda)\\|^2. \\] Finalement on choisit la valeur de \\(\\lambda\\) qui minime ce critère. Le principe de la validation croisée est de répéter plusieurs fois le découpage entre test et apprentissage et de considérer la moyenne des valeurs du critère pour chaque valeur de \\(\\lambda\\). lambda_seq &lt;- seq(0, 1, by = 0.001) fitridge &lt;- glmnet(tildeX,tildeY, alpha = 0, lambda = lambda_seq,family=c(&quot;gaussian&quot;),intercept=F) df=data.frame(tau = rep(-log(fitridge$lambda),ncol(tildeX)), theta=as.vector(t(fitridge$beta)), variable=rep(colnames(x_var),each=length(fitridge$lambda))) g1 = ggplot(df,aes(x=tau,y=theta,col=variable))+ geom_line()+ ylab(&#39;Estimator of theta&#39;)+xlab(&quot;-log(lambda)&quot;)+ theme(legend.title = element_text(size = 5),legend.text = element_text(size = 3)) g1 Figure 6.9: Chemins de regularisation pour la régression ridge sur notre exemple. ridge_cv &lt;- cv.glmnet(tildeX, tildeY, alpha = 0, lambda = lambda_seq,nfolds=10, type.measure=c(&quot;mse&quot;),intercept=F) best_lambda &lt;- ridge_cv$lambda.min g1+geom_vline(xintercept = -log(best_lambda),linetype=&quot;dotted&quot;,color = &quot;red&quot;)+ xlim(c(0,-log(best_lambda)+2)) Figure 6.10: Sélection de lambda par validation croisée pour la régression ridge sur notre exemple. 6.5.2 Régression Lasso L’idée de la régression LASSO (Least Absolute Selection and Shrinkage Operator) proposée par Tibshirani (Tibshirani 1996) est d’essayer d’annuler des coefficients du vecteur \\(\\theta\\) afin d’avoir un estimateur parcimonieux (sparse en anglais). Cela induit une sélection de variables rendant le modèle plus interprétable et une matrice des variables explicatives avec de meilleures propriétés que \\(X&#39;X\\). Pour forcer à annuler des coordonnées de \\(\\theta\\), on contraint la norme \\(\\ell_1\\) : \\(\\|\\theta\\|_1 = \\sum_{j=1}^p |\\theta_j|\\). Comme pour la régression ridge, on commence par centrer-réduire les variables explicatives (\\(\\tilde X\\)) et au moins centrer le vecteur des réponses (\\(\\tilde Y\\)). L’estimateur LASSO est défini pour \\(\\tau&gt;0\\) par \\[\\begin{equation} \\tag{6.7} \\widehat{\\theta}_{{\\scriptsize lasso}}(\\lambda) \\in \\underset{\\theta\\in\\mathbb{R}^p}{\\mbox{argmin}}\\ \\|\\tilde Y - \\tilde X \\theta\\|_2^2 + \\lambda \\|\\theta\\|_1. \\end{equation}\\] Ce problème de minimisation est équivalent à minimiser \\(\\|\\tilde Y - \\tilde X \\theta\\|_2^2\\) sous la contrainte \\(\\|\\theta\\|_1\\leq r(\\lambda)\\) avec \\(r(.)\\) bijective. La solution du problème (6.7) peut ne pas être unique mais le vecteur des valeurs ajustées en résultant \\(\\tilde X \\widehat{\\theta}_{{\\scriptsize lasso}}(\\lambda)\\) est lui toujours unique. L’estimateur LASSO a l’avantage d’avoir un certain nombre de coefficients nuls lorsque \\(\\lambda\\) est suffisamment grand. C’est un estimateur parcimonieux qui induit une sélection des variables. Quand \\(\\lambda=0\\), \\(\\widehat{\\theta}_{{\\scriptsize lasso}}(0) = \\widehat{\\theta}\\); quand \\(\\lambda\\rightarrow +\\infty\\), \\(\\widehat{\\theta}_{{\\scriptsize lasso}}(+\\infty)=0\\). Comme pour la régression ridge, le choix de \\(\\lambda\\) est délicat, il est impossible de faire ce choix a priori. On peut tracer le chemin de régularisation de la régression Lasso c’est-à-dire l’ensemble des fonctions \\(\\lambda\\mapsto \\widehat{\\theta}_{{\\scriptsize lasso}}(\\lambda)_j\\) pour \\(j=1,\\ldots,p\\) (voir Figure 6.11). Comme pour la régression ridge, on passe par une procédure de validation croisée pour stabiliser le choix de \\(\\lambda\\) (voir Figure ??). lambda_seq=seq(0,1,0.001) fitlasso &lt;- glmnet(tildeX,tildeY, alpha = 1, lambda = lambda_seq,family=c(&quot;gaussian&quot;),intercept=F) lasso_cv &lt;- cv.glmnet(tildeX, tildeY, alpha = 1, lambda = lambda_seq,nfolds=10,type.measure=c(&quot;mse&quot;),intercept=F) best_lambda &lt;-lasso_cv$lambda.min # red best_lambda.1se &lt;- lasso_cv$lambda.1se # black df=data.frame(tau = rep(-log(fitlasso$lambda),ncol(tildeX)), theta=as.vector(t(fitlasso$beta)),variable=rep(colnames(x_var),each=length(fitlasso$lambda))) g3 = ggplot(df,aes(x=tau,y=theta,col=variable))+ geom_line()+ ylab(&quot;Estim. de theta&quot;)+ xlab(&quot;-log(lambda)&quot;) g3 + geom_vline(xintercept = -log(best_lambda),linetype=&quot;dotted&quot;, color = &quot;red&quot;)+ geom_vline(xintercept = -log(lasso_cv$lambda.1se),linetype=&quot;dotted&quot;, color = &quot;black&quot;) Figure 6.11: Chemins de régularisation pour la régression Lasso sur notre exemple. 6.5.3 Régression Elastic-Net La régression Elastic-Net combine les avantages de la régression ridge et de la régression Lasso. En particulier, elle pallie le défaut de l’estimation Lasso lorsque les \\(x^{(j)}\\) sont fortement corrélées. L’estimateur Elastic-Net (Zou and Hastie 2005) est défini pour \\(\\lambda&gt;0\\) et \\(\\alpha&gt;0\\) par \\[ \\widehat{\\theta}_{{\\scriptsize net}}(\\lambda,\\alpha) \\in \\underset{\\theta\\in\\mathbb{R}^p}{\\mbox{argmin}}\\ \\|\\tilde Y - \\tilde X\\theta\\|_2^2 + \\lambda \\{\\alpha \\|\\theta\\|_1 + (1-\\alpha) \\|\\theta\\|_2^2\\} \\] ce qui peut se reformuler en minimiser \\(\\|\\tilde Y - \\tilde X \\theta\\|_2^2\\) sous la contrainte \\(\\alpha \\|\\theta\\|_1 + (1-\\alpha) \\|\\theta\\|_2^2 \\leq r(\\lambda)\\). Il faut alors utiliser des algorithmes d’optimisation pour déterminer \\(\\widehat{\\theta}_{{\\scriptsize net}}(\\lambda,\\alpha)\\) et la calibration des seuils \\(\\lambda\\) et \\(\\alpha\\) est souvent faite par validation croisée en pratique. La Figure 6.12 illustre les différences sur les chemins de régularisation des trois méthodes. fitEN &lt;- glmnet(tildeX,tildeY, alpha = 0.3, lambda = lambda_seq,family=c(&quot;gaussian&quot;),intercept=F) EN_cv &lt;- cv.glmnet(tildeX, tildeY, alpha = 0.3, lambda = lambda_seq,nfolds=10,type.measure=c(&quot;mse&quot;),intercept=F) best_lambda &lt;-EN_cv$lambda.min df=data.frame(tau = rep(-log(fitEN$lambda),ncol(tildeX)), theta=as.vector(t(fitEN$beta)),variable=rep(colnames(x_var),each=length(fitEN$lambda))) g6 = ggplot(df,aes(x=tau,y=theta,col=variable))+ geom_line()+ geom_vline(xintercept =-log(best_lambda),linetype=&quot;dotted&quot;,color = &quot;red&quot;)+ ylab(&quot;Estim. theta&quot;)+ xlab(&quot;-log(lambda)&quot;) g6 Figure 6.12: Chemins de régularisation pour 3 variables du jeu de données pour la régression Lasso (alpha=1), régression ridge (alpha=0) et la régression Elastic Net (ici alpha=0.5) 6.6 Validation du modèle 6.6.1 Contrôle graphique a posteriori Une fois le modèle mis en oeuvre, on doit vérifier a posteriori le “bien-fondé statistique” de ce modèle du point de vue de la normalité des erreurs, l’adéquation de la valeur ajustée \\(\\widehat{Y_i}\\) à la valeur observée \\(Y_i\\) et l’absence de données aberrantes. Il est alors indispensable de commencer par s’entourer de “protections” graphiques pour vérifier empiriquement les 4 postulats de base (au moins les hypothèses H1-H3, puisque l’hypothèse H4 n’est pas vraiment important dès que l’on dispose de suffisamment de données). En régression linéaire simple, la confrontation graphique entre le nuage de points \\((z_i,y_i)\\) et la droite de régression de \\(Y\\) par \\(z\\) par moindres carrés ordinaires donne une information quasi exhaustive (cf Figure 6.2). Sur ce graphique, si nous voyons une courbure de la “vraie” courbe de régression de \\(Y\\), nous pouvons alors penser que le modèle est inadéquat et que l’hypothèse H1 n’est pas vérifiée. Dans le cas de la régression multiple, ce type de graphique n’est pas utilisable car il y a plusieurs régresseurs. Les différentes hypothèses sont donc à vérifier sur les termes des erreurs \\(\\varepsilon_i\\) qui sont malheureusement inobservables. Nous utilisons alors leurs prédicteurs naturels, les résidus \\(\\widehat{\\varepsilon_i}=Y_i-\\widehat{Y_i}\\). Le graphe des \\(n\\) points \\((y_i, \\widehat{y_i})\\) est également très informatif. Il suffit alors de vérifier si les points sont alignés selon la première bissectrice (cf. Figure 6.13). Figure 6.13: Graphique des réponses par rapport aux valeurs ajustées pour l’exemple en régression linéaire simple (à gauche) et multiple (à droite) Voici maintenant plusieurs démarches permettant de s’assurer de la légitimité des conclusions, démarches à effectuer pour toute régression linéaire multiple. 6.6.2 Pour vérifier les hypothèses H1 et H2 : adéquation et homoscédasticité Le graphique le plus classique consiste à représenter les résidus \\((\\widehat{\\varepsilon_i})_i\\) en fonction des valeurs prédites \\((\\widehat{Y_i})_i\\) (cf graphique en haut à gauche Figures 6.14 et 6.15). Ce graphique doit être fait pratiquement systématiquement. Cela revient encore à tracer les coordonnées du vecteur \\(P_{[X]^{\\perp}}Y\\) en fonction de celles de \\(P_{[X]}Y\\). L’intérêt d’un tel graphique réside dans le fait que si les 4 hypothèses H1-H4 sont bien respectées, il y a indépendance entre ces 2 vecteurs qui sont centrés et gaussiens (d’après le théorème de Cochran). Cependant, à partir de ce graphe, nous ne pourrons nous apercevoir que de la possible déficience des hypothèses H1 et H2. Concrètement, si on ne voit rien de notable sur le graphique, i.e. si l’on observe un nuage de points centrés et alignés quelconque, c’est très bon signe : les résidus ne semblent alors n’avoir aucune propriété intéressante et c’est bien ce que l’on demande à l’erreur. autoplot(reg.simple, label.size = 2) Figure 6.14: Graphiques pour l’étude des résidus pour l’exemple en régression linéaire simple autoplot(reg.multi, label.size = 2) Figure 6.15: Graphiques pour l’étude des résidus pour l’exemple en régression linéaire multiple Voyons maintenant 2 types de graphes résidus/valeurs prédites “pathologiques” (Figure 6.16) : Type 1 “forme banane” : Dans ce cas, on peut penser que le modèle n’est pas adapté aux données. En effet, il ne semble pas y avoir indépendance entre les \\(\\widehat{\\varepsilon_i}\\) et les \\(\\widehat{Y_i}\\), puisque, par exemple, les \\(\\widehat{\\varepsilon_i}\\) ont tendance à décroître lorsque les \\(\\widehat{Y_i}\\) sont dans un certain intervalle et croissent. Il faut donc améliorer l’analyse du problème pour proposer d’autres régresseurs pertinents ou transformer les régresseurs \\(z^{(j)}\\) par une fonction de type \\((log,sin)\\). Type 2 “forme trompette” Dans ce cas la variance des résidus semble inhomogène, puisque les \\(\\widehat{\\varepsilon_i}\\) ont une dispersion de plus en plus importante au fur et à mesure que les \\(\\widehat{Y_i}\\) croissent. Un changement de variable pour \\(Y\\) pourrait être une solution envisageable afin de “rendre” constante la variance du bruit (cf pararaphe suivant). Figure 6.16: Exemple du type forme banane (gauche) et forme trompette (droite) En cas de comportement inadéquat, les modifications possibles à apporter au modèle sont : On peut librement transformer les régresseurs \\(z^{(1)}, \\cdots, z^{(p)}\\) par toutes les transformations algébriques ou analytiques connues (fonctions puissances, exponentielles, logarithmiques…), pourvu que le nouveau modèle reste interprétable. Cela peut permettre d’améliorer l’adéquation du modèle ou diminuer son nombre de termes si on utilise ensuite une procédure de choix de modèles. En revanche, on ne peut envisager de transformer \\(Y\\) que si les graphiques font suspecter une hétéroscédasticité. Dans ce cas, cette transformation doit obéir à des règles précises basées sur la relation suspectée entre l’écart-type résiduel \\(\\sigma\\) et la réponse \\(Y\\) : c’est ce que précise le tableau en figure 6.17. Figure 6.17: Table de changements de variable pour la variable à expliquer afin de stabiliser la variance de Y 6.6.3 Pour vérifier l’hypothèse H3 : indépendance Un graphe pertinent pour s’assurer de l’indépendance des erreurs entre elles est celui des résidus \\(\\widehat{\\varepsilon_i}\\) en fonction de l’ordre des données (lorsque celui-ci a un sens, en particulier s’il représente le temps). Un tel graphique est potentiellement suspect si les résidus ont tendance à rester par paquets lorsqu’ils se trouvent d’un côté ou de l’autre de 0. On pourra confirmer ces doutes en effectuant un test de runs (cf Draper and Smith (1998), p. 157). Ce test est basé sur le nombre de runs, i.e. sur le nombre de paquets de résidus consécutifs de même signe. Par ailleurs, si les erreurs sont corrélées suivant certaines conditions (par exemple si ce sont des processus ARMA), il est tout d’abord possible d’obtenir encore des résultats quant à l’estimation des paramètres. Mais il existe également des méthodes de correction telles que les estimations par moindres carrés généralisés ou pseudo-généralisés, cf Guyon (2001) ou d’autres. 6.6.4 Pour vérifier l’hypothèse H4 : gaussianité %Nous l’avons déjà évoqué et nous le redirons : l’hypothèse H4 de gaussianité des données n’est importante que si l’on dispose de très peu de données (i.e. grossièrement, car tout dépend du modèle et du nombre de variables, moins de quelques dizaines). Dans ce cas, Notamment pour que les tests de Fisher et de Student aient un sens, il peut être intéressant de vérifier si l’hypothèse de gaussianité est acceptable. Pour cela, nous déconseillons fortement les tests d’adéquation classiques de Kolmogorov-Smirnov, Cramer-Von Mises,…, du fait qu’on les appliquera sur les résidus \\(\\widehat{\\varepsilon_i}\\), qui ne sont (quasiment) jamais indépendants. On préfèrera se “contenter” d’une vérification graphique à partir du tracé d’une droite de Henri, dite encore graphique QQ-plot (cf graphiques en haut à droite Figures 6.14 et 6.15. Celle-ci relie les points de \\(\\mathbb{R}^2\\) formés par les quantiles empiriques des résidus studentisés (i.e. le \\(\\widehat{\\varepsilon_i}\\) divisés par leur écart-type empirique) en fonction des quantiles théoriques (pour les probabilités \\(k/(n+1)\\) où \\(k=1,\\cdots,n\\), \\(n\\) étant le nombre de données) d’une loi normale centrée réduite. La loi de Student “ressemblant” fortement à une loi gaussienne dès que le paramètre dépasse la dizaine, si les erreurs \\((\\varepsilon_i)\\) sont gaussiennes, i.e. sous H4, alors la droite de Henri est une bissectrice du plan. Ce type de tracé permet surtout de voir si une loi à “queue de distribution lourde” ne pourrait pas être plus adéquate (dans ce cas, les points s’éloignent de la droite de Henri en ses extrémités). 6.6.5 Détection de données aberrantes Nous allons ici décrire deux méthodes permettant de détecter des données “aberrantes”. 6.6.5.1 Effet levier avec la matrice \\(H\\) On reprend la matrice chapeau \\(H=X(X&#39;X)^{-1}X&#39;\\). La prédiction pour le ième individu est donné par \\[ \\hat Y_i = (X \\hat\\theta)_i = (HY)_i = H_{ii} Y_i + \\sum_{j\\neq i} H_{ij} Y_{j}. \\] Si \\(H_{ii}=1\\), \\(\\hat Y_i\\) est entièrement déterminée par la ième observation alors que, si \\(H_{ii}=0\\), la ième observation n’a aucune influence sur \\(\\hat Y_i\\). Ainsi, pour mesurer l’influence d’une observation sur sa propre estimation, on peut examiner le diagramme en batons des termes diagonaux de \\(H\\) (cf Figure 6.18). En pratique, on déclare la ième observation comme levier si \\(H_{ii}\\) dépasse \\(2k/n\\) ou \\(3k/n\\). 6.6.5.2 Distances de Cook Les points influents sont les points tels que, si on les retire de l’étude, l’estimation des coefficients du modèle sera fortement modifiée. La mesure la plus classique d’influence est la distance de Cook. C’est une distance entre le coefficient estimé avec toutes les observations et celui estimé en enlevant une observation. La distance de Cook pour la ième observation est définie par \\[ DC_i = (\\widehat{\\theta}-\\widehat{\\theta}^{(-i)})&#39;T&#39;T(\\widehat{\\theta}-\\widehat{\\theta}^{(-i)}) \\] où \\(T\\) est le vecteur des résidus studentisés et \\(\\widehat{\\theta}^{(-i)}\\) l’EMV sans l’observation \\(i\\). On peut là encore tracer le diagramme en bâtons des \\(DC_i\\) (cf Figure 6.18). Si une distance se révèle grande par rapport aux autres alors ce point sera considéré comme influent. Il faut alors chercher à comprendre pourquoi il est influent : il est levier, aberrant, les deux, …. Figure 6.18: Diagramme en bâtons des termes diagonaux de la matrice chapeau H (à gauche) et des distances de Cook (à droite). 6.7 En résumé Savoir écrire un modèle de régression linéaire (pour chaque individu et matriciellement) Savoir déterminer les estimateurs de la régression linéaire et leur loi. Savoir construire en régression linéaire un intervalle de confiance pour un paramètre et un intervalle de prédiction Savoir construire un test de nullité d’un (ou des) paramètre(s) Comprendre la problématique de la sélection de variables, savoir proposer des stratégies en pratique et savoir interpréter mes sorties de R Comprendre le principe des méthodes de régression régularisées et savoir lire les sorties de R associées. Savoir interpréter les graphiques de contrôle pour valider les hypothèses du modèle linéaire. 6.8 Quelques codes python Dans cette partie, on donne quelques lignes de codes en python pour reproduire (partiellement) l’étude faite précédemment en R. import statsmodels.api as sm import numpy as np # on passe par reticulate pour récupérer le jeu de données ici fitnesspy=r.fitness # régression linéaire simple x = np.array(fitnesspy.runtime).reshape((-1, 1)) x = sm.add_constant(x) y = np.array(fitnesspy.oxy) regsimplepy = sm.OLS(y, x) resultsregsimple = regsimplepy.fit() print(resultsregsimple.summary()) OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.743 Model: OLS Adj. R-squared: 0.735 Method: Least Squares F-statistic: 84.01 Date: Jeu, 28 oct 2021 Prob (F-statistic): 4.59e-10 Time: 12:19:59 Log-Likelihood: -74.254 No. Observations: 31 AIC: 152.5 Df Residuals: 29 BIC: 155.4 Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P&gt;|t| [0.025 0.975] ------------------------------------------------------------------------------ const 82.4218 3.855 21.379 0.000 74.537 90.307 x1 -3.3106 0.361 -9.166 0.000 -4.049 -2.572 ============================================================================== Omnibus: 0.032 Durbin-Watson: 1.924 Prob(Omnibus): 0.984 Jarque-Bera (JB): 0.072 Skew: 0.028 Prob(JB): 0.964 Kurtosis: 2.770 Cond. No. 84.2 ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. # régression linéaire multiple list_var=fitnesspy.columns.drop(&quot;oxy&quot;) X=fitnesspy[list_var] X = sm.add_constant(X) y=np.array(fitnesspy.oxy) regmultipy = sm.OLS(y, X) resultsregmulti = regmultipy.fit() print(resultsregmulti.summary()) OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.849 Model: OLS Adj. R-squared: 0.811 Method: Least Squares F-statistic: 22.43 Date: Jeu, 28 oct 2021 Prob (F-statistic): 9.72e-09 Time: 12:20:00 Log-Likelihood: -66.068 No. Observations: 31 AIC: 146.1 Df Residuals: 24 BIC: 156.2 Df Model: 6 Covariance Type: nonrobust ============================================================================== coef std err t P&gt;|t| [0.025 0.975] ------------------------------------------------------------------------------ const 102.9345 12.403 8.299 0.000 77.335 128.534 age -0.2270 0.100 -2.273 0.032 -0.433 -0.021 weight -0.0742 0.055 -1.359 0.187 -0.187 0.038 runtime -2.6287 0.385 -6.835 0.000 -3.422 -1.835 rstpulse -0.0215 0.066 -0.326 0.747 -0.158 0.115 runpulse -0.3696 0.120 -3.084 0.005 -0.617 -0.122 maxpulse 0.3032 0.136 2.221 0.036 0.022 0.585 ============================================================================== Omnibus: 2.609 Durbin-Watson: 1.711 Prob(Omnibus): 0.271 Jarque-Bera (JB): 1.465 Skew: -0.069 Prob(JB): 0.481 Kurtosis: 4.056 Cond. No. 7.91e+03 ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. [2] The condition number is large, 7.91e+03. This might indicate that there are strong multicollinearity or other numerical problems. # Pour SSR, SSE, SST print(&#39;SSR:&#39;, resultsregsimple.ssr) SSR: 218.4814449878273 print(&#39;SSE:&#39;, resultsregsimple.ess) SSE: 632.9000998508823 print(&#39;SST:&#39;, resultsregsimple.centered_tss) SST: 851.3815448387096 # Coefficient de détermination R2 print(&#39;coefficient of determination:&#39;,round(np.float(resultsregsimple.rsquared),3)) coefficient of determination: 0.743 print(&#39;coefficient of determination:&#39;, round(np.float(resultsregmulti.rsquared),3)) coefficient of determination: 0.849 # Exemple de tests de Fisher de sous-modèle from statsmodels.formula.api import ols from statsmodels.stats.anova import anova_lm resregfin = ols(&#39;oxy~age + runtime+runpulse+maxpulse&#39;, data=fitnesspy).fit() anovaResults = anova_lm(resregfin,resultsregmulti) print(anovaResults) df_resid ssr df_diff ss_diff F Pr(&gt;F) 0 26.0 138.930018 0.0 NaN NaN NaN 1 24.0 128.837938 2.0 10.09208 0.939979 0.40455 # Intervalles de confiances resultsregsimple.conf_int(0.1) array([[75.87112183, 88.97242353], [-3.9242713 , -2.69683942]]) resultsregmulti.conf_int(0.05) 0 1 const 77.335413 128.533546 age -0.433028 -0.020919 weight -0.186852 0.038497 runtime -3.422350 -1.834955 rstpulse -0.157863 0.114796 runpulse -0.616992 -0.122263 maxpulse 0.021505 0.584929 # Regression ridge Xtildepy=r.tildeX ytildepy=r.tildeY from sklearn.linear_model import Ridge lambdas=np.arange(0.001,1.01,0.01) p=Xtildepy.shape[1] coefs=np.empty((0,p),float) for a in lambdas: ridge = Ridge(alpha=a, fit_intercept=False); ridge.fit(Xtildepy, ytildepy); coefs=np.append(coefs,ridge.coef_,axis=0); # Regression Lasso from sklearn.linear_model import lasso_path lambdas=np.arange(0.01,1.01,0.01) alphas_lasso, coefs_lasso, _ = lasso_path(Xtildepy, ytildepy, alphas=lambdas, fit_intercept=False) import matplotlib.pyplot as plt plt.figure(); neg_log_alphas_lasso = -np.log(alphas_lasso) for i in range(coefs_lasso.shape[1]): plt.plot(neg_log_alphas_lasso, coefs_lasso[0,i,:],&#39;-&#39;); plt.xlabel(&#39;-log(lambda)&#39;); plt.ylabel(&#39;weights&#39;); plt.axis(&#39;tight&#39;); plt.show() # regression Elastic-Net from sklearn.linear_model import enet_path lambdas=np.arange(0.01,1.01,0.01) alphas_enet, coefs_enet, _ = enet_path(Xtildepy, ytildepy, alphas=lambdas, l1_ratio=0.3, fit_intercept=False) References "],["ANOVA.html", "Chapitre 7 Analyse de variance (ANOVA) 7.1 Vocabulaire 7.2 Analyse de variance à un facteur 7.3 Analyse de variance à deux facteurs 7.4 En résumé 7.5 Quelques codes en python", " Chapitre 7 Analyse de variance (ANOVA) Les slides associés à l’ANOVA sont disponibles ici Le jeu de données utilisé dans ce chapitre en ANOVA à deux facteurs est disponible ici Ble.txt 7.1 Vocabulaire On se place ici dans le cas où l’on souhaite expliquer une variable quantitative à l’aide d’une ou plusieurs variables qualitatives explicatives, appelées facteurs. Les modalités d’une variable qualitative explicative sont appelées niveaux du facteur. Un plan d’expérience répertorie l’ensemble des combinaisons des différents facteurs considérés par l’expérimentateur. Nous donnons ici qu’un peu de vocabulaire sur les plans d’expérience pour la suite, nous n’aborderons pas la théorie de la planification expérimentale dans ce cours. Definition 7.1 Vocabulaire issu de la planification expérimentale : On appelle cellule d’un plan d’expérience une case du tableau, associée à une combinaison des facteurs contrôlés. Un plan est dit complet s’il a au moins une observation dans chaque cellule. Un plan est dit répété s’il a plus d’une observation par cellule. Un plan est dit équilibré si chaque cellule comporte le même nombre d’observations. Un plan équilibré et répété est dit equirépété. 7.2 Analyse de variance à un facteur 7.2.1 Exemple et notations On dispose d’une variable quantitative \\(Y\\) à expliquer et d’un seul facteur explicatif. On note \\(i\\) l’indice du niveau (ou de la “cellule”) pour le facteur explicatif, \\(I\\) le nombre de niveaux (\\(i=1, \\cdots, I\\)), \\(n_i\\) le nombre d’expériences dans le niveau \\(i\\), - \\(j=1,\\cdots,n_i\\) l’indice de l’expérience dans le niveau \\(i\\), \\(n=\\sum_{i=1}^I n_i\\) le nombre total d’expériences. Une expérience (ou encore un “individu”) est repérée par deux indices : le numéro de la cellule (\\(i\\)) et le numéro de l’observation dans la cellule (\\(j\\)). Ainsi on note \\[Y_{ij} = \\textrm{ la valeur théorique de la réponse quantitative pour l&#39;expérience } j \\textrm{ du niveau }i\\] Dans cette section, nous allons illustrer les notions abordées avec l’exemple suivant : On s’intéresse aux notes obtenues par des étudiants à un oral. On s’interroge sur un effet potentiel de l’examinateur sur la note obtenue. Examiner (i) A B C Mark \\(Y_{ij}\\) 10, 11, 11 8, 10, 11, 12 10, 13, 14, 14 12,13,15 14, 15, 16, 16 15, 16, 16 Number \\(n_i\\) 6 8 7 Average \\(Y_{i.}\\) 12 12.75 14 7.2.2 Modèle régulier On modélise une variable quantitative en fonction d’un facteur à \\(I\\) niveaux. \\(Y\\) est la variable à expliquer qui prend la valeur \\(Y_{ij}\\) pour l’individu \\(j\\) du niveau \\(i\\) du facteur. Le modèle s’écrit : \\[\\begin{equation} \\left\\{ \\begin{array}{l} Y_{ij}=m_i+\\varepsilon_{ij},\\ \\forall i=1, \\cdots I,\\ \\forall j=1,\\cdots, n_i \\\\ \\\\ \\varepsilon_{ij} \\textrm{ i.i.d } \\mathcal{N}(0,\\sigma^2) \\end{array} \\right. \\tag{7.1} \\end{equation}\\] Le modèle peut se réécrire sous la forme matricielle suivante : \\[Y= \\left(\\begin{array}{c}Y_{1,1}\\\\ \\vdots \\\\ Y_{1n_1}\\\\Y_{21}\\\\ \\vdots \\\\Y_{In_I} \\end{array}\\right) = \\left( \\begin{array}{c c c c c} 1_{n_1} &amp; 0_{n_1} &amp; 0_{n_1} &amp; \\cdots &amp; 0_{n_1} \\\\ 0_{n_2} &amp; 1_{n_2} &amp; 0_{n_2} &amp; \\cdots &amp; 0_{n_2} \\\\ 0_{n_3} &amp; 0_{n_3} &amp; 1_{n_3} &amp; \\cdots &amp; 0_{n_3} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots\\\\ 0_{n_I} &amp; 0_{n_I} &amp; 0_{n_I} &amp; \\cdots &amp; 1_{n_I} \\\\ \\end{array} \\right) \\left( \\begin{array}{c} m_1\\\\ m_2\\\\ \\vdots \\\\ m_I \\end{array} \\right) + \\varepsilon\\] avec \\(\\varepsilon \\sim \\mathcal{N}_n \\left(0_{n},\\sigma^2 I_n\\right).\\) Le modèle (7.1) est régulier (\\(rg(X)=I\\)). On peut donc facilement estimer les paramètres en utilisant \\(\\widehat{\\theta}=(X&#39;X)^{-1}X&#39;Y\\). Proposition 7.1 Estimateur - Cas régulier Dans la modélisation (7.1), les \\(m_i\\) sont estimés par \\[ \\widehat{m_i} = Y_{i.} : = \\frac{1}{n_i}\\sum_{j=1}^{n_i} Y_{ij} \\] On les appelle les effets principaux des facteurs. Les \\(\\widehat{m_i}\\) sont indépendants et de loi respective \\[ \\widehat{m_i}\\sim\\mathcal{N}\\left(m_i,\\frac{\\sigma^2}{n_i}\\right) \\] Avec le logiciel R, il suffit d’utiliser la commande anReg&lt;-lm(Marks~Exam -1). On peut vérifier la matrice de design \\(X\\) par la commande model.matrix(Marks~Exam -1). anReg&lt;-lm(Marks~Exam -1) summary(anReg) Call: lm(formula = Marks ~ Exam - 1) Residuals: Min 1Q Median 3Q Max -4.75 -1.00 0.00 2.00 3.25 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) ExamA 12.0000 0.9789 12.26 3.58e-10 *** ExamB 12.7500 0.8478 15.04 1.23e-11 *** ExamC 14.0000 0.9063 15.45 7.88e-12 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 2.398 on 18 degrees of freedom Multiple R-squared: 0.9716, Adjusted R-squared: 0.9668 F-statistic: 205 on 3 and 18 DF, p-value: 4.226e-14 7.2.3 Modèle singulier Pour des raisons d’interprétation, on peut s’intéresser à un changement de paramétrage. Il s’agit d’un changement de variables dans la fonction à minimiser dont les variables sont les paramètres du modèle. Soulignons que les nouvelles équations que nous allons définir ci-après correspondent toujours à celles d’un modèle à un facteur. Si on veut comparer les effets des niveaux du facteur, on peut prendre comme référence un effet moyen et examiner les écarts des effets des différents niveaux à cet effet moyen. Le modèle initial (7.1) peut s’écrire sous la forme : \\[\\begin{equation} Y_{ij}= \\mu + \\alpha_i + \\varepsilon_{ij} \\tag{7.2} \\end{equation}\\] où \\(\\mu\\) est l’effet moyen et \\(\\alpha_i=m_i-\\mu\\) l’effet différentiel du niveau \\(i\\). Mais ce modèle est alors surparamétré (cf Chapitre 5). Pour le rendre identifiable, il faut imposer une contrainte entre les paramètres. Généralement, on considère le modèle (7.2) sous la contrainte \\(\\sum_{i=1}^I n_i \\alpha_i=0\\) (on l’appellera par la suite la contrainte “naturelle”) car elle rend le modèle orthogonal. Attention sous R, la contrainte “par défaut” est \\(\\alpha_1=0\\). Proposition 7.2 Les paramètres \\(\\mu\\) et \\(\\alpha_i\\) de la modélisation (7.2) sont estimés, sous la contrainte “naturelle”, par : \\[ \\widehat{\\mu}=Y_{..} := \\frac{1}{n}\\sum_{i=1}^{I} \\sum_{j=1}^{n_i} Y_{ij} \\textrm{ et } \\widehat{\\alpha_i}=Y_{i.} -Y_{..} \\] sous la contrainte \\(\\alpha_1=0\\), par : \\[ \\widehat{\\mu}=Y_{1.} \\mbox{ et } \\widehat{\\alpha_i}=Y_{i.} -Y_{1.}, \\forall i&gt;1. \\] Exercise 7.1 Pour prouver la proposition 7.2, pour la modélisation (7.2) sous la contrainte d’orthogonalité, vous pouvez minimiser la fonction des moindres carrés \\[ h(\\mu,\\alpha_1,\\ldots,\\alpha_I) = \\sum_{i=1}^I \\sum_{j=1}^{n_i} (Y_{ij} - \\mu - \\alpha_i)^2 \\] sous la contrainte \\(\\sum_{i=1}^I n_i\\alpha_i=0\\). Pour la modélisation (7.2) sous la contrainte \\(\\alpha_1=0\\), vous pouvez faire le lien avec la modélisation régulière et utiliser la proposition 7.1. Ces résultats sont résumés dans notre exemple par les Figures 7.1 pour la contrainte “naturelle” (d’orthogonalité) et Figure 7.2 pour la contrainte par défaut sous R. Figure 7.1: Schéma d’estimation dans le cas singulier de l’anova à un facteur sous la contrainte d’orthogonalité. Figure 7.2: Schéma d’estimation dans le cas singulier de l’anova à un facteur sous la contrainte d’orthogonalité. Sous R, on ajuste le modèle sous la contrainte \\(\\alpha_1=0\\) toujours avec la commande lm() : anSing &lt;- lm(Notes~Exam,data=Data) summary(anSing) Call: lm(formula = Notes ~ Exam, data = Data) Residuals: Min 1Q Median 3Q Max -4.75 -1.00 0.00 2.00 3.25 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 12.0000 0.9789 12.258 3.58e-10 *** ExamB 0.7500 1.2950 0.579 0.570 ExamC 2.0000 1.3341 1.499 0.151 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 2.398 on 18 degrees of freedom Multiple R-squared: 0.115, Adjusted R-squared: 0.01669 F-statistic: 1.17 on 2 and 18 DF, p-value: 0.333 7.2.4 Prédictions, résidus et variance On retrouve que les valeurs ajustées et donc les résidus ne sont pas impactés par le choix de la modélisation (régulière ou singulière) et par la contrainte dans le cas singulier. Proposition 7.3 Pour la modélisation (7.1) et la modélisation (7.2), on obtient que les valeurs ajustées \\(\\widehat{Y_{ij}}\\) dans la cellule \\(i\\) sont constantes et sont égales à la moyenne \\(Y_{i.}\\) des observations dans la cellule \\(i\\) : \\[\\widehat{Y_{ij}}=Y_{i.},\\] les résidus : \\[\\widehat{\\varepsilon_{ij}} = Y_{ij}-\\widehat{Y_{ij}}.\\] l’estimateur de \\(\\sigma^2\\) est donné par : \\[\\widehat{\\sigma^2}=\\frac{1}{n-I}\\sum_{i=1}^I \\sum_{j=1}^{n_i} (Y_{ij}-Y_{i.})^2.\\] On a les propriétés suivantes analogues à celles de la régression linéaire. Proposition 7.4 On a les propriétés suivantes analogues à celles de la régression linéaire. La moyenne des résidus par cellule est nulle : \\(\\forall i=1,\\cdots, I, \\, \\frac{1}{n_i} \\sum_{j=1}^{n_i} \\widehat{\\varepsilon}_{ij}=0.\\) La moyenne générale des résidus est nulle : \\(\\frac{1}{n} \\sum_{i=1}^I \\sum_{j=1}^{n_i} \\widehat{\\varepsilon}_{ij}=0\\). La moyenne des valeurs ajustées est égale à la moyenne des valeurs observées : \\(\\frac{1}{n} \\sum_{i=1}^I \\sum_{j=1}^{n_i} \\widehat{Y_{ij}}= \\frac{1}{n} \\sum_{i=1}^I \\sum_{j=1}^{n_i} Y_{ij}\\). \\(cov(\\widehat{\\varepsilon},\\widehat{Y})=0\\). \\(var(Y)=var(\\widehat{Y})+var(\\widehat{\\varepsilon})\\). La dernière propriété nous amène à définir les quantités suivantes. Definition 7.2 La variance se décompose en \\(var(\\widehat{Y})\\) appelée variance inter-groupe définie par \\[var(\\widehat{Y}) = \\frac{1}{n}\\sum_{i=1}^I n_i\\left(Y_{i.}-Y_{..}\\right)^2,\\] variance des moyennes par cellule, pondérées par les poids des cellules \\(n_i/n\\). \\(var(\\widehat{\\varepsilon})\\) appelée variance intra-groupe, ou variance résiduelle, définie par : \\[var(\\widehat{\\varepsilon})=\\frac{1}{n}\\sum_{i=1}^I \\sum_{j=1}^{n_i} (Y_{ij}-Y_{i.})^2=\\frac{1}{n}\\sum_{i=1}^I n_i var_i(Y)\\] où \\(var_i(Y)\\) est la variance des valeurs observées dans le niveau \\(i\\) : \\(\\displaystyle var_i(Y)=\\frac{1}{n_i}\\sum_{j=1}^{n_i} (Y_{ij}-Y_{i.})^2\\). Par conséquent, \\(var(\\widehat{\\varepsilon})\\) est la moyenne des variances des observations dans les cellules. La relation \\(var(Y) = var(\\widehat{Y})+var(\\widehat{\\varepsilon})\\) s’écrit ici : \\[ \\textrm{ Variance totale = Variance inter + Variance intra.} \\] On définit également le coefficient \\(R^2\\) comme le rapport de la variance inter-groupe sur la variance totale : \\[R^2=\\frac{ var(\\widehat{Y})}{ var(Y)} = 1 - \\frac{var(\\widehat{\\varepsilon})}{var(Y)}.\\] On l’appelle rapport de corrélation empirique entre la variable quantitative \\(Y\\) et le facteur considéré. C’est une mesure de liaison entre une variable quantitative et une variable qualitative. On peut mentionner les deux cas particuliers suivants : \\(R^2=1 \\leftrightarrow \\widehat{\\varepsilon}=0_{n} \\leftrightarrow \\forall j=1, \\cdots, n_i, Y_{ij}=Y_{i.}\\) i.e. \\(Y\\) est constant dans chaque cellule. \\(R^2=0 \\leftrightarrow var(\\widehat{Y})=0 \\leftrightarrow \\forall i=1, \\cdots, I,Y_{i.}=Y_{..}\\), i.e. la moyenne de \\(Y\\) est la même dans chaque cellule. 7.2.5 Intervalle de confiance et test sur l’effet facteur 7.2.5.1 Intervalle de confiance pour les \\(m_i\\) Dans le cadre général du modèle gaussien, on a montré que les estimateurs des paramètres du modèle sont distribués selon une loi gaussienne. Cette propriété peut s’appliquer au modèle à un facteur pour lequel on a posé l’hypothèse de normalité et d’indépendance des erreurs. Pour construire un intervalle de confiance pour les \\(m_i\\), il suffit donc de construire un intervalle de confiance de Student en utilisant que \\[\\widehat{m_i}\\sim \\mathcal{N}(m_i,\\sigma^2/n_i) ,\\ \\frac{(n-I)\\widehat{\\sigma}^2}{\\sigma^2}\\sim \\chi^2(n-I) \\textrm{ et } \\widehat{m_i}\\perp \\!\\!\\! \\perp\\widehat{\\sigma}^2\\] On obtient donc \\[IC_{1-\\alpha}(m_i)=\\left[\\widehat{m_i}\\pm t_{n-I,1-\\alpha/2}\\sqrt{\\frac{\\widehat{\\sigma}^2}{n_i}}\\ \\right].\\] anReg&lt;-lm(Marks~Exam -1) confint(anReg) 2.5 % 97.5 % ExamA 9.943313 14.05669 ExamB 10.968857 14.53114 ExamC 12.095878 15.90412 7.2.5.2 Intervalle de confiance pour \\(\\mu\\) et \\(\\alpha_i\\) Dans le cas du modèle singulier (7.2) sous la contrainte \\(\\alpha_1\\) par défaut sous R, on obtient des intervalles de confiance pour les paramètres avec la commande confint(). anSing&lt;-lm(Marks~Exam) confint(anSing) 2.5 % 97.5 % (Intercept) 9.9433129 14.056687 ExamB -1.9707414 3.470741 ExamC -0.8027921 4.802792 Exercise 7.2 Construisez les intervalles de confiance donnés par les commandes ci-dessous. Pour cela, on estime \\(\\mu\\) par \\(Y_{1.}=\\hat m_1\\sim\\mathcal{N}(\\mu,\\frac{\\sigma^2}{n_1})\\) on estime \\(\\alpha_i\\) par \\(\\hat m_i - \\hat m_1 \\sim \\mathcal{N}(\\alpha_i,\\sigma^2(\\frac{1}{n_1}+\\frac{1}{n_i}))\\) car \\(\\hat m_i\\perp \\!\\!\\! \\perp\\hat m_1\\) on estime \\(\\sigma^2\\) par \\(\\widehat{\\sigma}^2\\) \\(\\frac{(n-I)\\widehat{\\sigma}^2}{\\sigma^2}\\sim \\chi^2(n-I) \\textrm{ et } \\widehat{\\alpha}_i\\perp \\!\\!\\! \\perp\\widehat{\\sigma}^2\\) 7.2.6 Test d’effet du facteur On peut étudier l’effet du facteur sur la variable \\(Y\\) en posant l’hypothèse d’égalité de tous les paramètres du modèle : \\[\\mathcal{H}_0 : m_1 = m_2 = \\cdots = m_I = m \\Longleftrightarrow \\forall i=1, \\cdots, I,\\ \\alpha_i=0\\] versus \\[\\mathcal{H}_1 : \\exists (i,i&#39;) \\mbox{ tel que } m_i \\neq m_{i&#39;}.\\] Sous \\(\\mathcal{H}_0\\), tous les paramètres \\(m_i\\) sont égaux et le modèle s’écrit : \\[Y_{ij}=m +\\varepsilon_{ij} \\mbox{ avec } \\widehat{m} = Y_{..} = \\frac{1}{n}\\sum_{i=1}^I\\sum_{j=1}^{n_i}Y_{ij}.\\] On teste l’hypothèse d’égalité des paramètres \\(m_i\\) du modèle à partir de la statistique de Fisher : \\[F=\\frac{\\displaystyle\\sum_{i=1}^In_i(Y_{i.}-Y_{..})^2 / (I-1)}{\\displaystyle\\sum_{i=1}^I\\sum_{j=1}^{n_i}(Y_{ij}-Y_{i.} )^2 / (n-I)}= \\frac{SSE/ (I-1)}{SSR / (n-I)} \\underset{\\mathcal{H}_0}{\\sim} \\mathcal{F}(I-1,n-I),\\] où \\(SSE\\) désigne la sommes des carrés inter-groupes et \\(SSR\\) est la somme des carrés intra-groupes. On rejette \\(\\mathcal{H}_0\\) si \\(F &gt; f_{1-\\alpha,I-1,n-I}\\). anmequal&lt;-lm(Marks~1) anova(anmequal,anReg) Analysis of Variance Table Model 1: Marks ~ 1 Model 2: Marks ~ Exam - 1 Res.Df RSS Df Sum of Sq F Pr(&gt;F) 1 20 116.95 2 18 103.50 2 13.452 1.1698 0.333 7.2.7 Tableau d’analyse de la variance à un facteur Toutes ces estimations peuvent être présentées sous la forme d’un tableau d’analyse de la variance à un facteur : Figure 7.3: Table résumé de l’ANOVA à un facteur 7.3 Analyse de variance à deux facteurs 7.3.1 Notations et exemple Soit \\(Y\\) la variable réponse quantitative que l’on veut expliquer ici par rapport à deux variables qualitatives, i.e deux facteurs. Le premier facteur (facteur ligne), dit “\\(A\\)”, admet \\(I\\) niveaux, le deuxième (facteur colonne), dit “\\(B\\)”, admet \\(J\\) niveaux. On considère le tableau croisé entre les \\(I\\) modalités du facteur \\(A\\) et les \\(J\\) modalités du facteur \\(B\\). On appelle cellule une case du tableau. Par la suite, on note : \\[ \\begin{array}{ll} i=1, \\cdots I &amp; \\mbox{ les indices des niveaux du facteur ligne } A\\\\ j= 1, \\cdots J &amp; \\mbox{ les indices des niveaux du facteur colonne } B \\\\ n_{ij } &amp; \\mbox{ le nombre d&#39;observations pour le niveau } i \\mbox{ du facteur } A \\mbox{ et pour le niveau }\\\\ &amp; j \\mbox{ du facteur } B \\mbox{ (nombre d&#39;observations dans la cellule } (i,j))\\\\ \\ell=1, \\cdots, n_{ij} &amp; \\mbox{ les indices des observations de la cellule } (i,j)\\\\ Y_{ij\\ell} &amp; \\mbox{ la } \\ell \\mbox{-ième observation dans la cellule } (i,j)\\\\ Y_{ij.} &amp; \\mbox{la moyenne des observations dans la cellule } (i,j) : Y_{ij.}=\\frac{1}{n_{ij}}\\sum_{\\ell=1}^{n_{ij}}Y_{ij\\ell}. \\end{array} \\] On utilisera également les notations suivantes : \\[ \\begin{array}{l} Y_{i..} = \\frac{1}{n_{i+}} \\sum_{j=1}^J \\sum_{\\ell=1}^{n_{ij}} Y_{ij\\ell} \\textrm{ avec } n_{i+} = \\sum_{j=1}^J n_{ij}\\\\ \\\\ Y_{.j.} = \\frac{1}{n_{+j}} \\sum_{i=1}^I \\sum_{\\ell=1}^{n_{ij}} Y_{ij\\ell} \\textrm{ avec } n_{+j} = \\sum_{i=1}^I n_{ij}\\\\ \\\\ Y_{...} = \\frac{1}{n} \\sum_{i=1}^I \\sum_{j=1}^J \\sum_{\\ell=1}^{n_{ij}} Y_{ij\\ell} \\textrm{ avec } n= \\sum_{i=1}^I n_{i+} =\\sum_{j=1}^J n_{+j} \\end{array} \\] Example 7.1 Cet exemple est issu du livre de Husson and Pagès (2013). Au cours d’une étude sur les facteurs influençant le rendement de blé, on a comparé trois variétés de blé (facteur B de modalités L, N et NF) et deux apports d’azote (facteur A apport normal, dose 1; apport intensif, dose 2). Trois répétitions pour chaque couple (variété, dose) ont été effectuées (\\(n_{ij}=3\\), plan équilibré) et le rendement (en q/ha) a été mesuré (\\(Y_{ij\\ell}\\)). On s’intéresse aux différences qui pourraient exister d’une variété à l’autre, et aux interactions éventuelles des variétés avec les apports azotés. Variety L (\\(j=1\\)) N (\\(j=2\\)) NF (\\(j=3\\)) Dose 1 (\\(i=1\\)) (\\(Y_{1,j,k}\\)) 70.35 63.59 79.83 62.56 58.89 55.65 69.45 64.84 66.12 \\(n_{1j}=3\\) \\(Y_{11.}=71.26\\) \\(Y_{12.}=59.03\\) \\(Y_{13.}=66.80\\) Dose 2 (\\(i=2\\)) (\\(Y_{2,j,k}\\)) 74.97 69.12 77.18 58.78 64.39 60.83 69.85 64.89 67.15 \\(n_{2j}=3\\) \\(Y_{21.}=73.76\\) \\(Y_{22.}=61.33\\) \\(Y_{23.}=67.30\\) summary(Ble) Dose Variety Yield 1:9 L :6 Min. :55.65 2:9 N :6 1st Qu.:62.82 NF:6 Median :65.50 Mean :66.58 3rd Qu.:69.75 Max. :79.83 7.3.2 Modélisation 7.3.2.1 ANOVA à deux facteurs croisés Le modèle général à deux facteurs croisés s’écrit sous la forme : \\[\\begin{equation} \\tag{7.3} Y_{ij\\ell}= m_{ij}+\\varepsilon_{ij\\ell} \\mbox{ avec } i=1, \\cdots, I, \\, j = 1, \\cdots, J, \\, \\ell=1, \\cdots, n_{ij} \\end{equation}\\] où \\(\\varepsilon_{ij\\ell}\\sim \\mathcal{N}(0,\\sigma^2)\\), \\(n\\) variables aléatoires indépendantes. Cette paramétrisation ne permet pas de distinguer les effets de chaque facteur et de leur interaction. On considère donc la paramétrisation centrée qui décompose \\(m_{ij}\\) par rapport à un effet moyen général et permet de mesurer des “effets séparés” des deux facteurs et les “effets conjoints”. Le modèle complet s’écrit sous la forme : \\[\\begin{equation} \\tag{7.4} Y_{ij\\ell}=\\mu +\\alpha_i+\\beta_j+\\gamma_{ij} + \\varepsilon_{ij\\ell},\\ i=1, \\cdots, I, \\, j = 1, \\cdots, J, \\, \\ell=1, \\cdots, n_{ij} \\end{equation}\\] Mais on se retrouve avec un modèle défini avec \\(1 + I + J + IJ\\) paramètres. On doit donc introduire \\(1 +I+J\\) contraintes pour estimer ces paramètres. On a vu dans le chapitre 5 qu’il est intéressant de considérer des contraintes dans le cadre d’un dispositif orthogonal. Dans le cas de l’analyse de la variance à deux facteurs croisés, le dispositif orthogonal est caractérisé par la propriété suivante. Proposition 7.5 Dans le modèle d’analyse de variance à deux facteurs croisés il existe des contraintes qui rendent la partition \\(\\mu, \\, \\alpha, \\, \\beta, \\, \\gamma\\) orthogonale si et seulement si \\[\\begin{equation} \\tag{7.5} n_{ij}=\\frac{n_{i +}n_{+ j}}{n}. \\end{equation}\\] Dans ce cas, les contraintes (dites de type I) sont \\[\\begin{equation} \\tag{7.6} \\sum_{i=1}^I n_{i+}\\alpha_i=0; \\, \\sum_{j=1}^J n_{+j}\\beta_j=0 ; \\, \\forall i, \\, \\sum_{j=1}^J n_{ij}\\gamma_{ij}=0; \\, \\forall j, \\, \\sum_{i=1}^I n_{ij}\\gamma_{ij}=0. \\end{equation}\\] La preuve de la proposition 7.5 est donnée en annexe B.2. En pratique, les contraintes utilisées sont souvent celles dites de type III : \\[\\sum_i \\alpha_i =0, \\, \\sum_j \\beta_j=0, \\, \\forall i, \\, \\sum_j \\gamma_{ij}=0 \\mbox{ et } \\forall j, \\, \\sum_j \\gamma_{ij} =0\\] Avec ce système de contraintes, il n’y a possibilité d’orthogonalité que si le modèle est équilibré, i.e. \\(n_{ij}=cte\\), d’après la proposition 7.5. Attention, les contraintes (7.6) ne sont pas les contraintes par défaut sous R (cf model.matrix(Yield~Dose * Variety )). Sous R, les contraintes sont \\(\\alpha_1=\\beta_1=\\gamma_{1j}=\\gamma_{i1}=0\\) mais on peut les modifier (cf section suivante). Dans le cas des contraintes d’orthogonalité, les \\(IJ\\) paramètres \\(m_{ij}\\) sont donc redéfinis en fonction de \\(\\mu\\) un paramètre de centrage général, \\(\\alpha_i\\), \\(I-1\\) paramètres qui caractérisent les effets principaux du facteur \\(A\\), \\(\\beta_j\\), \\(J-1\\) paramètres qui caractérisent les effets principaux du facteur \\(B\\), \\(\\gamma_{ij}\\), \\((I-1)(J-1)\\) paramètres qui prennent en compte les effets d’interaction. Dans la suite, on va se placer dans le cadre d’un dispositif orthogonal. 7.3.2.2 ANOVA à deux facteurs additifs Le modèle d’ANOVA à deux facteurs additif est un modèle où on suppose qu’il n’y a pas d’effet d’interaction entre les deux facteurs. Le modèle s’écrit donc sous la forme \\[\\begin{equation} \\tag{7.7} Y_{ij\\ell}=\\mu +\\alpha_i+\\beta_j + \\varepsilon_{ij\\ell}. \\end{equation}\\] Le modèle additif est un sous-modèle du modèle complet avec interaction. Déterminez sous quelle condition il existe des contraintes rendant le modèle additif ci-dessus orthogonal. 7.3.3 Estimation des paramètres Proposition 7.6 Dans le cadre de la paramétrisation générale \\(Y_{ij\\ell} = m_{ij} +\\varepsilon_{ij\\ell}\\), \\(m_{ij}\\) est estimé par \\[\\displaystyle \\widehat{m}_{ij}=\\frac{1}{n_{ij}}\\sum_{\\ell=1}^{n_{ij}}Y_{ij\\ell} =Y_{ij.} \\sim \\mathcal{N}\\left(m_{ij},\\frac{\\sigma^2}{n_{ij}}\\right)\\] Pour démontrer cette proposition, il suffit d’utiliser l’expression \\(\\widehat{\\theta}=(X&#39;X)^{-1}X&#39;Y\\) car on est ici dans le cas régulier. Les paramètres du modèle complet d’équation (7.4) sous les contraintes d’orthogonalité (7.6) sont estimés par \\[ \\left\\{ \\begin{array}{l} \\widehat \\mu = Y_{...}\\\\ \\widehat \\alpha_i = Y_{i..} - Y_{...}\\\\ \\widehat \\beta_j = Y_{.j.} - Y_{...}\\\\ \\widehat \\gamma_{ij} = Y_{ij.} - Y_{i..} - Y_{.j.}+Y_{...} \\end{array} \\right. \\] Pour démontrer cette proposition, il faut minimiser la fonction des moindres carrés sous les contraintes (7.6). Proposition 7.7 Les paramètres du modèle complet d’équation (7.4) sous les contraintes par défaut sous R (\\(\\alpha_1=\\beta_1=\\gamma_{1j}=\\gamma_{i1}=0,\\ \\forall i,\\ \\forall j\\)) sont estimés par \\[ \\left\\{ \\begin{array}{l} \\widehat \\mu = Y_{11.}\\\\ \\widehat \\alpha_i = Y_{i1.} - Y_{11.}\\\\ \\widehat \\beta_j = Y_{1j.} - Y_{11.}\\\\ \\widehat \\gamma_{ij} = Y_{ij.} - Y_{i1.} - Y_{1j.}+Y_{11.} \\end{array} \\right. \\] Pour notre exemple, les résultats obtenus avec R sont les suivants anov2 = lm(Yield~Dose*Variety,data=Ble) summary(anov2) Call: lm(formula = Yield ~ Dose * Variety, data = Ble) Residuals: Min 1Q Median 3Q Max -7.667 -2.296 -0.325 2.623 8.573 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 71.257 2.536 28.101 2.55e-12 *** Dose2 2.500 3.586 0.697 0.49899 VarietyN -12.223 3.586 -3.409 0.00519 ** VarietyNF -4.453 3.586 -1.242 0.23801 Dose2:VarietyN -0.200 5.071 -0.039 0.96919 Dose2:VarietyNF -2.007 5.071 -0.396 0.69928 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 4.392 on 12 degrees of freedom Multiple R-squared: 0.6725, Adjusted R-squared: 0.536 F-statistic: 4.928 on 5 and 12 DF, p-value: 0.01105 Il est possible de modifier les contraintes sous R. On donne ci-dessous un exemple aov2Cbis&lt;-lm(Yield~C(Dose,sum) + C(Variety,sum) + C(Dose,sum):C(Variety,sum),data=Ble) #lm(Rendement ~ Dose * Variete,data=Ble,contrasts=list(Dose=&quot;contr.sum&quot;,Variete=&quot;contr.sum&quot;)) summary(aov2Cbis) Call: lm(formula = Yield ~ C(Dose, sum) + C(Variety, sum) + C(Dose, sum):C(Variety, sum), data = Ble) Residuals: Min 1Q Median 3Q Max -7.667 -2.296 -0.325 2.623 8.573 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 66.5800 1.0352 64.316 &lt; 2e-16 *** C(Dose, sum)1 -0.8822 1.0352 -0.852 0.410775 C(Variety, sum)1 5.9267 1.4640 4.048 0.001615 ** C(Variety, sum)2 -6.3967 1.4640 -4.369 0.000913 *** C(Dose, sum)1:C(Variety, sum)1 -0.3678 1.4640 -0.251 0.805897 C(Dose, sum)1:C(Variety, sum)2 -0.2678 1.4640 -0.183 0.857923 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 4.392 on 12 degrees of freedom Multiple R-squared: 0.6725, Adjusted R-squared: 0.536 F-statistic: 4.928 on 5 and 12 DF, p-value: 0.01105 7.3.4 Prédiction, résidus et variance On retrouve une fois de plus que les valeurs ajustées et donc les résidus ne sont pas impactés par le choix de la modélisation et des contraintes d’identifiabilité. Proposition 7.8 Dans le cadre de l’ANOVA à deux facteurs avec interaction, on obtient que les valeurs ajustées valent \\[\\widehat{Y}_{ij\\ell}=\\widehat{m}_{ij} = Y_{ij.}= \\widehat{\\mu}+\\widehat{\\alpha}_i+\\widehat{\\beta}_j+\\widehat{\\gamma}_{ij}\\] - les résidus valent \\(\\widehat{\\varepsilon}_{ij\\ell}=Y_{ij\\ell}-Y_{ij.}\\) La variance est estimée par \\[ \\displaystyle \\widehat{\\sigma}^2 = \\frac{1}{n-IJ}\\sum_{ij\\ell}(\\widehat{\\varepsilon}_{ij\\ell})^2 = \\frac{1}{n-IJ}\\sum_{ij\\ell}(Y_{ij\\ell}-Y_{ij.})^2 \\] et \\[ \\frac{(n - IJ) \\widehat{\\sigma}^2}{\\sigma^2}\\sim \\chi^2(n - IJ) \\] 7.3.5 Décomposition de la variabilité Comme dans l’analyse de variance à un facteur, la variabilité totale de \\(Y\\) se décompose en une variabilité inter-cellule expliquée par le modèle (notée \\(SSE\\)) et une variabilité intra-cellule non expliquée par le modèle (notée \\(SSR\\)) : \\[ \\underbrace{\\sum_{i=1}^I \\sum_{j=1}^J\\sum_{\\ell=1}^{n_{ij}}(Y_{ij\\ell}-Y_{...})^2}_{\\mbox{SST}} = \\underbrace{\\sum_{i=1}^I \\sum_{j=1}^J n_{ij}(Y_{ij.}-Y_{...})^2}_{\\mbox{SSE}} + \\underbrace{\\sum_{i=1}^I \\sum_{j=1}^Jn_{ij}var_{ij}(Y)}_{\\mbox{SSR}} \\] avec \\(var_{ij}(Y)=\\frac{1}{n_{ij}}\\sum_{\\ell=1}^{n_{ij}}(Y_{ij\\ell}- Y_{ij.})^2.\\) Dans le cas du modèle à deux facteurs croisés, la variance inter-cellule \\(SSE\\) peut être décomposée en une variance expliquée par le premier facteur, une variance expliquée par le second facteur et une variance expliquée par les interactions entre les deux facteurs. Dans le cas d’un plan orthogonal à deux facteurs, on définit les quantités suivantes : \\(SSA\\), la somme des carrés corrigés de l’effet différentiel du facteur \\(A\\) : \\[ SSA=\\sum_{i=1}^I n_{i+} (Y_{i..}-Y_{...})^2 = \\sum_{i=1}^I n_{i+} (\\widehat{\\alpha}_i)^2 \\] \\(SSB\\), la somme des carrés corrigés de l’effet différentiel du facteur \\(B\\) : \\[ SSB= \\sum_{j=1}^J n_{+j} (Y_{.j.} - Y_{...})^2 = \\sum_{j=1}^J n_{+j} (\\widehat{\\beta}_j)^2 \\] \\(SSI\\), la somme des carrés corrigés de l’effet d’interaction entre les deux facteurs : \\[ SSI= \\sum_{i=1}^I\\sum_{j=1}^J n_{ij} (Y_{ij.} - Y_{i..} - Y_{.j.} + Y_{...})^2 = \\sum_{i=1}^I\\sum_{j=1}^J n_{ij} (\\widehat{\\gamma}_{ij})^2 \\] On peut montrer que : \\[ SSE = SSA + SSB + SSI. \\] 7.3.6 Le diagramme d’interactions Le diagramme d’interactions permet de visualiser graphiquement la présence ou l’absence d’interactions. Pour chaque \\(j\\) fixé, on représente dans un repère orthogonal les points \\((i,j)\\) de coordonnées \\((i, \\widehat{m}_{ij}=Y_{ij.})\\). Puis on trace les segments joignants les couples de points \\(((i-1,j), \\, (i,j))\\). On obtient ainsi pour chaque \\(j\\) fixé une ligne brisée. Proposition 7.9 Si l’hypothèse de non-interaction est vraie, alors les lignes brisées dans le diagramme d’interaction sont parallèles. Proof. La ligne brisée associée au niveau \\(j\\) joint les points \\((1,\\widehat m_{1j}),(2,\\hat m_{2j}),\\cdots,(I,\\widehat m_{Ij})\\). S’il n’y a pas d’interaction, alors ces points ont pour coordonnées \\((1,\\widehat \\alpha_1+ \\widehat \\beta_j),(2,\\widehat \\alpha_2+ \\widehat \\beta_j),\\cdots,(I,\\widehat \\alpha_I+ \\widehat \\beta_j)\\). Par conséquent, les lignes brisées associées aux niveaux \\(j\\) et \\(j&#39;\\) se correspondent par une translation verticale d’amplitude \\(\\widehat \\beta_j-\\widehat \\beta_{j&#39;}\\). On lit sur ce graphique l’effet principal des modalités \\(j\\) (le niveau moyen d’une ligne brisée), l’effet principal des modalités \\(i\\) (la moyenne des ordonnées des points à abscisse fixée). En ce qui concerne les interactions, on obtiendra rarement des lignes brisées strictement parallèles. Le problème sera alors de savoir si leur non-parallélisme traduit une interaction significative. Un test est donc nécessaire. Figure 7.4: Moyennes de la variable \\(Y\\) pour chaque niveau d’un facteur en fonction des niveaux de l’autre facteur : avec interaction à droite; sans interaction à gauche. La Figure 7.4 illustre les comportements des moyennes des cellules de modèles avec ou sans interaction (additif). Chaque ligne est appelée un profil, et la présence d’interactions se caractérise par le croisement de ces profils tandis que le parallélisme indique l’absence d’interactions. La question est évidemment de tester si des croisements observés sont jugés significatifs. Attention, un manque de parallélisme peut aussi être dû à la présence d’une relation non-linéaire entre la variable \\(Y\\) et l’un des facteurs. Dans notre exemple, on obtient le graphique d’interaction suivant attach(Ble) interaction.plot(Variety,Dose,Yield,col=c(2,4),pch=c(18,24),main=&quot;Interaction plot&quot;,type=&quot;b&quot;) 7.3.7 Tests d’hypothèses Trois hypothèses sont couramment considérées : L’hypothèse d’absence d’interactions entre les deux facteurs ou hypothèse d’additivité des 2 facteurs : \\[\\mathcal{H}_I: \\forall i=1, \\cdots, I, _, \\forall j=1, \\cdots, J, \\, \\gamma_{ij}=0.\\] Cette hypothèse impose \\((I-1)(J-1)\\) contraintes. L’hypothèse d’absence d’effet du facteur \\(A\\) : \\[\\mathcal{H}_A : \\forall i=1, \\cdots, I, \\, \\alpha_i =0.\\] Cette hypothèse impose \\((I-1)\\) contraintes. L’hypothèse d’absence d’effet du facteur \\(B\\) : \\[\\mathcal{H}_B : \\forall j=1, \\cdots, J, \\, \\beta_j=0.\\] Cette hypothèse impose \\((J-1)\\) contraintes. Une remarque très importante porte sur la démarche de ces tests d’hypothèses. S’il existe des interactions entre les deux facteurs, alors les deux facteurs qui constituent cette interaction doivent impérativement être introduits dans le modèle ; dans ce cas, il est donc inutile de tester l’effet de chacun des deux facteurs. En effet, la présence d’interactions entre les deux facteurs signifie qu’il y a un effet combiné des deux facteurs et donc un effet de chaque facteur. 7.3.7.1 Test de non-interaction entre les deux facteurs On commence par tester l’absence d’effet d’interaction \\(\\mathcal{H}_I: \\gamma_{ij}=0,\\ \\forall i=1, \\cdots, I,\\ \\forall j=1, \\cdots, J\\). On est donc ramené à un test de Fisher entre les modèles suivants : \\([M_1]\\) \\(Y_{ij\\ell} = \\mu + \\alpha_i + \\beta_j + \\gamma_{ij} + \\varepsilon_{ij\\ell}\\) (modèle avec interaction) \\([M_0]\\) \\(Y_{ij\\ell} = \\mu + \\alpha_i + \\beta_j + \\varepsilon_{ij\\ell}\\) (modèle additif) La statistique de test est alors donnée par \\[ F=\\frac{SSI / (I-1)(J-1)}{SSR/(n-IJ) } \\underset{\\mathcal{H}_0}{\\sim} \\mathcal{F}((I-1)(J-1),n-IJ). \\] anov2add = lm(Yield ~Variety + Dose, data=Ble) anova(anov2add,anov2) Analysis of Variance Table Model 1: Yield ~ Variety + Dose Model 2: Yield ~ Dose * Variety Res.Df RSS Df Sum of Sq F Pr(&gt;F) 1 14 235.14 2 12 231.47 2 3.6654 0.095 0.91 Dans notre exemple, on retient donc le modèle additif (pas d’effet d’inetraction entre les deux facteurs). 7.3.7.2 Test d’absence d’effet du facteur \\(A\\) Si on a conservé le modèle additif au test précédent, on peut tester l’absence du facteur \\(A\\) \\[\\mathcal{H}_A: \\alpha_i =0,\\ \\forall i=1, \\cdots, I\\] Le test de Fisher compare donc les modèles suivants - \\([M_1]\\) \\(Y_{ij\\ell} = \\mu + \\alpha_i + \\beta_j + \\varepsilon_{ij\\ell}\\) (modèle additif) \\([M_0]\\) \\(Y_{ij\\ell} = \\mu + \\beta_j + \\varepsilon_{ij\\ell}\\) (Anova à un facteur) La statistique de test est donnée par \\[F=\\frac{SSA/(I-1)}{SSRAB/(n-(I+J-1))} \\underset{\\mathcal{H}_0}{\\sim} \\mathcal{F}(I-1,n-(I+J-1)),\\] où \\(SSRAB\\) est le SSR du modèle additif. anovA = lm(Yield ~Variety,data=Ble) anova(anovA,anov2add) Analysis of Variance Table Model 1: Yield ~ Variety Model 2: Yield ~ Variety + Dose Res.Df RSS Df Sum of Sq F Pr(&gt;F) 1 15 249.15 2 14 235.14 1 14.01 0.8341 0.3765 Dans notre exemple, on peut conclure qu’il n’y a pas d’effet de la dose. 7.3.8 Test d’absence d’effet du facteur \\(B\\) Si on a conservé le modèle additif au test précédent, on peut tester l’absence du facteur \\[\\mathcal{H}_B: \\beta_j =0,\\ \\forall j=1, \\cdots, J\\] Le test de Fisher compare donc les modèles suivants \\([M_1]\\) \\(Y_{ij\\ell} = \\mu + \\alpha_i + \\beta_j + \\varepsilon_{ij\\ell}\\) (modèle additif) \\([M_0]\\) \\(Y_{ij\\ell} = \\mu + \\alpha_i +\\varepsilon_{ij\\ell}\\) (ANOVA à un facteur) La statistique de test est donnée par \\[F=\\frac{SSB/(J-1)}{SSRAB/(n-(I+J-1))} \\underset{\\mathcal{H}_0}{\\sim} \\mathcal{F}(J-1,n-(I+J-1)),\\] où \\(SSRAB\\) est le SSR du modèle additif. anovB = lm(Yield~Dose,data=Ble) anova(anovB,anov2add) Analysis of Variance Table Model 1: Yield ~ Dose Model 2: Yield ~ Variety + Dose Res.Df RSS Df Sum of Sq F Pr(&gt;F) 1 16 692.72 2 14 235.14 2 457.58 13.622 0.0005192 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 On retient dans notre exemple qu’il y a un effet de la variété sur le rendement. 7.3.9 Tableau d’analyse de variance à deux facteurs croisés dans le cas d’un plan orthogonal Dans le cas du modèle à deux facteurs croisés avec dispositif orthogonal, on rappelle que la variabilité totale peut être décomposée en \\[ SST = SSE + SSR = SSA + SSB + SSI + SSR. \\] On peut ainsi dresser le tableau d’analyse de variance d’un plan orthogonal à deux facteurs croisés : Figure 7.5: Table résumé de l’ANOVA à deux facteurs 7.4 En résumé Savoir écrire un modèle d’ANOVA à un et deux facteurs (individuellement et matriciellement), régulier et singulier Savoir distinguer un modèle régulier d’un modèle singulier Savoir estimer les paramètres du modèle d’ANOVA dans le cas régulier et dans le cas singulier (en s’adaptant à la / les contrainte(s) choisie(s)) Savoir construire un intervalle de confiance pour un paramètre du modèle d’ANOVA Savoir construire un test pour tester l’effet d’un facteur, l’effet d’interaction entre facteurs, … et savoir organiser ces différents tests Savoir interpréter un diagramme d’interaction Savoir manipuler SSA, SSB, SSI, SSE, SSR dans le cas d’un plan orthogonal. 7.5 Quelques codes en python Dans cette partie, on donne quelques lignes de codes en python pour reproduire (partiellement) l’étude faite précédemment en R. 7.5.1 Exemple d’ANOVA à un facteur # Récupération des données de R avec la librairie reticulate Datapy=r.Data import statsmodels.api as sm from statsmodels.formula.api import ols # Ajustement du modèle ANOVA 1 régulier anRegpy = ols(&#39;Marks ~ Exam-1&#39;, data=Datapy).fit(); anRegpy.summary() # Ajustement du modèle ANOVA 1 singulier &lt;class &#39;statsmodels.iolib.summary.Summary&#39;&gt; &quot;&quot;&quot; OLS Regression Results ============================================================================== Dep. Variable: Marks R-squared: 0.115 Model: OLS Adj. R-squared: 0.017 Method: Least Squares F-statistic: 1.170 Date: Jeu, 28 oct 2021 Prob (F-statistic): 0.333 Time: 12:20:14 Log-Likelihood: -46.546 No. Observations: 21 AIC: 99.09 Df Residuals: 18 BIC: 102.2 Df Model: 2 Covariance Type: nonrobust ============================================================================== coef std err t P&gt;|t| [0.025 0.975] ------------------------------------------------------------------------------ Exam[A] 12.0000 0.979 12.258 0.000 9.943 14.057 Exam[B] 12.7500 0.848 15.039 0.000 10.969 14.531 Exam[C] 14.0000 0.906 15.447 0.000 12.096 15.904 ============================================================================== Omnibus: 0.750 Durbin-Watson: 1.388 Prob(Omnibus): 0.687 Jarque-Bera (JB): 0.773 Skew: -0.356 Prob(JB): 0.679 Kurtosis: 2.386 Cond. No. 1.15 ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. &quot;&quot;&quot; anSingpy = ols(&#39;Marks ~ Exam&#39;, data=Datapy).fit() anSingpy.summary() &lt;class &#39;statsmodels.iolib.summary.Summary&#39;&gt; &quot;&quot;&quot; OLS Regression Results ============================================================================== Dep. Variable: Marks R-squared: 0.115 Model: OLS Adj. R-squared: 0.017 Method: Least Squares F-statistic: 1.170 Date: Jeu, 28 oct 2021 Prob (F-statistic): 0.333 Time: 12:20:14 Log-Likelihood: -46.546 No. Observations: 21 AIC: 99.09 Df Residuals: 18 BIC: 102.2 Df Model: 2 Covariance Type: nonrobust ============================================================================== coef std err t P&gt;|t| [0.025 0.975] ------------------------------------------------------------------------------ Intercept 12.0000 0.979 12.258 0.000 9.943 14.057 Exam[T.B] 0.7500 1.295 0.579 0.570 -1.971 3.471 Exam[T.C] 2.0000 1.334 1.499 0.151 -0.803 4.803 ============================================================================== Omnibus: 0.750 Durbin-Watson: 1.388 Prob(Omnibus): 0.687 Jarque-Bera (JB): 0.773 Skew: -0.356 Prob(JB): 0.679 Kurtosis: 2.386 Cond. No. 4.00 ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. &quot;&quot;&quot; # Intervalles de confiance pour les paramètres anRegpy.conf_int(alpha=0.05) 0 1 Exam[A] 9.943313 14.056687 Exam[B] 10.968857 14.531143 Exam[C] 12.095878 15.904122 anSingpy.conf_int(alpha=0.05) 0 1 Intercept 9.943313 14.056687 Exam[T.B] -1.970741 3.470741 Exam[T.C] -0.802792 4.802792 # Test d&#39;effet de l&#39;examinateur from statsmodels.stats.anova import anova_lm anmequalpy = ols(&#39;Marks ~1&#39;, data=Datapy).fit(); anova_lm(anmequalpy,anRegpy) df_resid ssr df_diff ss_diff F Pr(&gt;F) 0 20.0 116.952381 0.0 NaN NaN NaN 1 18.0 103.500000 2.0 13.452381 1.169772 0.332952 7.5.2 Exemple d’ANOVA à deux facteurs import statsmodels.api as sm from statsmodels.formula.api import ols # Récuperation des données Blepy=r.Ble Blepy[&#39;Dose&#39;]=Blepy[&#39;Dose&#39;].astype(str) Blepy[&#39;Variety&#39;]=Blepy[&#39;Variety&#39;].astype(str) # Ajustement du modèle avec interaction anov2Singpy = ols(&#39;Yield ~ Dose * Variety&#39;, data=Blepy).fit(); anov2Singpy.summary() &lt;class &#39;statsmodels.iolib.summary.Summary&#39;&gt; &quot;&quot;&quot; OLS Regression Results ============================================================================== Dep. Variable: Yield R-squared: 0.672 Model: OLS Adj. R-squared: 0.536 Method: Least Squares F-statistic: 4.928 Date: Jeu, 28 oct 2021 Prob (F-statistic): 0.0111 Time: 12:20:17 Log-Likelihood: -48.528 No. Observations: 18 AIC: 109.1 Df Residuals: 12 BIC: 114.4 Df Model: 5 Covariance Type: nonrobust =========================================================================================== coef std err t P&gt;|t| [0.025 0.975] ------------------------------------------------------------------------------------------- Intercept 71.2567 2.536 28.101 0.000 65.732 76.781 Dose[T.2] 2.5000 3.586 0.697 0.499 -5.313 10.313 Variety[T.N] -12.2233 3.586 -3.409 0.005 -20.037 -4.410 Variety[T.NF] -4.4533 3.586 -1.242 0.238 -12.267 3.360 Dose[T.2]:Variety[T.N] -0.2000 5.071 -0.039 0.969 -11.250 10.850 Dose[T.2]:Variety[T.NF] -2.0067 5.071 -0.396 0.699 -13.056 9.043 ============================================================================== Omnibus: 1.202 Durbin-Watson: 2.764 Prob(Omnibus): 0.548 Jarque-Bera (JB): 0.199 Skew: 0.182 Prob(JB): 0.905 Kurtosis: 3.365 Cond. No. 9.77 ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. &quot;&quot;&quot; /Users/maugis/opt/anaconda3/envs/r-reticulate/lib/python3.6/site-packages/scipy/stats/stats.py:1604: UserWarning: kurtosistest only valid for n&gt;=20 ... continuing anyway, n=18 &quot;anyway, n=%i&quot; % int(n)) from statsmodels.graphics.factorplots import interaction_plot from matplotlib import pyplot as plt # graphique d&#39;interaction interaction_plot(Blepy[&#39;Variety&#39;],Blepy[&#39;Dose&#39;],Blepy[&#39;Yield&#39;]); plt.show() from statsmodels.stats.anova import anova_lm # Test d&#39;absence d&#39;interaction anov2addpy = ols(&#39;Yield~Dose + Variety&#39;, data=Blepy).fit() anovaResults = anova_lm(anov2addpy,anov2Singpy) print(anovaResults) df_resid ssr df_diff ss_diff F Pr(&gt;F) 0 14.0 235.137778 0.0 NaN NaN NaN 1 12.0 231.472400 2.0 3.665378 0.09501 0.910041 # Test d&#39;absence de l&#39;effet Dose anovApy = ols(&#39;Yield~Variety&#39;, data=Blepy).fit() print(anova_lm(anovApy,anov2addpy)) df_resid ssr df_diff ss_diff F Pr(&gt;F) 0 15.0 249.147467 0.0 NaN NaN NaN 1 14.0 235.137778 1.0 14.009689 0.834131 0.376541 # Test d&#39;absence d&#39;effet Variété anovBpy = ols(&#39;Yield~Dose&#39;, data=Blepy).fit() print(anova_lm(anovBpy,anov2addpy)) df_resid ssr df_diff ss_diff F Pr(&gt;F) 0 16.0 692.719511 0.0 NaN NaN NaN 1 14.0 235.137778 2.0 457.581733 13.622108 0.000519 References "],["ANCOVA.html", "Chapitre 8 Analyse de covariance (ANCOVA) 8.1 Les données 8.2 Modélisation 8.3 Estimation des paramètres 8.4 Tests d’hypothèses 8.5 En résumé 8.6 Quelques codes en python", " Chapitre 8 Analyse de covariance (ANCOVA) Les slides associés à l’ANCOVA sont disponibles ici Le jeu de données utilisé dans ce chapitre est disponible ici Ble.txt 8.1 Les données Dans ce chapitre, nous allons présenter le modèle d’analyse de covariance (ANCOVA) seulement dans le cadre simple où on observe deux variables quantitatives \\(z\\) et \\(Y\\), et une variable qualitative \\(T\\) sur un échantillon de \\(n\\) individus : la variable quantitative \\(Y\\) est la variable réponse que l’on cherche à expliquer en fonction de la variable quantitative \\(z\\) (appelée ) et du facteur \\(T\\) à \\(I\\) niveaux. Les notions peuvent être généralisées pour plusieurs covariables et plusieurs facteurs. Chaque individu de l’échantillon est repéré par un double indice \\((i,j)\\), \\(i\\) représente le niveau du facteur \\(T\\) auquel appartient l’individu et \\(j\\) correspond à l’indice de l’individu dans le niveau \\(i\\). Pour chaque individu \\((i,j)\\), on dispose d’une valeur \\(z_{ij}\\) de la variable \\(z\\) et d’une valeur \\(Y_{ij}\\) de la variable \\(Y\\). Pour chaque niveau \\(i\\) de \\(T\\) (avec \\(i=1,\\cdots, I\\)), on observe \\(n_i\\) valeurs \\(z_{(i)} =(z_{i1}, \\cdots, z_{in_i})&#39;\\) de \\(z\\) et \\(n_i\\) valeurs \\(Y_{(i)} = (Y_{i1}, \\cdots, Y_{in_i})&#39;\\) de \\(Y\\). Au final \\(n=\\sum_{i=1}^I n_i\\) est le nombre d’observations disponibles. Dans tout ce chapitre, nous allons illustrer les notions abordées à l’aide de l’exemple suivant. Example 8.1 On cherche à savoir si des conditions de température et d’oxygénation influencent l’évolution du poids des huîtres. On dispose de \\(n = 20\\) sacs de \\(10\\) huitres. On place, pendant un mois, ces \\(20\\) sacs de façon aléatoire dans \\(I = 5\\) emplacements différents d’un canal de refroidissement d’une centrale électrique à raison de \\(n_i = 4\\) sacs par emplacement. Ces emplacements se différencient par leurs températures et oxygénations. Pour chaque sac, on a son poids avant l’expérience (variable Pds Init), son poids après l’expérience (variable Pds Final), l’emplacement (variable Traitement) codé de 1 à 5. print(oyster) InitWeight FinalWeight Treatment 1 27.2 32.6 1 2 32.0 36.6 1 3 33.0 37.7 1 4 26.8 31.0 1 5 28.6 33.8 2 6 26.8 31.7 2 7 26.5 30.7 2 8 26.8 30.4 2 9 28.6 35.2 3 10 22.4 29.1 3 11 23.2 28.9 3 12 24.4 30.2 3 13 29.3 35.0 4 14 21.8 27.0 4 15 30.3 36.4 4 16 24.3 30.5 4 17 20.4 24.6 5 18 19.6 23.4 5 19 25.1 30.3 5 20 18.1 21.8 5 Les données peuvent être représentées conjointement sur un même graphique permettant de visualiser la relation éventuelle entre \\(Y, \\, z\\) et \\(T\\). Il s’agit de tracer un nuage de points de coordonnées \\((z_{ij},Y_{ij})\\), où tous les points du niveau \\(i\\), \\(i=1, \\cdots, I\\), sont représentés par le même symbole (Figure 8.1). On peut également tracer un boxplot du poids initial et du poids final pour chaque emplacement (Figure 8.2). Figure 8.1: Graphique des poids finaux par rapport aux poids initiaux selon chaque emplacement. Figure 8.2: Evolution des poids initiaux et poids finaux pour chaque traitement 8.2 Modélisation 8.2.1 Modélisation régulière Dans le cadre d’une ANCOVA simple, le modèle régulier s’écrit sous la forme : \\[(MR) : Y_{ij} =a_i + b_i\\ z_{ij} +\\varepsilon_{ij}, \\forall i=1, \\cdots, I, \\, j=1, \\cdots, n_i\\] où \\(\\varepsilon_{ij}\\sim\\mathcal{N}(0,\\sigma^2)\\), \\(n\\) variables indépendantes. Cela revient à estimer une droite de régression linéaire de \\(Y\\) sur \\(z\\) pour chaque niveau \\(i\\) du facteur \\(T\\). Pour le niveau \\(i\\), on estime les paramètres \\(a_i\\), constantes à l’origine des droites de régression et \\(b_i\\), pentes des droites de régression. Matriciellement, le modèle s’écrit sous la forme \\[ \\underbrace{\\left(\\begin{array}{c} Y_{(1)} \\\\ \\vdots \\\\ \\vdots \\\\ Y_{(I)} \\end{array}\\right)}_{Y} = \\underbrace{\\left(\\begin{array}{c c c c} X_{(1)} &amp; &amp; &amp; \\\\ &amp; X_{(2)} &amp; &amp; \\\\ &amp; &amp; \\ddots &amp; \\\\ &amp; &amp; &amp; X_{(I)} \\end{array}\\right)}_{X} \\underbrace{\\left(\\begin{array}{c} a_1 \\\\ b_1 \\\\ \\vdots \\\\ a_I \\\\ b_I \\end{array}\\right)}_{\\theta} + \\underbrace{\\left(\\begin{array}{c} \\varepsilon_{(1)} \\\\ \\vdots \\\\ \\vdots \\\\ \\varepsilon_{(I)} \\end{array}\\right)}_{\\varepsilon} \\textrm{ avec } X_{(i)} = \\left(\\begin{array}{cc} 1&amp; z_{i1}\\\\ \\vdots&amp;\\vdots\\\\ 1&amp; z_{i,n_i} \\end{array}\\right) \\] 8.2.2 Modélisation singulière Comme pour les modèles factoriels, il existe une reparamétrisation faisant apparaître des effets différentiels par rapport à un niveau de référence. Le modèle associé à cette nouvelle paramétrisation s’écrit : \\[ (MS) : Y_{ij} = (\\mu + \\alpha_i) + (\\beta + \\gamma_i) z_{ij} + \\varepsilon_{ij},\\ \\ \\forall i=1, \\cdots, I, \\, j=1, \\cdots, n_i. \\] Cette paramétrisation permet de faire apparaître : un effet d’interaction entre la covariable \\(z\\) et le facteur \\(T\\) : \\(\\gamma_{i}\\) ; un effet différentiel du facteur \\(T\\) sur la variable \\(Y\\) : \\(\\alpha_i\\) ; un effet différentiel de la covariable \\(z\\) sur la variable \\(Y\\) : \\(\\beta\\). Exercise 8.1 Considérons le premier niveau comme référence dans cette paramétrisation (MS) pour rendre le modèle identifiable (ce qui est fait sous R par défaut). Les contraintes d’identifiabilité sont donc \\(\\alpha_1=\\gamma_1=0\\). Dans ce cas, donnez le lien entre les paramètres \\(\\mu\\), \\(\\alpha_i\\), \\(\\beta\\), \\(\\gamma_i\\) et les paramètres \\(a_i\\) et \\(b_i\\) de la modélisation (MR). Donnez une interprétation “graphique” des paramètres \\(\\mu\\), \\(\\alpha_i\\), \\(\\beta\\) et \\(\\gamma_i\\). 8.3 Estimation des paramètres Dans le cas du modèle régulier (MR), on peut utiliser la formule générale \\(\\hat \\theta = (X&#39;X)^{-1}X&#39;Y\\). En utilisant le fait que la matrice \\(X\\) est diagonale par bloc, \\(X=\\mbox{diag}(X_{(1)},\\ldots,X_{(I)})\\), on obtient que \\[ \\hat\\theta = \\left(\\begin{array}{c} (X_{(1)}&#39;X_{(1)})^{-1}X&#39;_{(1)} Y_{(1)}\\\\ \\vdots\\\\ X_{(I)}&#39;X_{(I)})^{-1}X&#39;_{(I)} Y_{(I)}\\end{array}\\right). \\] On en déduit des résultats en régression linéaire simple que \\[ \\left\\{\\begin{array}{l} \\displaystyle \\widehat{b}_i = \\frac{\\mbox{cov}(Y_{(i)},z_{(i)}) }{\\mbox{var}(z_{(i)})} \\\\ \\\\ \\widehat{a}_i = \\bar Y_{(i)} - \\bar z_{(i)} \\widehat{b}_i \\end{array}\\right. \\] Dans notre exemple, on obtient les estimations suivantes : a1 b1 a2 b2 a3 b3 a4 b4 a5 b5 1 5.241 0.983 -9.149 1.501 4.818 1.056 4.296 1.057 -0.432 1.224 et on peut graphiquement observer l’ajustement des droites de régression aux données (Figure 8.3). Figure 8.3: Ajustement des droites de régression aux données Dans le cas du modèle singulier (MS) avec les contraintes d’identifiabilité \\(\\alpha_1=\\gamma_1=0\\), on peut déduire les estimateurs en faisant le lien entre de modèle singulier et le modèle régulier (MR). On obtient donc \\[ \\left\\{ \\begin{array}{l} \\hat\\mu = \\hat a_1\\\\ \\hat \\alpha_i = \\hat a_i - \\hat a_1\\\\ \\hat \\beta = \\hat b_1\\\\ \\hat \\gamma_i = \\hat b_i - \\hat b_1 \\end{array} \\right. \\] Dans notre exemple, on obtient les résultats suivants sous R complet&lt;-lm(FinalWeight~InitWeight * Treatment,data=oyster) summary(complet) Call: lm(formula = FinalWeight ~ InitWeight * Treatment, data = oyster) Residuals: Min 1Q Median 3Q Max -0.68699 -0.28193 0.02184 0.10425 0.63075 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 5.24126 2.86473 1.830 0.0972 . InitWeight 0.98265 0.09588 10.249 1.27e-06 *** Treatment2 -14.39058 9.15971 -1.571 0.1472 Treatment3 -0.42330 3.97747 -0.106 0.9174 Treatment4 -0.94550 3.50725 -0.270 0.7930 Treatment5 -5.67309 3.57150 -1.588 0.1433 InitWeight:Treatment2 0.51871 0.33406 1.553 0.1515 InitWeight:Treatment3 0.07342 0.14699 0.499 0.6282 InitWeight:Treatment4 0.07428 0.12229 0.607 0.5571 InitWeight:Treatment5 0.24124 0.13980 1.726 0.1151 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.5324 on 10 degrees of freedom Multiple R-squared: 0.9921, Adjusted R-squared: 0.985 F-statistic: 139.5 on 9 and 10 DF, p-value: 2.572e-09 On peut ensuite s’intéresser à construire des intervalles de confiance pour ces paramètres, faire des tests de nullité pour chacun des paramètres, … 8.4 Tests d’hypothèses 8.4.1 Absence de tout effet On peut tout d’abord commencer par tester l’absence de tout effet, aussi bien de la covariable \\(z\\) que du facteur \\(T\\). Pour cela, on veut comparer le modèle “blanc” contre le modèle complet (MS) \\([M0] : Y_{ij} = \\mu + \\varepsilon_{ij},\\, \\forall i=1, \\cdots, I,\\, \\forall j=1, \\cdots, n_i\\) \\([MS]: \\ Y_{ij}=\\mu + \\alpha_i + \\beta z_{ij} + \\gamma_{i}z_{ij} +\\varepsilon_{ij},\\, \\forall i=1, \\cdots, I,\\, \\forall j=1, \\cdots, n_i.\\) Le modèle \\((M0)\\) consiste à ajuster une droite horizontale aux données (Figure 8.4). Dans ce cas, la statistique de test du test de Fisher est \\[ F=\\frac{SSE / (2I -1)}{SSR/n-2I} \\underset{\\mathcal{H}_0}{\\sim} \\mathcal{F}(2I-1,n-2I) \\] avec \\(SSR=\\|Y - \\widehat{Y}\\|^2\\) et \\(SSE=\\|\\widehat{Y} - \\bar{Y}\\mathbb{1}_n\\|^2\\). Dans notre exemple, on obtient M0&lt;-lm(FinalWeight~1,data=oyster) anova(M0,complet) Analysis of Variance Table Model 1: FinalWeight ~ 1 Model 2: FinalWeight ~ InitWeight * Treatment Res.Df RSS Df Sum of Sq F Pr(&gt;F) 1 19 358.67 2 10 2.83 9 355.84 139.51 2.572e-09 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Figure 8.4: Ajustement du modèle (M0) aux données. Si on rejette le modèle (M0), on peut poursuivre l’étude mais il est important de suivre une démarche logique dans la mise en place des tests d’hypothèses, comme dans le cadre de l’analyse de la variance. 8.4.2 Test d’absence d’interaction On commence par tester l’hypothèse de non-interaction entre le facteur \\(T\\) et la covariable \\(z\\). On souhaite donc tester l’hypothèse nulle suivante : \\[ \\mathcal{H}_0^{(MnonI)} : b_1= b_2 = \\cdots = b_I \\Longleftrightarrow \\gamma_{1} = \\gamma_{2} = \\cdots = \\gamma_{I} = 0. \\] Ce test revient à comparer le modèle complet et le sous-modèle sans interaction : \\([MS]: \\ Y_{ij}=\\mu + \\alpha_i + (\\beta+ \\gamma_{i}) z_{ij} +\\varepsilon_{ij},\\, \\forall i=1, \\cdots, I,\\, \\forall j=1, \\cdots, n_i\\) \\([MnonI]:\\ Y_{ij}=\\mu + \\alpha_i+ \\beta z_{ij} +\\varepsilon_{ij},\\, \\forall i=1, \\cdots, I,\\, \\forall j=1, \\cdots, n_i.\\) Le modèle \\([MnonI]\\) consiste graphiquement à ajuster des droites parallèles pour chaque traitement (Figure 8.5). Figure 8.5: Ajustement du modèle d’ANCOVA sans interaction (à gauche) et avec interaction (à droite). La statistique de test de Fisher vaut dans ce cas \\[ F = \\frac{SSR_{nonI}-SSR / (I-1)}{SSR/(n-2I)}\\underset{\\mathcal{H}_0}{\\sim}\\mathcal{F}(I-1,n-2I) \\] Si on rejette l’hypothèse \\(\\mathcal{H}_0^{[MnonI]}\\), on conclut à la présence d’interactions dans le modèle. Il est alors inutile de tester l’absence d’effet du facteur \\(T\\) ou de la covariable \\(z\\) sur \\(Y\\), car toute variable constituant une interaction doit apparaître dans le modèle. En revanche, si le test montre que l’hypothèse \\(\\mathcal{H}_0^{[MnonI]}\\) est vraisemblable (i.e. les \\(I\\) droites de régression partagent la même pente de régression), on peut alors évaluer l’effet de la covariable \\(z\\) sur \\(Y\\) et celui du facteur \\(T\\) sur \\(Y\\). Dans notre exemple, on retient le modèle sans interaction. Nous allons donc poursuivre notre étude. nonI&lt;-lm(FinalWeight~InitWeight+Treatment) summary(nonI) Call: lm(formula = FinalWeight ~ InitWeight + Treatment) Residuals: Min 1Q Median 3Q Max -0.8438 -0.3154 -0.2171 0.4863 0.8871 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 2.25040 1.44308 1.559 0.141205 InitWeight 1.08318 0.04762 22.746 1.87e-12 *** Treatment2 -0.03581 0.40723 -0.088 0.931169 Treatment3 1.89922 0.45802 4.147 0.000988 *** Treatment4 1.35157 0.41937 3.223 0.006135 ** Treatment5 0.24446 0.57658 0.424 0.678022 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.5492 on 14 degrees of freedom Multiple R-squared: 0.9882, Adjusted R-squared: 0.984 F-statistic: 235 on 5 and 14 DF, p-value: 5.493e-13 anova(nonI,complet) Analysis of Variance Table Model 1: FinalWeight ~ InitWeight + Treatment Model 2: FinalWeight ~ InitWeight * Treatment Res.Df RSS Df Sum of Sq F Pr(&gt;F) 1 14 4.2223 2 10 2.8340 4 1.3883 1.2247 0.3602 8.4.3 Test d’absence de l’effet de la covariable z On souhaite tester l’hypothèse d’absence d’effet de la covariable \\(z\\) sur \\(Y\\) : \\[\\mathcal{H}_0^{[MT]} : b_1= b_2= \\cdots = b_I= 0.\\] Seul le facteur \\(T\\) explique \\(Y\\). On met donc en place un modèle d’analyse de la variance à un facteur (Figure 8.6). On est donc ramené à comparer le modèle d’ANCOVA sans interaction avec le modèle d’anova à un facteur \\[[MT]: \\ Y_{ij}=\\mu + \\alpha_i +\\varepsilon_{ij},\\, \\forall i=1, \\cdots, I,\\, \\forall j=1, \\cdots, n_i.\\] Figure 8.6: Ajustement du modèle d’analyse de la variance à un facteur (à gauche) et le modèle d’ANCOVA sans interaction (à droite). La statistique de test de Fisher s’exprime alors comme suit : \\[ F=\\frac{SSR_T - SSR_{nonI} / 1}{SSR_{nonI}/(n-(I+1))}\\underset{\\mathcal{H}_0}{\\sim}\\mathcal{F}(1,n-(I+1)) \\] Dans notre exemple, on rejette l’absence d’effet du poids initial au risque \\(5\\%\\). MT&lt;-lm(FinalWeight~Treatment) anova(MT,nonI) Analysis of Variance Table Model 1: FinalWeight ~ Treatment Model 2: FinalWeight ~ InitWeight + Treatment Res.Df RSS Df Sum of Sq F Pr(&gt;F) 1 15 160.263 2 14 4.222 1 156.04 517.38 1.867e-12 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 8.4.4 Test d’absence de l’effet facteur T On veut tester l’hypothèse d’absence d’effet du facteur \\(T\\) sur \\(Y\\) \\[\\mathcal{H}_0^{[Mz]} : a_1=a_2=\\ldots=a_I \\Longleftrightarrow \\alpha_1= \\alpha_2 = \\ldots = \\alpha_I = 0.\\] Les \\(I\\) droites de régression partagent la même constante à l’origine, seule la covariable \\(z\\) explique \\(Y\\). On met alors en place un modèle de régression linéaire simple (Figure 8.7) On est donc ramené à comparer le modèle d’ANCOVA sans interaction avec un modèle de régression linéaire simple \\[[Mz]:\\ Y_{ij}=\\mu + \\beta\\ z_{ij} +\\varepsilon_{ij},\\, \\forall i=1, \\cdots, I,\\, \\forall j=1, \\cdots, n_i\\] Figure 8.7: Ajustement du modèle de régression linéaire simple (à gauche) et du modèle d’ANCOVA sans interaction (à droite). Dans ce cas, la statitsique de test de Fisher est définie par \\[ F=\\frac{SSR_z - SSR_{nonI} / (I-1)}{SSR_{nonI} / (n-(I+1))}\\underset{\\mathcal{H}_0}{\\sim}\\mathcal{F}(I-1,n-(I+1)) \\] Pour notre exemple, on rejette également l’absence d’effet du traitement. Mz&lt;-lm(FinalWeight~InitWeight) anova(Mz,nonI) Analysis of Variance Table Model 1: FinalWeight ~ InitWeight Model 2: FinalWeight ~ InitWeight + Treatment Res.Df RSS Df Sum of Sq F Pr(&gt;F) 1 18 16.3117 2 14 4.2223 4 12.089 10.021 0.0004819 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 8.5 En résumé Savoir écrire un modèle d’ANCOVA (individuellement et matriciellement), régulier et singulier Savoir distinguer un modèle régulier d’un modèle singulier Savoir estimer les paramètres du modèle d’ANCOVA dans le cas régulier et dans le cas singulier (en s’adaptant à la / les contrainte(s) choisie(s)) Savoir construire un intervalle de confiance pour un paramètre du modèle d’ANCOVA Savoir construire un test pour tester l’effet du facteur, l’effet d’interaction, … et savoir organiser ces différents tests Savoir associer une représentation graphique à un sous-modèle d’ANCOVA 8.6 Quelques codes en python import statsmodels.api as sm from statsmodels.formula.api import ols # Récupération des données oysterpy=r.oyster; # Ajustement du modèle d&#39;ANCOVA avec inetraction completpy = ols(&#39;FinalWeight ~ InitWeight * Treatment&#39;, data=oysterpy).fit(); completpy.summary() &lt;class &#39;statsmodels.iolib.summary.Summary&#39;&gt; &quot;&quot;&quot; OLS Regression Results ============================================================================== Dep. Variable: FinalWeight R-squared: 0.992 Model: OLS Adj. R-squared: 0.985 Method: Least Squares F-statistic: 139.5 Date: Jeu, 28 oct 2021 Prob (F-statistic): 2.57e-09 Time: 12:20:28 Log-Likelihood: -8.8384 No. Observations: 20 AIC: 37.68 Df Residuals: 10 BIC: 47.63 Df Model: 9 Covariance Type: nonrobust ============================================================================================= coef std err t P&gt;|t| [0.025 0.975] --------------------------------------------------------------------------------------------- Intercept 5.2413 2.865 1.830 0.097 -1.142 11.624 Treatment[T.2] -14.3906 9.160 -1.571 0.147 -34.800 6.019 Treatment[T.3] -0.4233 3.977 -0.106 0.917 -9.286 8.439 Treatment[T.4] -0.9455 3.507 -0.270 0.793 -8.760 6.869 Treatment[T.5] -5.6731 3.572 -1.588 0.143 -13.631 2.285 InitWeight 0.9826 0.096 10.249 0.000 0.769 1.196 InitWeight:Treatment[T.2] 0.5187 0.334 1.553 0.152 -0.226 1.263 InitWeight:Treatment[T.3] 0.0734 0.147 0.499 0.628 -0.254 0.401 InitWeight:Treatment[T.4] 0.0743 0.122 0.607 0.557 -0.198 0.347 InitWeight:Treatment[T.5] 0.2412 0.140 1.726 0.115 -0.070 0.553 ============================================================================== Omnibus: 0.296 Durbin-Watson: 1.831 Prob(Omnibus): 0.862 Jarque-Bera (JB): 0.466 Skew: 0.158 Prob(JB): 0.792 Kurtosis: 2.321 Cond. No. 2.22e+03 ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. [2] The condition number is large, 2.22e+03. This might indicate that there are strong multicollinearity or other numerical problems. &quot;&quot;&quot; # Test d&#39;absence de tout effet from statsmodels.stats.anova import anova_lm M0py = ols(&#39;FinalWeight~1&#39;, data=oysterpy).fit() anova_lm(M0py,completpy) df_resid ssr df_diff ss_diff F Pr(&gt;F) 0 19.0 358.669500 0.0 NaN NaN NaN 1 10.0 2.834009 9.0 355.835491 139.510053 2.572066e-09 # Test d&#39;absence d&#39;interaction nonIpy = ols(&#39;FinalWeight ~ InitWeight + Treatment&#39;, data=oysterpy).fit() from statsmodels.stats.anova import anova_lm anova_lm(nonIpy,completpy) df_resid ssr df_diff ss_diff F Pr(&gt;F) 0 14.0 4.222323 0.0 NaN NaN NaN 1 10.0 2.834009 4.0 1.388314 1.224691 0.360175 # Test d&#39;absence de l&#39;effet de la covariable MTpy = ols(&#39;FinalWeight ~ Treatment&#39;, data=oysterpy).fit() anova_lm(MTpy,nonIpy) df_resid ssr df_diff ss_diff F Pr(&gt;F) 0 15.0 160.262500 0.0 NaN NaN NaN 1 14.0 4.222323 1.0 156.040177 517.383995 1.867369e-12 # Test d&#39;absence de l&#39;effet du facteur Mzpy = ols(&#39;FinalWeight ~ InitWeight&#39;, data=oysterpy).fit() anova_lm(Mzpy,nonIpy) df_resid ssr df_diff ss_diff F Pr(&gt;F) 0 18.0 16.311683 0.0 NaN NaN NaN 1 14.0 4.222323 4.0 12.089359 10.021203 0.000482 "],["GLM.html", "Chapitre 9 Principe du modèle linéaire généralisé 9.1 Introduction 9.2 Caractérisation d’un modèle linéaire généralisé 9.3 Estimation 9.4 Loi asymptotique de l’EMV et inférence 9.5 Tests d’hypothèses 9.6 Intervalle de confiance pour \\(\\theta_j\\) 9.7 Qualité d’ajustement 9.8 Diagnostic, résidus 9.9 En résumé", " Chapitre 9 Principe du modèle linéaire généralisé Les slides associés à ce chapitre sont disponibles ici 9.1 Introduction Nous observons un vecteur \\(Y\\) de taille \\(n\\), réalisation d’une variable aléatoire de moyenne \\(\\mu\\) et dont les composants sont indépendants. Dans le cadre du modèle linéaire, on a \\(\\mu = X\\theta\\) où \\(X\\) est une matrice \\(n\\times k\\): le design. Le vecteur \\(\\theta\\) est inconnu et modélise l’influence des variables explicatives sur la réponse \\(Y\\). Le modèle linéaire tel que nous l’avons vu dans la partie précédente peut donc être caractérisé de la manière suivante : une composante aléatoire : \\(Y\\) est un vecteur aléatoire gaussien de moyenne \\(\\mu\\) (\\(Y\\sim\\mathcal{N}_n(\\mu,\\sigma^2 I_n)\\)), une composante “systématique” : les variables explicatives \\(\\textbf{x}^{(1)}, \\cdots, \\textbf{x}^{(p)}\\) définissent un prédicteur linéaire \\(\\eta=X\\theta\\) avec \\(\\theta\\in\\mathbb{R}^k\\) (\\(k\\) est lié à \\(p\\)) la relation liant \\(\\mu\\) et \\(\\eta\\) : \\(\\mu=\\eta\\) pour le modèle linéaire. Imposer une dépendance linéaire entre les variables explicatives et \\(\\mathbb{E}[Y]\\) permet une étude approfondie mais peut être parfois trop restrictive. Une généralisation possible du modèle linéaire consiste donc à supposer que la relation liant \\(\\mu\\) à \\(\\eta\\) n’est pas l’identité, mais plutôt un lien du type: \\[\\eta_i = g(\\mu_i), \\ \\mathrm{pour} \\ \\eta=(\\eta_1,\\dots,\\eta_n)&#39; \\ \\mathrm{et} \\ \\mu=(\\mu_1,\\dots, \\mu_n)&#39;.\\] La fonction \\(g\\) modélise donc le lien entre ces deux vecteurs. Cette formulation permet de modéliser un panel plus riche d’expériences. Example 9.1 Dans une expérience clinique, on cherche à comparer deux modes opératoires pour une opération chirurgicale donnée. L’expérience est menée sur deux hôpitaux différents. On dispose donc ici de deux facteurs à deux modalités: et hôpital. La variable réponse correspond pour chaque patient au succès ou à l’échec de l’intervention : il s’agit d’une variable binaire. Example 9.2 Un assureur s’intéresse au nombre de sinistres automobile déclarés pendant ces dix dernières années. Il souhaite étudier si ce nombre de sinistres est lié à l’âge du conducteur, la taille de la voiture, …. Le nombre de sinistres peut être modélisé par une loi de Poisson. Dans le cas particulier où la fonction de lien est de type canonique (i.e. \\(g(x)=x\\)), rien n’interdit d’utiliser la méthode des moindres carrés introduite dans la partie précédente. Cette dernière est en effet purement géométrique et peut donc tout à fait s’appliquer à des réponses de type “binaire”. Cependant, la partie inférentielle traitée dans ce cours nécessite quant à elle des hypothèses très fortes sur la distribution des observations. Pour des modèles alternatifs, il faut donc complètement repenser la construction des tests et des intervalles de confiance. Par ailleurs, une relation de type canonique est relativement restrictive (cf Figure 9.1). Il convient donc de se placer dans un cadre plus général afin de pouvoir faire face à des problèmes plus variés. Figure 9.1: Exemple d’observations pour un modèle de type binaire De manière plus générale, la méthode des moindres carrés introduite dans la partie précédente ne peut être implémentée. Bien souvent, le problème d’optimisation associé n’est en effet pas convexe. Une première “parade” consiste à utiliser l’estimateur du maximum de vraisemblance… mais dans la plupart des cas, ce dernier n’est pas calculable analytiquement. Il est cependant possible d’utiliser un algorithme itératif inspiré de la méthode de Newton-Raphson permettant d’approcher le maximum de vraisemblance. Sous certaines conditions, cet algorithme propose des résultats tout à fait satisfaisants. 9.2 Caractérisation d’un modèle linéaire généralisé L’objet de cette section est d’introduire le cadre théorique global permettant de regrouper tous les modèles (linéaire gaussien, logistique, log-linéaire) de ce cours qui cherchent à modéliser l’espérance d’une variable réponse \\(Y\\) en fonction d’une combinaison linéaire de variables explicatives. Le modèle linéaire généralisé développé initialement en 1972 par Nelder et Wedderburn et dont on trouvera des exposés détaillés dans McCullagh (2018), Agresti (2003) ou Antoniadis, Berruyer, and Carmona (1992), n’est ici qu’esquissé afin de définir les concepts communs à ces modèles : famille exponentielle, estimation par maximum de vraisemblance, tests, … Le modèle linéaire généralisé est caractérisé par trois quantités : La variable réponse \\(Y\\), composante aléatoire à laquelle est associée une loi de probabilité les variables explicatives \\(\\textbf{x}^{(1)},\\ldots,\\textbf{x}^{(p)}\\) (prédicteurs) le lien qui décrit la relation fonctionnelle entre la combinaison linéaire des \\(\\textbf{x}^{(1)},\\ldots,\\textbf{x}^{(p)}\\) et l’espérance de la variable réponse \\(Y\\). Nous allons par la suite détailler ces différentes quantités. 9.2.1 Loi de la variable réponse \\(Y\\) La composante aléatoire identifie la distribution de probabilité de la variable à expliquer \\(Y\\). On suppose que l’échantillon statistique est constitué de \\(n\\) variables aléatoires \\((Y_i)_{i=1,\\cdots, n}\\) indépendantes admettant des distributions issues d’une structure de famille exponentielle. Definition 9.1 Soit \\(Y\\) une variable aléatoire unidimensionnelle. On dit que la loi de \\(Y\\) appartient à une famille exponentielle si la loi de \\(Y\\) est dominée par une mesure dite de référence et si la vraisemblance de \\(Y\\) calculée en \\(y\\) par rapport à cette mesure s’écrit de la façon suivante : \\[\\begin{equation} f_Y(y,\\omega,\\phi) = \\exp \\left[ \\frac{y \\omega -b(\\omega)}{\\gamma(\\phi)}+c(y,\\phi) \\right]. \\tag{9.1} \\end{equation}\\] Le paramètre \\(\\omega\\) est appelé le paramètre naturel de la famille exponentielle. Cette formulation inclut la plupart des lois usuelles comportant un ou deux paramètres : gaussienne, gaussienne inverse, gamma, Poisson, binomiale (cf Table ??). Attention, la mesure de référence change d’une structure exponentielle à l’autre : la mesure de Lebesgue pour une loi continue, une mesure discrète combinaison de Dirac pour une loi discrète. Consulter Antoniadis, Berruyer, and Carmona (1992) pour une présentation générale de la famille exponentielle et des propriétés asymptotiques des estimateurs de leurs paramètres. Proposition 9.1 Soit \\(Y\\) une variable aléatoire dont la loi de probabilité appartient à la famille exponentielle alors \\[ \\mathbb{E}[Y]=b&#39;(\\omega) \\textrm{ et } \\mbox{Var}(Y)=b&#39;&#39;(\\omega)\\gamma(\\phi). \\] Exercise 9.1 Pour démontrer cette proposition, pour évaluer \\(\\mathbb{E}[Y]\\) : calculer \\(\\frac{\\partial }{\\partial \\omega} f_{Y}(y,\\omega,\\phi)\\) et intégrer par rapport à \\(y\\) pour évaluer \\(\\mbox{Var}(Y)\\) : calculer \\(\\frac{\\partial^2 }{\\partial \\omega^2} f_{Y}(y,\\omega,\\phi)\\) et intégrer par rapport à \\(y\\) Pour certaines lois, la fonction \\(\\gamma\\) est de la forme : \\(\\gamma(\\phi)=\\phi\\). Dans ce cas, \\(\\phi\\) est appelé paramètre de dispersion, c’est un paramètre de nuisance intervenant par exemple lorsque les variances des lois gaussiennes sont inconnues, mais égal à 1 pour les lois à un paramètre (Poisson, Bernoulli). L’expression de la structure exponentielle se met alors sous la forme canonique : \\[\\begin{equation} \\tag{9.2} f(y,\\omega)=a(\\omega)d(y)\\exp[yQ(\\omega)] \\end{equation}\\] avec \\(Q(\\omega)= \\frac{\\omega}{\\phi}\\), \\(a(\\omega) = \\exp\\left(-\\frac{b(\\omega)}{\\phi}\\right)\\) et \\(d(y)= \\exp[c(y,\\phi)]\\). Example 9.3 Exemples dans la famille exponentielle : Loi gaussienne : La densité de la loi \\(\\mathcal{N}(\\mu,\\sigma^2)\\) s’écrit : \\[\\begin{eqnarray*} f(y, \\mu,\\sigma^2)&amp;=&amp;\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left\\{-\\frac{(y-\\mu)^2}{2\\sigma^2}\\right\\}\\\\ &amp;=&amp; \\exp\\left[\\frac{y \\mu - \\mu^2 /2}{\\sigma^2} - \\left(\\frac{1}{2}\\frac{y^2}{\\sigma^2} + \\frac{1}{2}\\ln(2\\pi\\sigma^2) \\right)\\right] \\\\ &amp;=&amp;\\exp\\left\\{-\\frac{1}{2}\\frac{\\mu^2}{\\sigma^2}\\right\\}\\exp\\left\\{-\\frac{1}{2}\\frac{y^2}{\\sigma^2}-\\frac{1}{2}\\ln(2\\pi\\sigma^2) \\right\\}\\exp\\left\\{y\\frac{\\mu}{\\sigma^2} \\right\\}. \\end{eqnarray*}\\] La famille gaussienne est une famille exponentielle de paramètre de dispersion \\(\\phi=\\sigma^2\\) et de paramètre naturel \\(\\omega=\\mathbb{E}[Y]=\\mu\\). Loi de Bernoulli : Soit \\(Y\\) une variable aléatoire de loi de Bernoulli \\(\\mathcal B(\\pi)\\), alors \\[ f(y, \\pi)=\\pi^{y}(1-\\pi)^{1-y}=(1-\\pi)\\exp\\left\\{y\\ln\\frac{\\pi}{1-\\pi}\\right\\}, \\] qui est la forme d’une structure exponentielle de paramètre naturel \\(\\omega=\\ln(\\frac{\\pi}{1-\\pi}).\\) La loi binomiale conduit à des résultats identiques en considérant la somme de \\(n\\) (connu) variables de Bernoulli. Loi de Poisson : On considère une variable \\(Y\\) de loi de Poisson de paramètre \\(\\lambda\\). Alors \\[ f(y, \\lambda)=\\frac{\\lambda^{y}e^{-\\lambda}}{y!}=\\exp\\left\\{-\\lambda\\right\\}\\frac{1}{y!}\\exp\\left\\{y \\ln\\lambda\\right\\} = \\exp\\left [y \\ln(\\lambda)-\\lambda - \\ln(y!)\\right] \\] qui est issue d’une structure exponentielle et, mise sous la forme canonique, de paramètre naturel \\(\\omega=\\ln(\\lambda)\\). Distribution \\(\\omega\\) \\(b(\\omega)\\) \\(\\gamma(\\phi)\\) \\(\\mathbb{E}[Y]=b&#39;(\\omega)\\) \\(\\mbox{Var}(Y)=b^{&#39;&#39;}(\\omega)\\gamma(\\phi)\\) Gaussienne \\(\\mathcal N(\\mu,\\sigma^2)\\) \\(\\mu\\) \\(\\frac{\\omega^2}{2}\\) \\(\\phi = \\sigma^2\\) \\(\\mu=\\omega\\) \\(\\sigma^2\\) Bernoulli \\(\\mathcal B(p)\\) \\(\\ln(p/1-p)\\) \\(\\ln(1+e^\\omega)\\) \\(1\\) \\(p=\\frac{e^\\omega}{1+e^\\omega}\\) \\(p(1-p)\\) Poisson \\(\\mathcal P(\\lambda)\\) \\(\\ln(\\lambda)\\) \\(\\lambda=e^{\\omega}\\) \\(1\\) \\(\\lambda=e^{\\omega}\\) \\(\\lambda=e^{\\omega}\\) Gamma \\(\\mathcal G(\\mu,\\nu)\\) \\(-\\frac{1}{\\mu}\\) \\(-\\ln(-\\omega)\\) \\(\\frac{1}{\\nu}\\) \\(\\mu=-\\frac{1}{\\omega}\\) \\(\\frac{\\mu^2}{\\nu}\\) Inverse Gaussienne \\(IG(\\mu,\\sigma^2)\\) \\(-\\frac{1}{2\\mu^2}\\) \\(-\\sqrt{-2\\omega}\\) \\(\\sigma^2\\) \\(\\mu=(\\sqrt{-2\\omega})^{-1}\\) \\(\\mu^3 \\sigma^2\\) 9.2.2 Prédicteur linéaire Les observations planifiées des variables explicatives sont organisées dans la matrice \\(\\mathbf{X}\\) de planification d’expérience (design matrix). Soit \\(\\theta\\) un vecteur de \\(k\\) (\\(=p+1\\)) paramètres. Le prédicteur linéaire, composante déterministe du modèle est le vecteur à \\(n\\) composantes défini par \\[\\eta = \\mathbf{X}\\theta.\\] 9.2.3 Fonction de lien Cette troisième quantité exprime une relation fonctionnelle entre la composante aléatoire et le prédicteur linéaire. Soit \\(\\mu_i=\\mathbb{E}[Y_i] \\, ; \\, i=1,\\cdots,n\\). On pose \\[\\forall i=1,\\cdots,n, \\, \\eta_i=g(\\mu_i)\\] où \\(g\\), appelée fonction de lien, est supposée monotone et différentiable. Ceci revient donc à écrire un modèle dans lequel une fonction de la moyenne appartient au sous-espace vectoriel engendré par les variables explicatives : \\[\\forall i=1,\\cdots, n, \\, g(\\mu_i)=\\mathbf{x}_i\\theta.\\] La fonction de lien qui associe la moyenne \\(\\mu_i\\) au paramètre naturel \\(\\omega_i\\) est appelée fonction de lien canonique. Dans ce cas, \\[\\forall i=1,\\cdots, n, \\, g(\\mu_i)=\\omega_i=\\mathbf{x}_i\\theta.\\] Example 9.4 La fonction de lien canonique pour la loi gaussienne est l’identité : \\(\\omega_i = \\mu_i\\) la loi de Poisson est le logarithme \\(\\omega_i = \\ln(\\mu_i)\\) la loi de Bernoulli est la fonction logit \\(\\omega_i = \\ln\\left(\\frac{\\mu_i}{1-\\mu_i}\\right)\\) Dans le cas d’une variable réponse binaire \\(Y\\), on peut aussi considérer la fonction de lien probit : \\[ \\eta_i = g(\\pi_i) = \\Phi^{-1}(\\pi_i) \\] où \\(\\Phi(.)\\) est la fonction de répartition de la loi normale \\(\\mathcal N(0,1)\\). Dans le cadre de l’étude d’une variable réponse \\(Y\\) suivant une loi binomiale et considérant la fonction de lien logit, le modèle linéaire généralisé est appelé régression logistique. Dans le cadre de l’étude d’une variable réponse \\(Y\\) suivant une loi de Poisson et considérant la fonction de lien logarithme, le modèle linéaire généralisé est appelé modèle log-linéaire. 9.3 Estimation Le modèle étant posé, on souhaite maintenant estimer le vecteur des paramètres \\(\\theta=(\\theta_1,\\ldots,\\theta_k)&#39;\\) et le paramètre de dispersion \\(\\phi\\). Comme ce dernier paramètre n’apparait pas dans l’espérance, ce n’est pas le paramètre d’intérêt. Pour simplifier, on va supposer par la suite que \\(\\phi\\) est fixé (ou estimé préalablement), seul \\(\\theta\\) reste à estimer. 9.3.1 Estimation par maximum de vraisemblance La méthode des moindres carrés n’est pas applicable dans un grand nombre de situations pour le modèle linéaire généralisé (excepté pour des fonctions de lien canonique, i.e. identité). Pour ce problème d’estimation, on utilise donc la méthode d’estimation du maximum de vraisemblance (EMV). On va donc maximiser la log-vraisemblance du modèle linéaire généralisé. Par indépendance des observations, la vraisemblance du n-échantillon \\(\\underline{Y}=(Y_1,\\ldots,Y_n)\\) s’écrit \\(\\theta\\mapsto L(\\underline{Y};\\theta)\\) telle que \\[\\theta \\mapsto L(\\underline{y}; \\theta) = \\prod_{i=1}^n f_{Y_i}(y_i; \\omega_i)\\] et la log-vraisemblance vaut \\(\\theta\\mapsto l(Y;\\theta)\\) avec \\[\\theta\\mapsto l(\\underline{y}; \\theta) = \\sum_{i=1}^n \\ln[f_{Y_i}(y_i; \\omega_i)],\\] où \\(\\theta\\), \\(\\eta\\), \\(\\mu\\) et \\(\\omega\\) sont liés par le modèle. L’EMV associé vérifie donc \\[\\widehat{\\theta}_{\\scriptsize MV} \\in \\mathrm{arg} \\max_{\\theta} L(\\underline{Y}; \\theta) = \\mathrm{arg} \\max_{\\theta} l(\\underline{Y}; \\theta).\\] En particulier, si la fonction de lien \\(g\\) est celle du lien canonique, on a \\(\\omega_i=\\textbf{x}_i \\theta\\) et donc \\[l(\\underline{y}; \\theta) = \\sum_{i=1}^n \\frac{y_i \\textbf{x}_i \\theta -b(\\textbf{x}_i \\theta)}{\\gamma(\\phi)}+c(y_i,\\phi).\\] Afin d’obtenir une expression de l’EMV, on s’intéresse au score \\[S(\\underline{Y};\\theta) = \\left( \\frac{\\partial}{\\partial \\theta_1} l(\\underline{Y};\\theta), \\dots , \\frac{\\partial}{\\partial \\theta_k} l(\\underline{Y}; \\theta)\\right)&#39;.\\] L’estimateur du maximum de vraisemblance vérifie donc \\[\\begin{equation} S(\\underline{Y};\\widehat{\\theta}_{\\scriptsize MV})=0_k. \\tag{9.3} \\end{equation}\\] Dans le cas particulier où \\(g\\) est le lien canonique, on a \\[ \\forall j=1,\\ldots,k,\\ \\frac{\\partial}{\\partial \\theta_j} l(\\underline{y};\\theta) = \\sum_{i=1}^n \\frac{1}{\\gamma(\\phi)} x_i^{(j)} [y_i - b&#39;(\\textbf{x}_i \\theta)] = 0 \\Leftrightarrow \\sum_{i=1}^n [y_i - b&#39;(\\textbf{x}_i \\theta)] \\frac{\\textbf{x}_i}{\\gamma(\\phi)} = 0_k \\] On peut constater que ce système n’est linéaire que si \\(b&#39;(a)=a\\), c’est-à-dire si on est dans le cas du modèle linéaire. Pour tous les autres modèles linéaires généralisés, (9.3) est un système non linéaire en \\(\\theta\\) et il n’existe pas de formule analytique pour cet estimateur. Il est cependant possible de montrer que le problème associé à la détermination de \\(\\hat\\theta_{\\scriptsize MV}\\) est un problème d’optimisation convexe qui peut donc être traité par un algorithme de type Newton-Raphson, adapté à un cadre statistique, cf l’Annexe des rappels A.3 pour les détails de cet algorithme. 9.3.2 Algorithmes de Newton-Raphson et Fisher-scoring L’algorithme de Newton-Raphson est un algorithme itératif basé sur le développement de Taylor à l’ordre 1 du score. Il fait donc intervenir la matrice Hessienne de la log-vraisemblance \\[\\mathcal{J}_{j\\ell} = \\frac{\\partial^2 l(\\underline{y};\\theta)}{\\partial\\theta_j \\partial\\theta_\\ell}.\\] Il faut que \\(\\mathcal{J}\\) soit inversible et comme elle dépend de \\(\\theta\\), il convient de mettre à jour cette matrice à chaque étape de cet algorithme itératif. Cet algorithme est implémenté dans la plupart des logiciels statistiques. Algorithme de Newton-Raphson Initialisation: \\(u^{(0)}\\). Pour tout entier \\(h\\) \\[\\begin{equation} \\tag{9.4} u^{(h)} = u^{(h-1)} - [\\mathcal{J}^{(h-1)}]^{-1} S(\\underline{Y};u^{(h-1)}). \\end{equation}\\] Arrêt quand \\[| u^{(h)} - u^{(h-1)}| \\leq \\Delta.\\] on pose \\(\\hat\\theta_{\\scriptsize MV}=u^{(h)}\\). Parfois, au lieu d’utiliser la matrice hessienne, on utilise la matrice d’information de Fisher \\[ \\mathcal{I}_n(\\theta)_{j,\\ell} = -\\mathbb{E}\\left[ \\frac{\\partial^2}{\\partial\\theta_j \\partial\\theta_\\ell} l(\\underline{Y};\\theta) \\right]. \\] C’est l’algorithme de Fisher-scoring. Ici aussi, on a besoin que \\(\\mathcal{I}_n(\\theta)\\) soit inversible, quitte à imposer des contraintes sur \\(\\theta\\). Cette solution peut permettre d’éviter des problèmes de non inversibilité de la hessienne. 9.3.3 Equations de vraisemblance Les algorithmes de type Newton-Raphson précédents nécessitent d’évaluer le score et la matrice d’information de Fisher. Proposition 9.2 Soit le score \\(S(\\underline{Y};\\theta) = \\left(S_1,\\ldots,S_k\\right)&#39;\\) avec \\(S_j= \\frac{\\partial}{\\partial \\theta_j} l(\\underline{Y};\\theta)\\). Alors pour \\(j\\in\\{1,\\ldots,k\\}\\), \\[\\begin{equation} S_j = \\sum_{i=1}^n \\frac{(Y_i-\\mu_i)x_{i}^{(j)}}{\\text{Var}(Y_i)}\\ \\frac{\\partial\\mu_i}{\\partial\\eta_i} \\tag{9.5} \\end{equation}\\] et \\(\\mathbb{E}[S_j]=0\\). La preuve est donnée en annexe B.6. Proposition 9.3 La matrice d’information de Fisher s’écrit \\[ \\mathcal{I}_n(\\theta)=\\mathbf{X&#39;WX} \\] où \\(\\mathbf{W}\\) est la matrice diagonale de “pondération” : \\[ [\\mathbf{W}]_{ii}=\\frac{1}{\\text{Var}(Y_i)}\\left(\\frac{\\partial\\mu_i}{\\partial\\eta_i}\\right)^2. \\] Exercise 9.2 On rappelle que \\(\\mathcal{I}_n(\\theta)\\) est la matrice de variance-covariance de \\(S(\\underline{Y};\\theta)\\) donc \\(\\left(\\mathcal{I}_n(\\theta)\\right)_{j\\ell}=\\mathbb{E}[S_j S_\\ell]\\). En utilisant (9.5), démontrez la Proposition 9.3. Corollary 9.1 Dans le cas particulier où la fonction lien est le lien canonique associé à la structure exponentielle alors \\(\\eta_i = \\omega_i=\\mathbf{x}_i\\theta\\). On obtient donc les simplifications suivantes : \\[ \\frac{\\partial\\mu_i}{\\partial\\eta_i} = \\frac{\\partial\\mu_i}{\\partial\\omega_i}=b&#39;&#39;(\\omega_i) = \\frac{Var(Y_i)}{\\gamma(\\phi)}. \\] Ainsi, \\[ S_j =\\sum_{i=1}^n \\frac{(Y_i-\\mu_i)}{\\gamma(\\phi)}x_{i}^{(j)} \\textrm{ et } W_{ii}=\\frac{Var(Y_i)}{\\gamma(\\phi)^2}. \\] En particulier, comme \\(\\mathcal{I}_n(\\theta)\\) ne dépend plus de \\(Y_i\\), la hessienne est égale à la matrice d’information de Fisher et donc les méthodes de résolution du score de Fisher et de Newton-Raphson coïncident. Si de plus \\(\\gamma(\\phi)\\) est une constante pour les observations, \\[ S_j = \\frac{1}{\\gamma(\\phi)} \\underset{i=1}{\\stackrel{n}{\\sum}} (Y_i - \\mu_i) x_i^{(j)} = 0\\ \\forall j \\iff X&#39; Y = X&#39; \\mu. \\] Dans le cas gaussien, comme \\(\\mu=X\\theta\\) avec la fonction de lien canonique identité, on retrouve la solution \\((X&#39;X)^{-1} X&#39;Y = \\theta\\) qui coincide avec celle obtenue par minimisation des moindres carrés. 9.4 Loi asymptotique de l’EMV et inférence De part la complexité du modèle linéaire généralisé, l’obtention d’un intervalle de confiance va nécessiter un peu plus de travail que dans un cadre de statistique paramétrique usuel. Le théorème suivant donne des propriétés sur l’estimateur du maximum de vraisemblance \\(\\hat \\theta_{\\scriptsize MV}\\). Theorem 9.1 Sous certaines conditions de régularité de la densité de probabilité, l’EMV vérifie les propriétés suivantes : \\(\\hat \\theta_{\\scriptsize MV}\\) converge en probabilité vers \\(\\theta\\in\\mathbb{R}^k\\) \\(\\hat \\theta_{\\scriptsize MV}\\) converge en loi vers une loi gaussienne : \\[\\mathcal{I}_n(\\theta)^{1/2} (\\hat \\theta_{\\scriptsize MV} - \\theta) \\underset{n\\rightarrow +\\infty}{\\stackrel{\\mathcal{L}}{\\longrightarrow}} \\mathcal{N}(0_k,I_k)\\] La statistique de Wald \\(\\mathcal{W}\\) vérifie \\[\\mathcal{W}:=(\\hat\\theta_{\\scriptsize MV} - \\theta)&#39; \\mathcal{I}_n(\\theta) (\\hat\\theta_{\\scriptsize MV} - \\theta) \\underset{n\\rightarrow +\\infty}{\\stackrel{\\mathcal{L}}{\\longrightarrow}} \\chi^2(k).\\] Dans le cas particulier où la distribution des \\(Y_i\\) est gaussienne et la fonction de lien est canonique, il est possible de montrer que l’estimateur du maximum de vraisemblance est lui aussi gaussien et ce sans avoir recours à l’approximation \\(\\hat \\theta_{\\scriptsize MV} - \\theta \\stackrel{\\mathcal{L}}{\\simeq} \\mathcal{N}(0_k,\\mathcal{I}_n(\\theta)^{-1})\\) quand \\(n\\rightarrow +\\infty\\). Si maintenant les erreurs ne sont pas gaussiennes, le résultat précédent propose une alternative intéressante aux tests de Fisher. Ce théorème permet déjà de répondre à des problèmes intéressants comme la construction d’intervalles de confiance pour les \\(\\theta_j\\), tests sur des valeurs de \\(\\theta\\),…. D’autres approches complémentaires sont disponibles pour ce type de modèle, la plus connue étant basée sur le test du rapport de vraisemblance. A noter qu’un tel résultat n’est pas utilisable tel quel puisque la matrice \\(\\mathcal{I}_n(\\theta)\\) est inconnue. Mais en remplaçant \\(\\mathcal{I}_n(\\theta)\\) par \\(\\mathcal{I}_n(\\hat\\theta_{\\scriptsize MV})\\) avec \\(\\hat \\theta_{\\scriptsize MV}\\) converge en probabilité vers \\(\\theta\\), on obtient que \\[ \\mathcal{I}_n(\\hat \\theta_{\\scriptsize MV})^{1/2} (\\hat \\theta_{\\scriptsize MV} - \\theta) \\underset{n\\rightarrow +\\infty}{\\stackrel{\\mathcal{L}}{\\longrightarrow}} \\mathcal{N}(0_k,I_k) \\] et \\[ (\\hat\\theta_{\\scriptsize MV} - \\theta)&#39; \\mathcal{I}_n(\\hat \\theta_{\\scriptsize MV}) (\\hat\\theta_{\\scriptsize MV} - \\theta) \\underset{n\\rightarrow +\\infty}{\\stackrel{\\mathcal{L}}{\\longrightarrow}} \\chi^2(k). \\] 9.5 Tests d’hypothèses Contrairement au cas du modèle linéaire, la loi de l’estimateur du maximum de vraisemblance dans le cadre du modèle linéaire généralisé n’est connue qu’asymptotiquement. Aussi les procédures de test vont être menées dans un cadre asymptotique. Nous allons dans la suite considérer plusieurs problèmes de test qui permettent d’examiner la qualités du modèle, de déterminer si les différentes variables explicatives du modèles sont pertinentes ou pas, …. 9.5.1 Test de modèles emboîtés Le test de comparaison des modèles emboîtés permet de déterminer si un sous-ensemble de variables explicatives est suffisant pour expliquer la réponse \\(Y\\) comme dans le cas du modèle linéaire. On considère deux modèles emboîtés \\(M_1\\) et \\(M_0\\), définis par \\(g(\\mu)=X_1\\theta_1\\) et \\(g(\\mu)=X_0\\theta_0\\) respectivement, avec \\(M_0\\) sous-modèle de \\(M_1\\). 9.5.1.1 Test du rapport de vraisemblance Pour traiter ce problème, on peut considérer le test du rapport de vraisemblance. Proposition 9.4 La statistique de test du test du rapport de vraisemblance est définie par \\[ T=-2 \\ln\\left[\\frac{L(\\underline{Y};\\hat \\theta_0)}{L(\\underline{Y};\\hat \\theta_1)}\\right] = -2 \\left[l(\\underline{Y};\\hat \\theta_0) - l(\\underline{Y};\\hat \\theta_1)\\right] \\] où \\(\\hat \\theta_0\\) et \\(\\hat \\theta_1\\) sont les EMV de \\(\\theta\\) dans le modèle \\(M_0\\) et \\(M_1\\) respectivement. Sous certaines conditions, on peut montrer que \\[ T\\underset{n\\rightarrow +\\infty}{\\stackrel{\\mathcal{L}}{\\rightarrow}} \\chi^2(k_1-k_0) \\] où \\(k_0\\) et \\(k_1\\) sont les dimensions des sous-espaces engendrés par les colonnes de \\(X_0\\) et \\(X_1\\) respectivement. La zone de rejet est alors définie par \\[ \\mathcal{R}_\\alpha = \\{T&gt; v_{1-\\alpha,k_1-k_0}\\} \\] où \\(v_{1-\\alpha,k_1-k_0}\\) est le \\((1-\\alpha)\\)- quantile de la loi du \\(\\chi^2\\) à \\(k_1-k_0\\) degrés de liberté. Ce test est parfois présenté de façon un peu différente en faisant intervenir la déviance. Definition 9.2 La déviance d’un modèle d’intérêt \\(M\\) est l’écart entre la log-vraisemblance du modèle \\(M\\) et celle du modèle le plus complet possible \\(M_{sat}\\), appelé modèle saturé. Le modèle saturé est le modèle comportant \\(n\\) paramètres, c’est à dire autant que d’observations. La déviance de \\(M\\) est définie par : \\[ \\mathcal{D}(M)= -2\\left[l(\\underline{Y}; \\hat \\theta) - l(\\underline{Y}; \\hat \\theta_{sat})\\right]. \\] Ainsi la statistique de test \\(T\\) peut se réécrire avec la déviance sous la forme \\[ T = \\mathcal D(M_0) - \\mathcal D(M_1). \\] 9.5.1.2 Test de Wald Comme dans le modèle linéaire, on peut reformuler les hypothèses et vouloir tester \\[ \\mathcal H_0: C \\theta = 0_q \\textrm{ contre } \\mathcal H_1: C \\theta \\neq 0_q \\] où \\(C\\in \\mathcal M_{qk}(\\mathbb{R})\\) (définie l’ensemble \\(\\mathcal{H}_0\\) des hypothèses à tester sur les paramètres correspondant à \\(q\\) contraintes). Proposition 9.5 Connaissant la loi asymptotique de \\(\\hat \\theta_{\\scriptsize MV}\\), on obtient que \\[ \\left[C \\mathcal{I}_n(\\hat\\theta_{\\scriptsize MV})^{-1} C&#39; \\right]^{-1/2} (C\\hat \\theta_{\\scriptsize MV} - C\\theta) \\underset{n\\rightarrow +\\infty}{\\stackrel{\\mathcal L}{\\longrightarrow}} \\mathcal N(0_q,I_q) \\] et \\[ (C\\hat \\theta_{\\scriptsize MV} - C\\theta)&#39; \\left[C \\mathcal{I}_n(\\hat\\theta_{\\scriptsize MV})^{-1} C&#39;\\right]^{-1} (C\\hat \\theta_{\\scriptsize MV} - C\\theta) \\underset{n\\rightarrow +\\infty}{\\stackrel{\\mathcal L}{\\longrightarrow}} \\mathcal \\chi^2(q). \\] On considère donc la zone de rejet \\[\\mathcal R_\\alpha = \\{ (C\\hat \\theta_{\\scriptsize MV} )&#39; \\left[C \\mathcal{I}_n(\\hat\\theta_{\\scriptsize MV})^{-1} C&#39;\\right]^{-1} (C\\hat \\theta_{\\scriptsize MV}) &gt; v_{1-\\alpha,q} \\}\\] où \\(v_{1-\\alpha,q}\\) est le \\((1-\\alpha)\\) quantile d’un \\(\\chi^2(q)\\). Ce test est appelé test de Wald. Le test de Wald est basé sur la forme quadratique faisant intervenir la matrice de covariance des paramètres, l’inverse de la matrice d’information observée \\((X&#39;WX)^{-1}\\). Cette matrice généralise la matrice \\((X&#39;X)^{-1}\\) utilisée dans le cas du modèle linéaire gaussien en faisant intervenir une matrice \\(W\\) de pondération. Ainsi, les test de Wald et test de Fisher sont équivalents dans le cas particulier du modèle gaussien. Attention, le test de Wald peut ne pas être précis si le nombre d’observations est faible. 9.5.2 Test d’un paramètre \\(\\theta_j\\) On s’intéresse ici à tester quelles sont les variables qui ont une influence. On revient au problème général de vouloir tester \\(\\mathcal H_0: \\theta_j = a\\) contre \\(\\mathcal H_0: \\theta_j \\neq a\\), où \\(a\\) est une valeur définie a priori (souvent \\(a=0\\)). Proposition 9.6 D’après le théorème 9.1, on peut faire l’approximation de loi suivante sous \\(\\mathcal{H}_0\\) : \\[ (\\hat \\theta_{\\scriptsize MV})_j - a \\underset{\\mathcal{H}_0}{\\stackrel{\\mathcal L}{\\simeq}} \\mathcal{N}\\left(0,[\\mathcal{I}_{n}(\\hat \\theta_{\\scriptsize MV})^{-1}]_{jj}\\right). \\] On va donc rejeter \\(\\mathcal{H}_0\\) si \\[ T_j := \\left| (\\hat \\theta_{\\scriptsize MV})_j - a \\right| / \\sqrt{[\\mathcal{I}_{n}(\\hat \\theta_{\\scriptsize MV})^{-1}]_{jj}} &gt; z_{1-\\alpha/2} \\] où \\(z_{1-\\alpha/2}\\) est le \\(1-\\alpha/2\\) quantile de la loi \\(\\mathcal N(0,1)\\). Ce test est appelé le Z-test. Remark. Ce test est équivalent au test de Wald : on rejette \\(\\mathcal{H}_0\\) si \\[ \\left[ (\\hat \\theta_{\\scriptsize MV})_j - a \\right]^2 / [\\mathcal{I}_{n}(\\hat \\theta_{\\scriptsize MV})^{-1}]_{jj} &gt; v_{1-\\alpha,1} \\] où \\(v_{1-\\alpha,1}\\) est le \\(1-\\alpha\\) quantile de la loi \\(\\chi^2(1)\\). 9.6 Intervalle de confiance pour \\(\\theta_j\\) 9.6.1 Par Wald D’après le théorème 9.1, on peut faire l’approximation de loi suivante : \\[ \\left[ (\\hat \\theta_{\\scriptsize MV})_j - \\theta_j \\right] / \\sqrt{[\\mathcal{I}_{n}(\\hat\\theta_{\\scriptsize MV})^{-1}]_{jj}} \\underset{n\\rightarrow +\\infty}{\\stackrel{\\mathcal L}{\\longrightarrow}} \\mathcal{N}\\left(0, 1 \\right). \\] On peut donc construire l’intervalle de confiance asymptotique pour \\(\\theta_j\\) au niveau de confiance \\(1-\\alpha\\) suivant : \\[ IC_{1-\\alpha}(\\theta_j) = \\left [ (\\hat \\theta_{\\scriptsize MV})_j \\pm z_{1-\\alpha/2} \\sqrt{[\\mathcal{I}_{n}(\\hat\\theta_{\\scriptsize MV})^{-1}]_{jj}} \\right] \\] où \\(z_{1-\\alpha/2}\\) est le \\(1-\\alpha/2\\) quantile de la loi \\(\\mathcal N(0,1)\\). 9.6.2 Fondé sur le rapport de vraisemblances La fonction de vraisemblance profil de \\(\\theta_j\\) est définie par \\[ l^\\star(\\underline{Y}; \\theta_j) = \\underset{\\tilde\\theta}{\\max}\\ l(\\underline{Y}; \\tilde\\theta) \\] où \\(\\tilde\\theta\\) est le vecteur \\(\\theta\\) avec le \\(j\\)ème élément fixé à \\(\\theta_j\\). Si \\(\\theta_j\\) est la vraie valeur du paramètre alors \\[ 2 \\left[ l(\\underline{Y}; \\hat\\theta_{\\scriptsize MV}) - l^\\star(\\underline{Y}; \\theta_j) \\right] \\underset{n\\rightarrow +\\infty}{\\stackrel{\\mathcal L}{\\longrightarrow}} \\chi^2(1). \\] Ainsi, si on considère l’ensemble \\[ \\mathcal G=\\left\\{u; 2 \\left[ l(\\underline{Y}; \\hat\\theta_{\\scriptsize MV}) - l^\\star(\\underline{Y}; u) \\right] \\leq v_{1-\\alpha,1} \\right\\}, \\] on obtient que \\(\\mathbb{P}(\\theta_j\\in\\mathcal G) \\underset{n\\rightarrow +\\infty}{\\longrightarrow} 1-\\alpha\\). Ainsi \\(\\mathcal G\\) est un intervalle de confiance asymptotique pour \\(\\theta_j\\) au niveau de confiance \\(1-\\alpha\\). 9.7 Qualité d’ajustement 9.7.1 Le pseudo \\(R^2\\) Definition 9.3 Le pseudo-\\(R^2\\) d’un modèle d’intérêt \\(M\\) est défini par \\[ pseudoR^2 = \\frac{\\mathcal D(M_0)-\\mathcal D(M)}{\\mathcal D(M_0)}. \\] où \\(M_0\\) est le modèle nul (juste un intercept). Cette définition est obtenue par analogie avec le \\(R^2=1-\\frac{SSR}{SST}\\) utilisé dans le cadre du modèle linéaire. En effet, on rappelle que \\(SST\\) est la somme des carrés des résidus pour le modèle nul \\(M_0\\). Ce pseudo-\\(R^2\\) varie entre \\(0\\) et \\(1\\). Plus il est proche de \\(1\\), meilleur est l’ajustement du modèle. 9.7.2 Le \\(\\chi^2\\) de Pearson généralisé Le \\(\\chi^2\\) de Pearson généralisé est la statistique définie par \\[ Z^2=\\sum_{i=1}^{n}\\frac{(Y_i-\\hat \\mu_i)^2}{\\mbox{Var}_{\\widehat\\mu_i}(Y_i)} \\] où \\(\\hat \\mu_i= g^{-1}(\\textbf{x}_i\\hat \\theta_{\\scriptsize MV})\\) et \\(\\mbox{Var}_{\\widehat\\mu_i}(Y_i)= \\mbox{Var}_{\\mu}(Y_i)|_{\\mu=\\widehat\\mu_i}\\) ( la variance théorique de \\(Y_i\\) évaluée en \\(\\hat \\mu_i\\)). Sous l’hypothèse que le modèle étudié est le bon modèle et si l’approximation asymptotique est valable, alors la loi de \\(Z^2\\) est approchée par \\(\\chi^2(n-k)\\). On rejette donc le modèle étudié au niveau \\(\\alpha\\) si la valeur observée de \\(Z^2\\) est supérieure au \\((1-\\alpha)\\) quantile de la loi \\(\\chi^2(n-k)\\). 9.8 Diagnostic, résidus Dans le modèle linéaire généralisé, la définition la plus naturelle pour le résidu consiste à quantifier l’écart entre \\(Y_i\\) et sa prédiction par le modèle \\(\\hat \\mu_i\\). On définit ainsi les résidus bruts \\(\\varepsilon_i = Y_i - \\hat \\mu_i\\). Mais ces résidus n’ayant pas toujours la même variance, il est difficile de les comparer à un comportement type attendu. Par exemple dans le cas d’un modèle de Poisson, l’écart-type d’un effectif est \\(\\sqrt{\\hat \\mu_i}\\), de grosses différences tendent à apparaitre quand \\(\\mu_i\\) prend des valeurs élevées. En normalisant les résidus bruts par une variance estimée, on obtient les résidus “standardisés” de Pearson : \\[ r_{Pi} = \\frac{Y_i - \\hat \\mu_i}{\\sqrt{Var_{\\hat\\mu_i}(Y_i)}}. \\] On remarque que la somme des carrés des \\(r_{Pi}\\) correspond au \\(\\chi^2\\) de Pearson généralisé. On peut également étudier les résidus déviance définis par : \\[ r_{Di}=\\sqrt{d_i}\\ sgn(Y_i - \\hat \\mu_i) \\] où \\(d_i\\) représente la contribution de l’observation \\(i\\) à la déviance \\(\\mathcal D\\). Du fait que les résidus sont calculés sur les données de l’échantillon qui ont permis de construire le modèle, on risque de sous-estimer les résidus. En notant \\(h_i\\) le levier associé à l’observation \\(i\\), \\(i\\)ème terme diagonal de la matrice \\(H=\\mathbf{W}^{1/2}\\mathbf{X(X&#39;WX)}^{-1}\\mathbf{X&#39;}\\mathbf{W}^{1/2}\\), on définit alors : le résidu de Pearson normalisé : \\[r_{Pi}^\\star = \\frac{Y_i - \\hat \\mu_i}{\\sqrt{Var_{\\hat\\mu_i}(Y_i)(1-h_i)}}\\] le résidu déviance normalisé : \\[r_{Di}^\\star = \\frac{\\sqrt{d_i}\\ sgn(Y_i - \\hat \\mu_i)}{\\phi(1-h_i)}\\] le résidu vraisemblance normalisé : \\[r_{Gi}=sgn(Y_i-\\hat\\mu_i) \\sqrt{(1-h_i) r_{Di}^{\\star\\ 2} + h_i r_{Pi}^{\\star\\ 2}}\\] 9.9 En résumé Savoir modéliser un MLG en précisant bien les 3 parties (compo. aléatoire, compo. linéaire, fonction de lien) Savoir montrer qu’une loi fait partie de la famille exponentielle (la définition n’est pas à connaitre, elle sera rappelée si besoin) Connaitre la fonction de lien canonique pour les lois gaussienne, Bernoulli et Poisson Comprendre l’esprit général pour déterminer un estimateur du vecteur des paramètres en MLG Connaitre le théorème sur la loi de l’estimateur \\(\\hat\\theta_{{\\scriptsize MV}}\\) en MLG Savoir construire un test de modèles emboités, un test de Wald et un \\(Z\\)-test Construire un IC pour \\(\\theta_j\\) par Wald Connaitre la définition du pseudo-\\(R^2\\) References "],["RegLogistique.html", "Chapitre 10 Régression logistique 10.1 Introduction 10.2 Pourquoi des modèles particuliers ? 10.3 Odds et odds ratio 10.4 Régression logistique simple 10.5 Régression logistique multiple 10.6 Quelques codes avec python 10.7 Régression polytomique", " Chapitre 10 Régression logistique Les slides associés à la régression logistique sont disponibles ici (Partie I) Le jeu de données Default utilisé pour la régression logistique est issu de la librairie ISLR. Le jeu de données utilisé pour illustrer la régression polytomique non-ordonnée est MarqueSexe.csv Le jeu de données utilisé pour illustrer la régression polytomique ordonnée est SunRain.csv Remarque pour les étudiant-e-es de l’UF EMS : la partie sur la régression polytomique est hors programme. 10.1 Introduction Dans ce chapitre, on s’intéresse au cas où la variable réponse \\(Y\\) est binaire. Nous allons illustrer les différents points abordés dans ce chapitre avec l’exemple Default issu de la librarie ISLR. Example 10.1 Problème de défaut bancaire On s’intéresse à la variable réponse binaire default qui indique si des clients sont en défaut sur leur dette de carte de crédit (default=1 si le client fait défaut sur sa dette, 0 sinon). On considère ici un échantillon de \\(n=10000\\) clients et l’on souhaite expliquer la variable default à l’aide des \\(3\\) variables suivantes : student : 1 si le client est étudiant, 0 sinon balance : montant moyen mensuel d’utilisation de la carte de crédit income : revenu du client data(Default) attach(Default) summary(Default) default student balance income No :9667 No :7056 Min. : 0.0 Min. : 772 Yes: 333 Yes:2944 1st Qu.: 481.7 1st Qu.:21340 Median : 823.6 Median :34553 Mean : 835.4 Mean :33517 3rd Qu.:1166.3 3rd Qu.:43808 Max. :2654.3 Max. :73554 On considère donc ici des variables explicatives quantitatives et qualitatives. Le comportement de ces variables est résumé sur la Figure 10.1. Figure 10.1: Résumé des 4 variables de l’exemple de défaut bancaire. L’ensemble des méthodes de modélisation disponible pour apporter des réponses à ce type de problème est désigné par le terme de régression logistique. 10.2 Pourquoi des modèles particuliers ? Dans la suite, on note \\(Y=(Y_1,\\ldots,Y_n)&#39;\\ \\in\\{0,1\\}^n\\) le vecteur des réponses, et \\(\\textbf{x}_i\\) le vecteur ligne des variables explicatives considérées pour l’individu \\(i\\) dans \\(\\{1\\ldots,n\\}\\). La variable réponse à expliquer \\(Y_i | \\textbf{x}_i \\sim \\mathcal B(\\pi(\\textbf{x}_i))\\) vérifie \\[ \\mathbb{P}(Y_i=1|\\textbf{x}_i) = \\pi(\\textbf{x}_i). \\] L’objectif est de construire un modèle pour reconstituer \\(\\pi(\\textbf{x}_i)=\\mathbb{E}[Y_i | \\textbf{x}_i]\\) en fonction des variables explicatives. Si on utilise le modèle de régression usuel \\(Y_i=\\textbf{x}_i\\theta +\\varepsilon_i\\) pour une variable binaire, l’e résidu’erreur serait distribuée selon la loi \\[ \\varepsilon_i=\\left\\{\\begin{array}{ll} 1 - \\textbf{x}_i \\theta &amp; \\textrm{ avec probabilité } \\pi(\\textbf{x}_i),\\\\ -\\textbf{x}_i \\theta &amp; \\textrm{ avec probabilité } 1-\\pi(\\textbf{x}_i). \\end{array}\\right. \\] ce qui est trop éloigné des hypothèses usuelles de normalité des erreurs. De plus, la régression linéaire implique que \\(\\mathbb{E}[Y_i | \\textbf{x}_i]= \\textbf{x}_i \\theta\\). Or \\(Y_i | \\textbf{x}_i \\sim \\mathcal B(\\pi(\\textbf{x}_i))\\) donc \\(\\pi(\\textbf{x}_i)=\\textbf{x}_i \\theta\\). Cependant, rien n’indique que \\(\\textbf{x}_i \\theta\\in[0,1]\\). Les méthodes proposées partent du principe que le phénomène étudié est l’observation de \\(Y_i\\) (binaire), qui est la manifestation visible d’une variable \\(Z_i\\) latente (non observée) continue : \\(Y_i = \\mathbb{1}_{Z_i &gt;0}\\). On considère un modèle linéaire entre \\(Z_i\\) et \\(\\textbf{x}_i\\) : \\[ Z_i = \\textbf{x}_i \\theta + \\varepsilon_i. \\] On peut alors remarquer que \\[ \\pi(\\textbf{x}_i) = \\mathbb{P}(Y_i=1 | \\textbf{x}_i ) = \\mathbb{P}(Z_i&gt;0|\\textbf{x}_i) = \\mathbb{P}(-\\varepsilon_i &lt; \\textbf{x}_i \\theta) = F(\\textbf{x}_i \\theta), \\] où \\(F\\) est la fonction de répartition de \\(-\\varepsilon_i\\), qui correspond à l’inverse de la fonction de lien \\(g\\). Le choix du modèle porte donc sur le choix de cette fonction de répartition \\(F\\) ou de façon équivalente à la fonction de lien \\(g\\). Dans le cadre binaire, les fonctions les plus usuellement utilisées sont (Figure 10.2) : la fonction logistique: \\[ F(u)=\\frac{e^u}{1+e^u} \\quad \\Longleftrightarrow \\quad g(\\pi) = \\ln\\left(\\frac{\\pi}{1-\\pi}\\right)=\\textrm{logit}(\\pi). \\] Cette fonction est bien adaptée à la modélisation de probabilité car elle prend ses valeurs dans \\([0,1]\\). Dans ce cas, on parle de modèle logistique. la fonction probit : \\(F\\) est la fonction de répartition de la loi \\(\\mathcal N(0,1)\\) et donc \\(g=F^{-1}\\) est la fonction probit. Dans ce cas, on parle de modèle probit. la fonction Gompit ou log-log : \\(F\\) est la fonction de répartition de la loi de Gompertz \\[ F(u) = 1 - \\exp\\left(-e^{u}\\right) \\quad \\Longleftrightarrow \\quad g(\\pi) =\\ln[-\\ln(1-\\pi)], \\] mais cette fonction est dissymétrique. Dans ce cas, on parle de modèle log-log. Figure 10.2: Comparaison des fonctions de répartition (à gauche) et des fonctions de lien g (à droite). 10.3 Odds et odds ratio Il est souvent difficile d’interpréter directement les coefficients \\(\\theta\\), l’interprétation se fait plutôt via les odds ratio. Ces odds ratio servent à mesurer l’effet d’une variable quantitative ou le contraste entre les effets d’une variable qualitative. L’idée générale est de raisonner en termes de probabilités ou de rapport de “chances” (odds). Definition 10.1 L’odds (chance) pour un individu \\({\\bf x}\\) d’obtenir la réponse \\(Y=1\\) est défini par : \\[ \\textrm{odds}({\\bf x})= \\frac{\\pi({\\bf x})}{1-\\pi({\\bf x})}, \\quad \\textrm{ avec } \\pi({\\bf x})=\\mathbb{P}(Y=1|{\\bf x}). \\] L’odds ratio (rapport de chances) entre deux individus \\({\\bf x}\\) et \\(\\tilde {\\bf x}\\) est \\[ \\textrm{OR}({\\bf x},\\tilde {\\bf x}) = \\frac{\\textrm{odds}({\\bf x})}{\\textrm{odds}(\\tilde {\\bf x})}. \\] Example 10.2 Si un joueur \\({\\bf x}\\) a une probabilité \\(\\pi({\\bf x}) = 1/4 = 0.25\\) de gagner à un jeu, l’odds de succès vaut \\(0.25/0.75 = 1/3\\). On dira que les chances de succès sont de 1 contre 3. Remarquons que l’odds d’échec est de \\(0.75/0.25 = 3\\) et on dira que les chances d’échec sont de 3 contre 1. Les odds ratio peuvent être utilisés de plusieurs manières : Comparaison de probabilités de succès entre deux individus : \\[ \\left\\{\\begin{array}{r c l} \\textrm{OR}({\\bf x},\\tilde {\\bf x}) &gt;1 &amp;\\Leftrightarrow &amp; \\pi({\\bf x}) &gt; \\pi(\\tilde {\\bf x})\\\\ \\textrm{OR}({\\bf x},\\tilde {\\bf x}) =1 &amp;\\Leftrightarrow &amp; \\pi({\\bf x}) = \\pi(\\tilde {\\bf x})\\\\ \\textrm{OR}({\\bf x},\\tilde {\\bf x}) &lt; 1 &amp;\\Leftrightarrow &amp; \\pi({\\bf x}) &lt; \\pi(\\tilde {\\bf x})\\\\ \\end{array} \\right. \\] Mesure de l’impact d’une variable : pour le modèle logistique avec intercept, \\[ \\textrm{logit}[\\pi({\\bf x})] = \\theta_0 + \\theta_1 x^{(1)} + \\ldots + \\theta_p x^{(p)}, \\] il est facile de vérifier que \\[ \\textrm{OR}({\\bf x},\\tilde {\\bf x}) = \\prod_{j=1}^p \\exp\\left[\\theta_j (x^{(j)} - \\tilde x^{(j)})\\right]. \\] Pour mesurer l’influence d’une variable sur l’odds ratio, il suffit de considérer deux individus qui diffèrent uniquement sur la \\(j\\)ème variable. On obtient alors \\[ \\textrm{OR}({\\bf x},\\tilde {\\bf x}) = \\exp\\left[\\theta_j (x^{(j)} - \\tilde x^{(j)})\\right]. \\] Ainsi une variation de la \\(j\\)ème variable d’une unité correspond à un odds ratio \\(\\exp(\\theta_j)\\) qui est uniquement fonction du coefficient \\(\\theta_j\\). Le coefficient \\(\\theta_j\\) permet de mesurer l’influence de la \\(j\\)ème variable sur le rapport \\(\\pi({\\bf x})/[1-\\pi({\\bf x})]\\) lorsque \\(x^{(j)}\\) varie d’une unité, et ce, indépendamment de la valeur \\(x^{(j)}\\). Une telle analyse peut se révéler intéressante pour étudier l’influence d’un changement d’état d’une variable qualitative. Interprétation d’un risque relatif : si \\(\\pi({\\bf x})\\) et \\(\\pi(\\tilde {\\bf x})\\) sont petits par rapport à 1 (ex pour une maladie rare), l’odds ratio peut être approché par \\(\\pi({\\bf x})/\\pi(\\tilde {\\bf x})\\). On fait alors une interprétation simple : si \\(\\textrm{OR}({\\bf x},\\tilde {\\bf x}) = 4\\), la réponse \\(Y=1\\) est 4 fois plus probable en \\({\\bf x}\\) qu’en \\(\\tilde {\\bf x}\\). 10.4 Régression logistique simple Dans cette section, on cherche à expliquer la variable réponse binaire \\(Y\\) par une seule variable explicative. Nous allons distinguer deux cas : celui où la variable explicative est quantitative et celui où elle est qualitative. 10.4.1 Avec une variable explicative quantitative Dans notre exemple, nous allons chercher à expliquer la variable default à l’aide de la variable explicative balance. La Figure 10.3 montre qu’il est difficile de modéliser les données brutes mais si on regroupe les clients par classe de valeurs de balance, la liaison entre balance et default devient plus claire. Il apparait que lorsque le montant mensuel d’utilisation de la carte de crédit augmente, la proportion de clients en défaut augmente. Au vu de la forme de la courbe de liaison, une modélisation avec le lien logit semble “naturelle”. On va donc chercher à modéliser l’espérance conditionnelle de \\(Y_i\\) sachant \\(\\textbf{x}_i=(1,x_i)\\), par \\(\\mathbb{E}[Y_i|\\textbf{x}_i] = \\pi_\\theta(\\textbf{x}_i)\\), où \\[ \\pi_\\theta(\\textbf{x}_i) = F(\\theta_0 + \\theta_1 x_i)=\\frac{e^{\\theta_0 + \\theta_1 x_i}}{1+e^{\\theta_0 + \\theta_1 x_i}} \\quad \\Longleftrightarrow \\quad \\ln\\left(\\frac{\\pi_\\theta({\\bf x_i})}{1-\\pi_\\theta({\\bf x_i})}\\right) = \\theta_0 + \\theta_1 x_i. \\] Figure 10.3: A gauche, représentation de default en fonction de balance. A droite, proportion de clients en défaut par classe de valeurs pour balance. 10.4.1.1 Estimation des paramètres Les paramètres \\(\\theta=(\\theta_0,\\theta_1)\\) sont estimés par la méthode du maximum de vraisemblance. La vraisemblance des données \\(\\underline{Y}=(Y_1,\\ldots, Y_n)\\) est définie par : \\[ L(\\underline{Y}; \\theta) = \\prod_{i=1}^n \\pi_\\theta(\\textbf{x}_i)^{Y_i} [1-\\pi_\\theta(\\textbf{x}_i)]^{1-Y_i}, \\] et la log-vraisemblance par : \\[\\begin{eqnarray*} l(\\underline{Y}; \\theta) &amp;=&amp; \\sum_{i=1}^n \\big\\{Y_i \\ln[\\pi_\\theta(\\textbf{x}_i)] + (1-Y_i)\\ln[1-\\pi_\\theta(\\textbf{x}_i)]\\big\\}\\\\ &amp;=&amp; \\sum_{i=1}^n \\big\\{Y_i \\ln\\left[F(\\theta_0 + \\theta_1 x_i) \\right] + (1-Y_i)\\ln\\left[1-F(\\theta_0 + \\theta_1 x_i)\\right]\\big\\} \\end{eqnarray*}\\] On cherche alors à annuler les dérivées partielles. On commence par remarquer que si \\(F(u)=e^u/(1+e^u)\\), on a \\(F&#39;(u) = F(u) \\left[1-F(u)\\right]\\). D’où \\[\\begin{eqnarray*} \\frac{\\partial l(\\underline{Y}; \\theta) }{\\partial \\theta_0} &amp;=&amp; \\sum_{i=1}^n \\left[Y_i \\frac{F&#39;(\\theta_0 + \\theta_1 x_i)}{F(\\theta_0 + \\theta_1 x_i)} - (1-Y_i) \\frac{F&#39;(\\theta_0 + \\theta_1 x_i)}{1-F(\\theta_0 + \\theta_1 x_i)}\\right] \\\\ &amp;=&amp; \\sum_{i=1}^n \\left[Y_i \\left[ 1 - F(\\theta_0 + \\theta_1 x_i)\\right] - (1-Y_i) F(\\theta_0 + \\theta_1 x_i)\\right], \\end{eqnarray*}\\] et \\[\\begin{eqnarray*} \\frac{\\partial l(\\underline{Y}; \\theta) }{\\partial \\theta_1} &amp;=&amp; \\sum_{i=1}^n \\left[Y_i x_i \\frac{F&#39;(\\theta_0 + \\theta_1 x_i)}{F(\\theta_0 + \\theta_1 x_i)} - (1-Y_i) x_i \\frac{F&#39;(\\theta_0 + \\theta_1 x_i)}{1-F(\\theta_0 + \\theta_1 x_i)}\\right] \\\\ &amp;=&amp; \\sum_{i=1}^n \\left[x_i \\left[ Y_i - F(\\theta_0 + \\theta_1 x_i)\\right]\\right]. \\end{eqnarray*}\\] On obtient donc le système suivant : \\[ \\left\\{\\begin{array}{l} \\sum_{i=1}^n \\left[Y_i - F(\\theta_0 + \\theta_1 x_i)\\right] = 0 \\\\ \\sum_{i=1}^n x_i \\left[ Y_i - F(\\theta_0 + \\theta_1 x_i) \\right]= 0 \\end{array}\\right. \\quad\\Longleftrightarrow\\quad \\left\\{\\begin{array}{l} \\sum_{i=1}^n \\left[Y_i - \\pi_\\theta(\\textbf{x}_i)\\right]= 0 \\\\ \\sum_{i=1}^n x_i \\left[ Y_i - \\pi_\\theta(\\textbf{x}_i) \\right]= 0 \\end{array}\\right. \\] Pour mettre ensuite en place un algorithme de type Newton-Raphson ou de Fisher-scoring, on a besoin d’évaluer la matrice hessienne ou la matrice d’information de Fisher. Pour cela, on évalue les dérivées secondes : \\[ \\left\\{ \\begin{array}{l} \\displaystyle\\frac{\\partial ^2 l(\\underline{Y};\\theta)}{\\partial \\theta_0^2} = - \\sum_{i=1}^n F&#39;(\\theta_0 + \\theta_1 x_i) = - \\sum_{i=1}^n F(\\theta_0 + \\theta_1 x_i) [1-F(\\theta_0 + \\theta_1 x_i)]\\\\ \\\\ \\displaystyle\\frac{\\partial ^2 l(\\underline{Y};\\theta)}{\\partial \\theta_1^2} = - \\sum_{i=1}^n x_i^2 F&#39;(\\theta_0 + \\theta_1 x_i) = - \\sum_{i=1}^n x_i^2 F(\\theta_0 + \\theta_1 x_i) [1-F(\\theta_0 + \\theta_1 x_i)]\\\\ \\\\ \\displaystyle\\frac{\\partial ^2 l(\\underline{Y};\\theta)}{\\partial \\theta_0 \\partial \\theta_1} = - \\sum_{i=1}^n x_i F&#39;(\\theta_0 + \\theta_1 x_i) = - \\sum_{i=1}^n x_i F(\\theta_0 + \\theta_1 x_i) [1-F(\\theta_0 + \\theta_1 x_i)] \\end{array} \\right. \\] La matrice d’information de Fisher vaut alors \\[ \\mathcal{I}_n(\\theta) = \\left( \\begin{array}{c c} \\displaystyle\\sum_{i=1}^n \\pi_\\theta(\\textbf{x}_i) (1-\\pi_\\theta(\\textbf{x}_i)) &amp; \\displaystyle \\sum_{i=1}^n x_i \\pi_\\theta(\\textbf{x}_i) (1-\\pi_\\theta(\\textbf{x}_i)) \\\\ \\displaystyle\\sum_{i=1}^n x_i \\pi_\\theta(\\textbf{x}_i) (1-\\pi_\\theta(\\textbf{x}_i)) &amp; \\displaystyle \\sum_{i=1}^n x_i^2 \\pi_\\theta(\\textbf{x}_i) (1-\\pi_\\theta(\\textbf{x}_i)) \\end{array}\\right) = \\left(X&#39; W X \\right), \\] avec \\[ X=\\left(\\begin{array}{c c}1 &amp; x_1\\\\ 1 &amp; x_2 \\\\ \\vdots &amp; \\vdots\\\\ 1 &amp; x_n\\end{array}\\right) \\textrm{ et } W = \\mbox{diag}\\big[\\pi_\\theta(\\textbf{x}_1) (1-\\pi_\\theta(\\textbf{x}_1))\\ ,\\ \\ldots \\ ,\\ \\pi_\\theta(\\textbf{x}_n) (1-\\pi_\\theta(\\textbf{x}_n)))\\big]. \\] Sur notre exemple, on ajuste ce modèle entre la variable default et la variable balance avec la fonction glm() de R. glm.balance&lt;-glm(default~balance,data=Default,family=binomial(link=&quot;logit&quot;)) summary(glm.balance) Call: glm(formula = default ~ balance, family = binomial(link = &quot;logit&quot;), data = Default) Deviance Residuals: Min 1Q Median 3Q Max -2.2697 -0.1465 -0.0589 -0.0221 3.7589 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -1.065e+01 3.612e-01 -29.49 &lt;2e-16 *** balance 5.499e-03 2.204e-04 24.95 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 2920.6 on 9999 degrees of freedom Residual deviance: 1596.5 on 9998 degrees of freedom AIC: 1600.5 Number of Fisher Scoring iterations: 8 10.4.1.2 Prédiction Une fois le modèle ajusté, on obtient une estimation pour chaque prédicteur linéaire \\(\\eta_i=\\theta_0 + x_i\\theta_1\\) par \\(\\hat \\eta_i=\\hat \\theta_0 + x_i\\hat\\theta_1\\) et pour chaque paramètre \\[\\hat \\pi({\\bf x}_i) = F(\\hat\\eta_i) = F(\\hat \\theta_0 + \\hat \\theta_1 x_i) = \\pi_{\\hat\\theta}(\\textbf{x}_i).\\] En appliquant ensuite la règle de Bayes sur les \\(\\hat \\pi({\\bf x}_i)\\), on récupère les valeurs ajustées \\(\\widehat Y_i\\) pour les \\(Y_i\\) : \\[ \\widehat Y_i = \\left\\{\\begin{array}{l l } 1 &amp; \\textrm{ si } \\hat \\pi({\\bf x}_i) &gt; s\\\\ 0 &amp; \\textrm{ sinon}.\\end{array}\\right. \\] où \\(s\\) est un seuil choisi souvent à \\(0.5\\) par défaut. On peut alors comparer par une table de contingence les valeurs prédites par le modèle avec les valeurs observées des réponses. Dans notre exemple, on obtient hatpi &lt;- glm.balance$fitted.values hatY &lt;- (hatpi &gt; 0.5) table(default,hatY) hatY default FALSE TRUE No 9625 42 Yes 233 100 On constate que l’on retrouve assez bien les clients n’ayant pas de défaut, mais mal ceux avec un défaut. Si on se donne maintenant un nouvel individu décrit par \\(\\textbf{x}_0=(1,x_0)\\) alors le modèle ajusté permet de prédire une proportion \\(\\hat \\pi(\\textbf{x}_0)=F(\\hat \\theta_0 + \\hat \\theta_1 x_0)\\) et une réponse prédite \\(\\widehat Y_0 = \\mathbb{1}_{\\hat \\pi(\\textbf{x}_0) &gt; 0.5}\\). La Figure 10.4 représente l’ajustement des proportions estimées par le modèle logistique et par le modèle probit avec les proportions observées. Figure 10.4: Représentation des proportions prédites par le modèle logistique et par le modèle probit. 10.4.1.3 Intervalle de confiance Pour obtenir un intervalle de confiance pour chaque \\(\\theta_j\\), on peut utiliser la méthode de Wald ou la méthode fondée sur le rapport de vraisemblance présentées en section 9.6. Sous R, le premier est obtenu avec la commande confint.default(), le second avec la commande confint(). Sur notre exemple, on obtient : confint(glm.balance) 2.5 % 97.5 % (Intercept) -11.383288936 -9.966565064 balance 0.005078926 0.005943365 confint.default(glm.balance) 2.5 % 97.5 % (Intercept) -11.359186056 -9.943475172 balance 0.005066999 0.005930835 10.4.1.4 Test de nullité des paramètres Si l’on souhaite tester \\(\\mathcal{H}_0: \\theta_j= 0\\) contre \\(\\mathcal{H}_1: \\theta_j \\neq 0\\), on reprend la construction du \\(Z\\)-test décrite en section 9.5.2. Pour rappel, il suffit de remarquer que sous \\(\\mathcal{H}_0\\), \\[ \\frac{\\hat \\theta_j}{\\hat \\sigma_j} \\stackrel{\\mathcal L}{\\underset{n\\to +\\infty}{\\longrightarrow}} \\mathcal{N}(0,1) \\] avec \\(\\hat{\\sigma}_j = \\sqrt{[\\mathcal{I}(\\hat{\\theta}_{MV})^{-1}]_{jj}}\\). On peut alors construire un test asymptotique de niveau \\(\\alpha\\) de zone de rejet \\[ \\mathcal{R}_\\alpha = \\left\\{ \\left| \\hat \\theta_j / \\hat \\sigma_j \\right| &gt; z_{1-\\alpha/2} \\right\\}, \\] où \\(z_{1-\\alpha/2}\\) est le \\(1-\\alpha/2\\). Pour notre exemple, on obtient summary(glm.balance) Call: glm(formula = default ~ balance, family = binomial(link = &quot;logit&quot;), data = Default) Deviance Residuals: Min 1Q Median 3Q Max -2.2697 -0.1465 -0.0589 -0.0221 3.7589 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -1.065e+01 3.612e-01 -29.49 &lt;2e-16 *** balance 5.499e-03 2.204e-04 24.95 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 2920.6 on 9999 degrees of freedom Residual deviance: 1596.5 on 9998 degrees of freedom AIC: 1600.5 Number of Fisher Scoring iterations: 8 Les \\(p\\)-valeurs étant \\(&lt;2e^{-16}\\), on rejette la nullité pour les deux coefficients. On peut aussi voir le problème comme un test de sous-modèle et le résoudre avec un test de Wald ou un test du rapport du maximum de vraisemblance. Par exemple, pour tester la nullité de \\(\\theta_1\\) : anova(glm(default~1,data=Default,family=binomial(link=&quot;logit&quot;)), glm.balance, test=&quot;Chisq&quot;) Analysis of Deviance Table Model 1: default ~ 1 Model 2: default ~ balance Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) 1 9999 2920.7 2 9998 1596.5 1 1324.2 &lt; 2.2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 10.4.2 Avec une variable explicative qualitative On va chercher ici à expliquer la variable default par rapport à la variable student (binaire). On peut considérer le modèle suivant : \\[ \\textrm{logit}(\\pi({\\bf x}_i)) = \\ln\\left(\\frac{\\pi({\\bf x}_i)}{1-\\pi({\\bf x}_i)}\\right) = \\theta_0 + \\theta_1 \\mathbb{1}_{x_i =1} + \\theta_2 \\mathbb{1}_{x_i =0}. \\] Mais on peut remarquer qu’il est possible d’écrire le modèle également sous la forme \\[ \\textrm{logit}(\\pi({\\bf x}_i)) = \\ln\\left(\\frac{\\pi({\\bf x}_i)}{1-\\pi({\\bf x}_i)}\\right) = (\\theta_0 + \\theta_2) + (\\theta_1 - \\theta_2) \\mathbb{1}_{x_i =1} + 0\\ \\mathbb{1}_{x_i =0}. \\] On se retrouve comme pour l’analyse de variance avec un modèle non identifiable. Il faut donc imposer une contrainte sur les paramètres pour le rendre identifiable. Par exemple si on suppose que \\(\\theta_2=0\\), le modèle devient \\[\\textrm{logit}(\\pi({\\bf x}_i)) = \\theta_0 + \\theta_1 \\mathbb{1}_{x_i=1}.\\] Il faut donc adapter l’interprétation des paramètres selon la contrainte considérée. On peut alors reprendre les mêmes raisonnements et les mêmes calculs que dans la section 10.4.1 pour estimer les paramètres, construire un intervalle de confiance pour chacun des paramètres, tester la nullité de chacun des paramètres, etc. Dans notre exemple traité sous R, le modèle considéré est celui avec la contrainte \\(\\theta_2=0\\) : table(default,student) student default No Yes No 6850 2817 Yes 206 127 glm.student = glm(default~student, data=Default, family=binomial) summary(glm.student) Call: glm(formula = default ~ student, family = binomial, data = Default) Deviance Residuals: Min 1Q Median 3Q Max -0.2970 -0.2970 -0.2434 -0.2434 2.6585 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -3.50413 0.07071 -49.55 &lt; 2e-16 *** studentYes 0.40489 0.11502 3.52 0.000431 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 2920.6 on 9999 degrees of freedom Residual deviance: 2908.7 on 9998 degrees of freedom AIC: 2912.7 Number of Fisher Scoring iterations: 6 Pour l’interprétation des paramètres, on s’appuie sur les odds et odds ratio. On a que \\(odds(x_i=0)=e^{\\theta_0}\\), \\(odds(x_i=1)=e^{\\theta_0 + \\theta_1}\\) et \\(OR(x_i=1,x_i=0) = e^{\\theta_1}\\). new.data=data.frame(student=factor(c(&quot;No&quot;,&quot;Yes&quot;))) inv.logit(predict(glm.student,new.data)) # nécessite la librairie boot 1 2 0.02919501 0.04313859 exp(c(coef(glm.student),sum=sum(coef(glm.student)))) (Intercept) studentYes sum 0.03007299 1.49913321 0.04508342 ::: Si on n’est pas étudiant, \\(\\textrm{logit}(\\hat \\pi) = \\hat \\theta_0\\) donc \\(\\hat \\pi = 0.029\\) alors que si l’on est étudiant, \\(\\textrm{logit}(\\hat\\pi) = \\hat \\theta_0 + \\hat \\theta_1\\) d’où \\(\\hat \\pi = 0.043\\). Ainsi, \\[ \\left\\{\\begin{array}{l} \\mbox{odds}(\\mbox{&quot;étudiant&quot;}) = 0.045, \\\\ \\mbox{odds}(\\mbox{&quot;non étudiant&quot;}) = 0.030, \\\\ \\mbox{OR}(\\mbox{&quot;étudiant&quot;},\\mbox{&quot;non étudiant&quot;}) = 1.5. \\end{array}\\right. \\] Un étudiant a 1.5 fois plus de chance d’être en défaut qu’une personne non étudiante. Pour tester la pertinence d’utiliser la variable student, on peut également faire un test de sous-modèle : anova(glm(default~1, data=Default, family=binomial), glm.student, test=&quot;Chisq&quot;) Analysis of Deviance Table Model 1: default ~ 1 Model 2: default ~ student Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) 1 9999 2920.7 2 9998 2908.7 1 11.967 0.0005416 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 On en déduit que le modèle prenant en compte la variable student est meilleur que le modèle nul. 10.5 Régression logistique multiple Dans cette section, nous allons considérer le cas plus général où l’on souhaite expliquer la variable réponse binaire \\(Y\\) par rapport à \\(p\\) régresseurs \\(x^{(1)},\\ldots,x^{(p)}\\). Dans l’exemple, on a \\(p=3\\) régresseurs, une variable qualitative (student = \\(x^{(1)}\\)) et deux variables quantitatives (balance = \\(x^{(2)}\\), income=\\(x^{(3)}\\)). Nous allons aborder plusieurs questions au travers de l’étude de cet exemple. 10.5.1 Modèle sans interaction Dans un premier temps, on considère le modèle complet sans interaction suivant : \\[ \\textrm{logit}(\\pi({\\bf x}_i)) = \\theta_0 + \\theta_1\\ \\mathbb{1}_{x^{(1)}_i = 1} + \\theta_2\\ x^{(2)}_i + \\theta_3\\ x^{(3)}_i. \\] Le code suivant sous R permet d’ajuster ce modèle sans interaction : glm.additif&lt;-glm(default~.,data=Default,family=binomial(link=&quot;logit&quot;)) summary(glm.additif) Call: glm(formula = default ~ ., family = binomial(link = &quot;logit&quot;), data = Default) Deviance Residuals: Min 1Q Median 3Q Max -2.4691 -0.1418 -0.0557 -0.0203 3.7383 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -1.087e+01 4.923e-01 -22.080 &lt; 2e-16 *** studentYes -6.468e-01 2.363e-01 -2.738 0.00619 ** balance 5.737e-03 2.319e-04 24.738 &lt; 2e-16 *** income 3.033e-06 8.203e-06 0.370 0.71152 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 2920.6 on 9999 degrees of freedom Residual deviance: 1571.5 on 9996 degrees of freedom AIC: 1579.5 Number of Fisher Scoring iterations: 8 On obtient les estimations pour chacun des paramètres. On constate que si on teste la nullité individuellement de chaque paramètre par le test de Wald ou le \\(Z\\)-test, on rejette la nullité de \\(\\theta_0, \\theta_1, \\theta_2\\) et on accepte la nullité de \\(\\theta_3\\). 10.5.1.1 Test de nullité de plusieurs coefficients simultanément. Testons la nullité de \\(\\theta_2\\) et \\(\\theta_3\\) simultanément. On peut le voir comme un test de sous-modèle comparant le modèle complet sans interaction avec le modèle (introduit en section 10.4.2) \\[ \\textrm{logit}(\\pi({\\bf x}_i)) = \\theta_0 + \\theta_1 \\mathbb{1}_{x^{(1)}_i = 1} . \\] anova(glm.student,glm.additif,test=&quot;Chisq&quot;) Analysis of Deviance Table Model 1: default ~ student Model 2: default ~ student + balance + income Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) 1 9998 2908.7 2 9996 1571.5 2 1337.1 &lt; 2.2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 On rejette la nullité simultanée de ces deux paramètres. On peut aussi voir l’hypothèse nulle sous la forme \\[ \\mathcal{H}_0: C\\theta = 0_2 \\textrm{ avec } C=\\left(\\begin{array}{c c c c} 0 &amp; 0 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 1\\end{array}\\right). \\] On peut alors reprendre la construction du test de Wald présenté en section 9.5.1.2. On contrôle ici numériquement les quantités considérées dans ce test pour notre exemple hattheta &lt;- glm.additif$coefficients hatpi &lt;- glm.additif$fitted.values W &lt;- diag(hatpi*(1-hatpi)) X &lt;- cbind(rep(1,10000),student,balance,income) In &lt;- t(X) %*% W %*% X C &lt;- matrix(c(0,0,1,0,0,0,0,1),nrow=4) t(t(C)%*%hattheta) %*% solve(t(C)%*%solve(In)%*%C) %*% (t(C)%*%hattheta) &gt; qchisq(0.95,df=2) [,1] [1,] TRUE On rejette l’hypothèse nulle. 10.5.1.2 Sélection de variables Plus généralement, on peut envisager une procédure de sélection de variables. On choisit ici de faire une sélection de variable descendante avec le critère AIC : step.backward &lt;- step(glm.additif) Start: AIC=1579.54 default ~ student + balance + income Df Deviance AIC - income 1 1571.7 1577.7 &lt;none&gt; 1571.5 1579.5 - student 1 1579.0 1585.0 - balance 1 2907.5 2913.5 Step: AIC=1577.68 default ~ student + balance Df Deviance AIC &lt;none&gt; 1571.7 1577.7 - student 1 1596.5 1600.5 - balance 1 2908.7 2912.7 On peut également utiliser la fonction stepAIC() de la librairie MASS avec le critère AIC (option “p=2”) ou BIC (option \"p=log(n)) library(MASS) stepAIC(glm.additif, direction=c(&quot;backward&quot;),p=2) # AIC stepAIC(glm.additif, direction=c(&quot;backward&quot;),p=log(nrow(Default))) # BIC Pour les trois procédures, le modèle sélectionné est également celui sans la variable income. 10.5.2 Modèle avec interactions On va considérer ici le modèle complet avec toutes les interactions (d’ordre 2) entre variables et on met en place une procédure de sélection de variables pour déterminer un modèle plus simple pour expliquer la variable réponse default. Le modèle s’écrit : \\[ \\textrm{logit}(\\pi({\\bf x}_i)) = \\theta_0 + \\theta_2 x^{(2)}_i + \\theta_3 x^{(3)}_i + \\theta_{23} x^{(2)}_i x^{(3)}_i+ (\\beta_1 + \\beta_2 x^{(2)}_i + \\beta_3 x^{(3)}_i ) \\mathbb{1}_{x^{(1)}_i = 1}. \\] On commence par ajuster le modèle complet avec interactions avec le code suivant : glm.complet&lt;-glm(default~.^2,data=Default,family=&quot;binomial&quot;) summary(glm.complet) Call: glm(formula = default ~ .^2, family = &quot;binomial&quot;, data = Default) Deviance Residuals: Min 1Q Median 3Q Max -2.4848 -0.1417 -0.0554 -0.0202 3.7579 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -1.104e+01 1.866e+00 -5.914 3.33e-09 *** studentYes -5.201e-01 1.344e+00 -0.387 0.699 balance 5.882e-03 1.180e-03 4.983 6.27e-07 *** income 4.050e-06 4.459e-05 0.091 0.928 studentYes:balance -2.551e-04 7.905e-04 -0.323 0.747 studentYes:income 1.447e-05 2.779e-05 0.521 0.602 balance:income -1.579e-09 2.815e-08 -0.056 0.955 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 2920.6 on 9999 degrees of freedom Residual deviance: 1571.1 on 9993 degrees of freedom AIC: 1585.1 Number of Fisher Scoring iterations: 8 On peut remarquer que le test de nullité de chaque paramètre individuellement est accepté pour plusieurs des paramètres. On poursuit notre étude en cherchant à simplifier le modèle par sélection de variable. On applique une procédure de sélection de variable avec le critère AIC : step(glm.complet) Start: AIC=1585.07 default ~ (student + balance + income)^2 Df Deviance AIC - balance:income 1 1571.1 1583.1 - student:balance 1 1571.2 1583.2 - student:income 1 1571.3 1583.3 &lt;none&gt; 1571.1 1585.1 Step: AIC=1583.07 default ~ student + balance + income + student:balance + student:income Df Deviance AIC - student:balance 1 1571.3 1581.3 - student:income 1 1571.3 1581.3 &lt;none&gt; 1571.1 1583.1 Step: AIC=1581.28 default ~ student + balance + income + student:income Df Deviance AIC - student:income 1 1571.5 1579.5 &lt;none&gt; 1571.3 1581.3 - balance 1 2907.3 2915.3 Step: AIC=1579.54 default ~ student + balance + income Df Deviance AIC - income 1 1571.7 1577.7 &lt;none&gt; 1571.5 1579.5 - student 1 1579.0 1585.0 - balance 1 2907.5 2913.5 Step: AIC=1577.68 default ~ student + balance Df Deviance AIC &lt;none&gt; 1571.7 1577.7 - student 1 1596.5 1600.5 - balance 1 2908.7 2912.7 Call: glm(formula = default ~ student + balance, family = &quot;binomial&quot;, data = Default) Coefficients: (Intercept) studentYes balance -10.749496 -0.714878 0.005738 Degrees of Freedom: 9999 Total (i.e. Null); 9997 Residual Null Deviance: 2921 Residual Deviance: 1572 AIC: 1578 Une fois encore c’est le modèle additif avec seulement les variables student et balance qui est retenu. 10.5.3 Etude complémentaire du modèle retenu On commence par ajuster le modèle retenu \\[ \\textrm{logit}(\\pi({\\bf x}_i)) = \\theta_0 + \\theta_1 \\mathbb{1}_{x^{(1)}_i = 1} + \\theta_2 x^{(2)}_i \\] à l’aide du code suivant glm.final = glm(default ~ student + balance, data=Default, family=binomial) summary(glm.final) Call: glm(formula = default ~ student + balance, family = binomial, data = Default) Deviance Residuals: Min 1Q Median 3Q Max -2.4578 -0.1422 -0.0559 -0.0203 3.7435 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -1.075e+01 3.692e-01 -29.116 &lt; 2e-16 *** studentYes -7.149e-01 1.475e-01 -4.846 1.26e-06 *** balance 5.738e-03 2.318e-04 24.750 &lt; 2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 2920.6 on 9999 degrees of freedom Residual deviance: 1571.7 on 9997 degrees of freedom AIC: 1577.7 Number of Fisher Scoring iterations: 8 On peut vérifier graphiquement la non-interaction entre les variables student et balance. Sur la Figure 10.5, on constate que les droites représentant \\(\\textrm{logit}(\\pi_\\theta(\\cdot))\\) en fonction de la variable balance en distinguant selon la valeur de la variable student sont parallèles. Figure 10.5: Tracé de pi estimé (à gauche) et de la fonction logit appmliquée à pi estimé (à droite) en fonction de la variable balance en distinguant selon la valeur de la variable student (student = 1 en ligne pleine et = 0 en pointillé) Afin de comparer les valeurs de la variable réponse et les valeurs prédites par le modèle, on forme la table de contingence : hatpi &lt;- glm.final$fitted.values table(default,hatpi&gt;0.5) default FALSE TRUE No 9628 39 Yes 228 105 On constate que l’on retrouve assez bien les clients sans défaut sur leur dette, par contre la détection des clients étant en défaut sur leur dette sont très mal prédits. Concernant les résidus du modèle ajusté, on peut calculer les différents résidus introduits dans le chapitre général (section 9.8). library(boot) #residus y_i - \\hat\\mu_i res&lt;-residuals(glm.final,type=&quot;response&quot;) #residus de deviance res_dev&lt;-residuals(glm.final) #residus de Pearson res_pear&lt;-residuals(glm.final,type=&quot;pearson&quot;) #residus de deviance standardisés res_dev_stand&lt;-rstandard(glm.final) res_dev_stand&lt;-glm.diag(glm.final)$rd # residus de Pearson standardisés H&lt;-influence(glm.final)$hat res_pear_stand&lt;-res_pear/sqrt(1-H) res_pear_stand&lt;-glm.diag(glm.final)$rp # residus de Jackknife res_Jackknife&lt;-glm.diag(glm.final)$res 10.6 Quelques codes avec python import pandas as pd import numpy as np import statsmodels.api as sm Defaultpy=r.Default y=Defaultpy[&quot;default&quot;].cat.codes x=Defaultpy[&quot;balance&quot;] x_stat = sm.add_constant(x) modelbalance = sm.Logit(y, x_stat).fit() Optimization terminated successfully. Current function value: 0.079823 Iterations 10 modelbalance.summary() &lt;class &#39;statsmodels.iolib.summary.Summary&#39;&gt; &quot;&quot;&quot; Logit Regression Results ============================================================================== Dep. Variable: y No. Observations: 10000 Model: Logit Df Residuals: 9998 Method: MLE Df Model: 1 Date: Jeu, 28 oct 2021 Pseudo R-squ.: 0.4534 Time: 12:20:44 Log-Likelihood: -798.23 converged: True LL-Null: -1460.3 Covariance Type: nonrobust LLR p-value: 6.233e-290 ============================================================================== coef std err z P&gt;|z| [0.025 0.975] ------------------------------------------------------------------------------ const -10.6513 0.361 -29.491 0.000 -11.359 -9.943 balance 0.0055 0.000 24.952 0.000 0.005 0.006 ============================================================================== Possibly complete quasi-separation: A fraction 0.13 of observations can be perfectly predicted. This might indicate that there is complete quasi-separation. In this case some parameters will not be identified. &quot;&quot;&quot; ci = modelbalance.conf_int(0.05) print(ci) 0 1 const -11.359208 -9.943453 balance 0.005067 0.005931 modelbalance.summary() &lt;class &#39;statsmodels.iolib.summary.Summary&#39;&gt; &quot;&quot;&quot; Logit Regression Results ============================================================================== Dep. Variable: y No. Observations: 10000 Model: Logit Df Residuals: 9998 Method: MLE Df Model: 1 Date: Jeu, 28 oct 2021 Pseudo R-squ.: 0.4534 Time: 12:20:46 Log-Likelihood: -798.23 converged: True LL-Null: -1460.3 Covariance Type: nonrobust LLR p-value: 6.233e-290 ============================================================================== coef std err z P&gt;|z| [0.025 0.975] ------------------------------------------------------------------------------ const -10.6513 0.361 -29.491 0.000 -11.359 -9.943 balance 0.0055 0.000 24.952 0.000 0.005 0.006 ============================================================================== Possibly complete quasi-separation: A fraction 0.13 of observations can be perfectly predicted. This might indicate that there is complete quasi-separation. In this case some parameters will not be identified. &quot;&quot;&quot; from scipy.stats import chi2 LR_stat = (-2)* (modelbalance.llnull - modelbalance.llf); df = 1 pvalue = 1 - chi2(df).cdf(LR_stat); print(LR_stat) 1324.1980279638472 print(pvalue) 0.0 y=Defaultpy[&quot;default&quot;].cat.codes x=Defaultpy[&quot;student&quot;].cat.codes x_stat = sm.add_constant(x) modelstudent = sm.Logit(y, x_stat).fit(); Optimization terminated successfully. Current function value: 0.145434 Iterations 7 modelstudent.summary() &lt;class &#39;statsmodels.iolib.summary.Summary&#39;&gt; &quot;&quot;&quot; Logit Regression Results ============================================================================== Dep. Variable: y No. Observations: 10000 Model: Logit Df Residuals: 9998 Method: MLE Df Model: 1 Date: Jeu, 28 oct 2021 Pseudo R-squ.: 0.004097 Time: 12:20:48 Log-Likelihood: -1454.3 converged: True LL-Null: -1460.3 Covariance Type: nonrobust LLR p-value: 0.0005416 ============================================================================== coef std err z P&gt;|z| [0.025 0.975] ------------------------------------------------------------------------------ const -3.5041 0.071 -49.554 0.000 -3.643 -3.366 0 0.4049 0.115 3.520 0.000 0.179 0.630 ============================================================================== &quot;&quot;&quot; Defaultpy=r.DefaultBIS y=Defaultpy[&quot;default&quot;] x=Defaultpy[Defaultpy.columns.drop(&quot;default&quot;)] x_stat = sm.add_constant(x) modeladditif = sm.Logit(y, x_stat).fit() Optimization terminated successfully. Current function value: 0.078577 Iterations 10 modeladditif.summary() &lt;class &#39;statsmodels.iolib.summary.Summary&#39;&gt; &quot;&quot;&quot; Logit Regression Results ============================================================================== Dep. Variable: default No. Observations: 10000 Model: Logit Df Residuals: 9996 Method: MLE Df Model: 3 Date: Jeu, 28 oct 2021 Pseudo R-squ.: 0.4619 Time: 12:20:50 Log-Likelihood: -785.77 converged: True LL-Null: -1460.3 Covariance Type: nonrobust LLR p-value: 3.257e-292 ============================================================================== coef std err z P&gt;|z| [0.025 0.975] ------------------------------------------------------------------------------ const -10.8690 0.492 -22.079 0.000 -11.834 -9.904 student -0.6468 0.236 -2.738 0.006 -1.110 -0.184 balance 0.0057 0.000 24.737 0.000 0.005 0.006 income 3.033e-06 8.2e-06 0.370 0.712 -1.3e-05 1.91e-05 ============================================================================== Possibly complete quasi-separation: A fraction 0.15 of observations can be perfectly predicted. This might indicate that there is complete quasi-separation. In this case some parameters will not be identified. &quot;&quot;&quot; 10.7 Régression polytomique On souhaite étendre la régression logistique au cas de l’étude d’une variable réponse qualitative \\(Y\\) pouvant prendre \\(M\\) modalités \\(u_1,\\ldots,u_M\\) avec \\(M&gt;2\\). On note \\(x^{(1)},\\ldots,x^{(p)}\\) les variables explicatives. 10.7.1 Régression multinomiale ou polytomique non-ordonnée Dans cette section, on considère l’exemple suivant. Example 10.3 On considère \\(n=735\\) clients. On souhaite étudier pour un produit le choix du client entre la marque à petit prix, la marque de l’enseigne ou la marque de référence du marché. On souhaite savoir si l’âge et/ou le genre du client a une influence sur son choix. D &lt;- read.table(&quot;Data/MarqueSexe.csv&quot;,header=T,sep=&quot;;&quot;) D[,&quot;femme&quot;] &lt;- as.factor(D[,&quot;femme&quot;]) D[,1] &lt;- factor(D[,1], levels =c(&quot;Reference&quot;,&quot;Enseigne&quot;,&quot;PetitPrix&quot;)) summary(D) brand femme age Reference:221 0:269 Min. :24.0 Enseigne :307 1:466 1st Qu.:32.0 PetitPrix:207 Median :32.0 Mean :32.9 3rd Qu.:34.0 Max. :38.0 La variable réponse \\(Y\\) est ici nominale, c’est-à-dire que les \\(M\\) modalités n’ont pas de lien hiérarchique, pas d’ordre. On parle alors de régression multinomiale (ou polytomique non-ordonnée). Comme dans le cas binaire, on cherche à modéliser \\[ \\pi_m(\\textbf{x})=\\mathbb{P}(Y=u_m|\\textbf{x}),\\, \\forall m\\in\\{1,\\ldots,M\\}. \\] Comme \\(\\sum_{m=1}^M \\pi_m(\\textbf{x})=1\\), il suffit de modéliser \\((M-1)\\) des \\(\\pi_m(\\textbf{x})\\). La catégorie de référence s’impose souvent par le contexte d’étude : les non-malades contre les différents types de maladie; le produit phare du marché contre les produits outsiders, etc. Dans notre exemple, la modalité “Référence” sera considérée comme la modalité de référence. Dans la suite, nous considérons la première modalité comme référence. On définit alors le modèle de régression multinomiale par \\[ \\ln\\left[\\frac{\\pi_m(\\textbf{x})}{\\pi_1(\\textbf{x})}\\right] = \\theta_0^{(m)} + \\theta_1^{(m)} x^{(1)} + \\ldots + \\theta_p^{(m)} x^{(p)} = \\textbf{x} \\theta^{(m)},\\, \\forall m\\in\\{2,\\ldots,M\\} \\] avec \\(\\textbf{x}=(1,x^{(1)},\\ldots,x^{(p)})\\) et \\(\\theta^{(m)} = (\\theta_0^{(m)},\\theta_1^{(m)} ,\\ldots,\\theta_p^{(m)})&#39;\\) paramètres inconnus. Ceci revient à \\[ \\pi_m({\\bf x})= \\frac{\\exp(\\textbf{x}\\theta^{(m)})}{ 1 + \\sum_{m&#39;=2}^M \\exp(\\textbf{x} \\theta^{(m&#39;)})}. \\] On peut remarquer que pour \\(M=2\\), \\(u_1=0\\) et \\(u_2=1\\), on retrouve le modèle de régression logistique. Pour estimer les paramètres \\(\\theta^{(m)}\\) pour \\(m\\in\\{2,\\ldots,M\\}\\), on cherche à maximiser la vraisemblance du modèle \\[ L(\\underline{Y}|\\theta) = \\prod_{i=1}^n \\prod_{m=1}^M \\pi_{m}(\\textbf{x}_i)^{\\mathbb{1}_{Y_i=u_m}}. \\] On retrouve la vraisemblance de \\(n\\) lois multinomiales de paramètres \\((1,(\\pi_1(\\textbf{x}_i),\\ldots,\\pi_M(\\textbf{x}_i))\\) pour \\(1\\leq i\\leq n\\). Comme pour le cas binaire, il n’y a pas de solutions explicites pour les estimateurs, on utilise donc des méthodes numériques pour les évaluer. A partir des estimateurs \\(\\hat\\theta^{(1)},\\ldots,\\hat\\theta^{(M)}\\), on en déduit un estimateur pour chaque \\(\\pi_m({\\bf x})\\) : \\[ \\left\\{\\begin{array}{l} \\displaystyle \\hat \\pi_m(\\textbf{x}) = \\frac{\\exp(\\textbf{x} \\hat\\theta^{(m)})}{ 1 + \\sum_{m&#39;=2}^M \\exp(\\textbf{x} \\hat\\theta^{(m&#39;)})}, \\forall m\\in\\{2,\\ldots,M\\} \\\\ \\displaystyle \\hat \\pi_{1}(\\textbf{x}) = 1 - \\sum_{m=2}^M \\hat \\pi_m(\\textbf{x}). \\end{array}\\right. \\] On peut ensuite s’intéresser à la prédiction. Sachant qu’un individu prend les valeurs \\(\\textbf{x}_0=(1,x_0^{(1)},\\ldots, x_0^{(p)})\\) on peut faire de la prédiction : sur la probabilité que \\(Y_0=u_m\\) pour \\(m\\in\\{1,\\ldots,M\\}\\) : \\(\\hat \\pi_m(\\textbf{x}_0)\\). sur la modalité de \\(Y\\) la plus probable pour \\(\\textbf{x}_0\\) \\[ \\hat Y_0 = u_{\\hat m_0} \\textrm{ avec } \\hat m_0 = \\underset{m\\in\\{1,\\ldots,M\\}}{\\mbox{argmax}} \\hat \\pi_m(\\textbf{x}_0). \\] Sous des conditions similaires au cas binaire, on récupère les mêmes résultats sur le comportement asymptotique des estimateurs. On peut alors utiliser de la même façon les tests de Wald, du rapport de maximum de vraisemblance, etc. Pour interpréter les paramètres, on peut utiliser les odds ratio. On définit l’odds d’une modalité \\(m_1\\) contre une modalité \\(m_2\\) par \\[ \\textrm{odds}(Y=u_{m_1} \\mbox{vs }Y=u_{m_2} ; {\\bf x}) = \\frac{\\mathbb{P}(Y=u_{m_1} | {\\bf x}) }{\\mathbb{P}(Y=u_{m_2} | {\\bf x}) } = \\frac{\\pi_{m_1}({\\bf x})}{\\pi_{m_2}({\\bf x})} = \\exp[{\\bf x} (\\theta^{(m_1)} - \\theta^{(m_2)})]. \\] Pour deux individus \\(x\\) et \\(\\tilde{x}\\), on définit alors l’odds ratio par \\[\\begin{eqnarray*} \\textrm{OR}(Y=u_{m_1} \\mbox{vs }Y=u_{m_2} ; {\\bf x},\\tilde{\\bf x}) &amp;=&amp; \\frac{\\textrm{odds}(Y=u_{m_1} \\mbox{vs }Y=u_{m_2} ; {\\bf x})}{\\textrm{odds}(Y=u_{m_1} \\mbox{vs }Y=u_{m_2} ; \\tilde{\\bf x})} \\\\ &amp;=&amp; \\exp[({\\bf x} - \\tilde{\\bf x}) (\\theta^{(m_1)} - \\theta^{(m_2)})]. \\end{eqnarray*}\\] Ainsi si les deux individus \\({\\bf x}\\) et \\(\\tilde{\\bf x}\\) ne diffèrent que d’une unité pour la variable \\(j\\), on a \\[ \\textrm{OR}(Y=u_{m_1} \\mbox{vs }Y=u_{m_2} ; {\\bf x},\\tilde{\\bf x}) = \\exp[\\theta_j^{(m_1)} - \\theta_j^{(m_2)}]. \\] En pratique, on peut utiliser les fonctions multinom() ou vglm() des librairires nnet et VGAM pour ajuster un modèle de régression polytomique non-ordonnée sous R. Sur notre exemple, une régression multinomiale est mise en oeuvre avec la fonction multinom() : library(nnet) regMarq &lt;- multinom(brand ~ femme + age,data=D,Hess=T) # weights: 12 (6 variable) initial value 807.480032 iter 10 value 702.971567 final value 702.970704 converged On affiche les estimations obtenues pour les paramètres \\(\\theta^{(m)}\\) pour \\(m=2,3\\) : summary(regMarq) Call: multinom(formula = brand ~ femme + age, data = D, Hess = T) Coefficients: (Intercept) femme1 age Enseigne 10.94688 0.05798805 -0.3177081 PetitPrix 22.72150 -0.46576724 -0.6859142 Std. Errors: (Intercept) femme1 age Enseigne 1.493166 0.1964261 0.04400704 PetitPrix 2.058030 0.2260886 0.06262666 Residual Deviance: 1405.941 AIC: 1417.941 Par exemple, on a \\(\\ln [\\pi_{\\tt{\\tiny petit prix}} / \\pi_{\\tt{\\tiny Reference}} ] = 22.72 - 0.47 \\mathbb{1}_{Femme=1} -0.69 {\\tt age}\\). Les femmes ont moins confiance dans la marque petit prix par rapport à la marque de référence et plus l’âge augmente, moins le client fait le choix de la marque petit prix par rapport à la marque de référence. On peut alors calculer les estimations pour les \\(\\pi_m(\\textbf{x}_i)\\). Pour chaque individu du jeu de données, on récupère les probabilités avec regMarq$fitted.values : head(regMarq$fitted.values) Reference Enseigne PetitPrix 1 0.001812569 0.05023105 0.9479564 2 0.006741608 0.09896542 0.8942930 3 0.006741608 0.09896542 0.8942930 4 0.018431742 0.20868509 0.7728832 5 0.018431742 0.20868509 0.7728832 6 0.012740144 0.13611798 0.8511419 Ainsi pour le premier client, il fera plutôt le choix de la marque petit prix. On obtient les prédictions avec apply(regMarq$fitted.values,1,which.max) ou directement avec la commande pr = predict(regMarq, D). La matrice de confusion vaut alors pr = predict(regMarq, D) table(D[,1], pr) pr Reference Enseigne PetitPrix Reference 110 101 10 Enseigne 51 238 18 PetitPrix 13 136 58 On peut déterminer un intervalle de confiance pour chacun des paramètres avec la commande : confint(regMarq) , , Enseigne 2.5 % 97.5 % (Intercept) 8.0203294 13.8734340 femme1 -0.3270001 0.4429762 age -0.4039603 -0.2314558 , , PetitPrix 2.5 % 97.5 % (Intercept) 18.6878390 26.75516759 femme1 -0.9088928 -0.02264168 age -0.8086602 -0.56316817 Pour tester la nullité de chaque coefficient \\(\\theta_j^{(m)}=0\\) contre \\(\\theta_j^{(m)}\\neq 0\\), on fait un test de Wald (ou Z-test). On obtient les \\(p\\)-valeurs de chaque test avec les commandes suivantes : z = summary(regMarq)$coeff / summary(regMarq)$standard.errors pvaleur = 2 * (1 - pnorm(abs(z), 0, 1)) pvaleur (Intercept) femme1 age Enseigne 2.278178e-13 0.76782920 5.218048e-13 PetitPrix 0.000000e+00 0.03938811 0.000000e+00 On peut aussi utiliser la fonction coeftest() de la librairie AER : library(AER) coeftest(regMarq) z test of coefficients: Estimate Std. Error z value Pr(&gt;|z|) Enseigne:(Intercept) 10.946882 1.493166 7.3313 2.279e-13 *** Enseigne:femme1 0.057988 0.196426 0.2952 0.76783 Enseigne:age -0.317708 0.044007 -7.2195 5.219e-13 *** PetitPrix:(Intercept) 22.721503 2.058030 11.0404 &lt; 2.2e-16 *** PetitPrix:femme1 -0.465767 0.226089 -2.0601 0.03939 * PetitPrix:age -0.685914 0.062627 -10.9524 &lt; 2.2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 On peut remarquer que les variances de chaque coefficient sont données sur la diagonale de l’inverse de la matrice hessienne. Par exemple, pour “femme-enseigne” on retrouve bien l’écart-type \\(0.1964\\) : Sigma &lt;- solve(regMarq$Hessian) sqrt(Sigma[2,2]) [1] 0.1964261 Pour tester le modèle considéré contre le sous-modèle trivial (seulement l’intercept \\(\\theta_0^{(m)}\\)), on considère le test du rapport de vraisemblance : regMarq0 = multinom(brand ~1,data=D) # weights: 6 (2 variable) initial value 807.480032 final value 795.895819 converged rv = regMarq0$deviance - regMarq$deviance ddl = regMarq$edf - regMarq0$edf pvaleur = 1 - pchisq(rv, ddl) print(c(rv,ddl,pvaleur)) [1] 185.8502 4.0000 0.0000 La différence des déviances vaut \\(185.85\\). La statistique de test suit une loi du chi-deux à \\([n-(M-1)] - [n-(p+1)(M-1)] = p(M-1) = 4\\) degrés de liberté. La \\(p\\)-valeur étant très faible, on rejette l’hypothèse nulle. On veut maintenant tester si la variable sexe joue un rôle dans le choix des clients. On va donc tester si $_0 : _1^{(2)} = _1^{(3)} =0 $ contre \\(\\mathcal{H}_1: \\exists m; \\theta_1^{(m)}\\neq 0\\). La statistique du test \\((\\hat\\theta_1^{(2)},\\hat\\theta_1^{(3)}) \\hat \\Sigma_1^{-1} (\\hat\\theta_1^{(2)},\\hat\\theta_1^{(3)})&#39;\\) suit une loi du chi-deux à \\(2\\) degrés de liberté, où \\(\\hat \\Sigma_1\\) est la matrice de variance-covariance pour la variable “femme”. On peut aussi le voir comme un test de sous-modèle avec le test du rapport de vraisemblance. # TEST 1 regMarq1 = multinom(brand~age,data=D) # weights: 9 (4 variable) initial value 807.480032 final value 706.796304 converged rv1 = regMarq1$deviance - regMarq$deviance ddl = regMarq$edf - regMarq1$edf pvaleur = 1 - pchisq(rv1, ddl) print(c(rv1,ddl,pvaleur)) [1] 7.65119936 2.00000000 0.02180536 # TEST 2 thetafemme &lt;- summary(regMarq)$coeff[,2] Sigmafemme &lt;- Sigma[c(2,5),c(2,5)] Wfemme &lt;- thetafemme %*% solve(Sigmafemme) %*% thetafemme Wfemme &gt; qchisq(0.95,2) # test à 5% [,1] [1,] TRUE Comme la \\(p\\)-valeur vaut \\(0.0218\\), on rejette l’absence d’effet de la variable sexe à \\(5\\%\\). Pour interpréter les coefficients, on peut revenir aux odds ratios. Par exemple si on regarde l’odds de petits prix et marque vs la référence pour une femme avec même âge, on peut les calculer avec \\(\\exp(\\theta_1^{(m)})\\) : exp(summary(regMarq)$coeff[,2]) Enseigne PetitPrix 1.0597023 0.6276534 Une femme a 0.628 fois plus de chances qu’un homme de préférer la marque petit prix à la marque référence. On peut faire le même travail avec la fonction vglm() de la librairie VGAM. Notons que cette fonction prend la dernière modalité comme référence. library(VGAM) D1 &lt;- D D1[,1] &lt;- factor(D[,1], levels=c(&quot;PetitPrix&quot;,&quot;Enseigne&quot;,&quot;Reference&quot;)) regMarq2 &lt;- vglm(brand ~ femme + age, data=D1, family=multinomial()) summary(regMarq2) Call: vglm(formula = brand ~ femme + age, family = multinomial(), data = D1) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept):1 22.72140 2.05802 11.040 &lt; 2e-16 *** (Intercept):2 10.94674 1.49316 7.331 2.28e-13 *** femme1:1 -0.46594 0.22609 -2.061 0.0393 * femme1:2 0.05787 0.19643 0.295 0.7683 age:1 -0.68591 0.06263 -10.952 &lt; 2e-16 *** age:2 -0.31770 0.04401 -7.219 5.22e-13 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Names of linear predictors: log(mu[,1]/mu[,3]), log(mu[,2]/mu[,3]) Residual deviance: 1405.941 on 1464 degrees of freedom Log-likelihood: -702.9707 on 1464 degrees of freedom Number of Fisher scoring iterations: 5 Warning: Hauck-Donner effect detected in the following estimate(s): &#39;age:1&#39; Reference group is level 3 of the response 10.7.2 Régression polytomique ordonnée Dans cette section, on suppose que \\(Y\\) est une variable qualitative ordinale, c’est-à-dire que \\(Y\\) admet \\(M\\) modalités ordonnées \\(u_1 \\prec u_2\\prec \\ldots \\prec u_M.\\) Par exemple, si on s’intéresse au degré de satisfaction pour un produit (mauvais, moyen, bon, très bon); pour le stade d’évolution d’une maladie; etc. On va illustrer cette section avec le jeu de données suivant : on s’intéresse à la qualité de 34 vins selon l’ensoleillement et la pluviométrie (les variables explicatives sont centrées réduites). SunRain &lt;- read.table(&quot;Data/SunRain.csv&quot;, header=T, sep=&quot;;&quot;) SunRain[,&quot;Quality&quot;] &lt;- factor(SunRain[,&quot;Quality&quot;], levels=c(&quot;bad&quot;,&quot;medium&quot;,&quot;good&quot;)) #centre-reduit les variables Sun et Rain SunRain[,&quot;Sun&quot;] &lt;- (SunRain[,&quot;Sun&quot;] - mean(SunRain[,&quot;Sun&quot;])) / sd(SunRain[,&quot;Sun&quot;]) SunRain[,&quot;Rain&quot;] &lt;- (SunRain[,&quot;Rain&quot;] - mean(SunRain[,&quot;Rain&quot;])) / sd(SunRain[,&quot;Rain&quot;]) attach(SunRain) Un résumé graphique des données est donné en Figure ??. Sous R, les fonctions polr() et vglm() des librairies MASS et VGAM permettent d’ajuster des modèles de régression polytomique ordonnée. 10.7.2.1 Modélisation par les logits cumulatifs Pour la modélisation, on suppose qu’il existe une variable latente \\(Z\\) telle que \\(Y\\) s’écrit à partir de \\(Z\\) sous la forme suivante \\[ Y = \\left\\{\\begin{array}{l l l } u_1 &amp; \\textrm{ si } &amp; Z\\in\\ ]a_0,a_1]\\\\ u_2 &amp; \\textrm{ si } &amp; Z\\in\\ ]a_1,a_2]\\\\ \\ldots\\\\ u_M &amp; \\textrm{ si } &amp; Z\\in\\ ]a_{M-1},a_M] \\end{array}\\right. \\] avec \\(-\\infty = a_0 &lt; a_1 &lt; \\ldots &lt; a_{M-1} &lt; a_M=+\\infty\\) (\\(M-1\\) coefficients inconnus), une liaison linéaire entre \\(Z\\) et les \\(x^{(1)},\\ldots,x^{(p)}\\) : \\[ Z = \\beta_1 x^{(1)} + \\ldots + \\beta_p x^{(p)} + \\gamma, \\] où \\((\\beta_1,\\ldots,\\beta_p)\\) sont des paramètres inconnus et \\(\\gamma\\) est une variable aléatoire symétrique de fonction de répartition \\(F_\\gamma\\). Le modèle de régression polytomique ordonnée revient alors à modéliser pour tout \\(m\\in\\{1,\\ldots,M-1\\}\\), \\[ \\mathbb{P}(Y \\leq u_m | {\\bf x}) = \\mathbb{P}(Z\\leq a_m | {\\bf x}) = F_\\gamma(a_m - [\\beta_1 x^{(1)} + \\ldots + \\beta_p x^{(p)}]) = F_\\gamma({\\bf x}\\theta^{(m)}), \\] avec \\({\\bf x}=(1,x^{(1)},\\ldots,x^{(p)})\\) et \\(\\theta^{(m)}=(a_m,-\\beta_1,\\ldots,-\\beta_p)&#39;\\). Comme dans le cadre binaire, il faut alors choisir une fonction de répartition \\(F_\\gamma\\). Si \\(\\gamma\\) est supposée suivre une loi logistique, on modélise les logits cumulatifs par \\[\\textrm{logit}\\left[ \\mathbb{P}(Y \\leq u_m | {\\bf x}) \\right] = {\\bf x}\\theta^{(m)} = \\theta_0^{(m)} + \\theta_1 x^{(1)} + \\ldots + \\theta_p x^{(p)},\\] avec \\[\\begin{eqnarray*} \\textrm{logit}\\left[ \\mathbb{P}(Y \\leq u_m | {\\bf x}) \\right] &amp;=&amp; \\ln\\left[\\frac{\\mathbb{P}(Y \\leq u_m | {\\bf x})}{\\mathbb{P}(Y &gt; u_m | {\\bf x})}\\right]\\\\ &amp;=&amp; \\ln\\left[\\frac{\\pi_1({\\bf x}) + \\ldots+\\pi_m({\\bf x})}{\\pi_{m+1}({\\bf x}) + \\ldots+\\pi_M({\\bf x})}\\right]. \\end{eqnarray*}\\] Dans ce modèle, les coefficients des variables explicatives sont identiques et seules les constantes (intercepts) diffèrent selon les modalités de \\(Y\\). Ainsi, quelque soit la modalité \\(u_m\\) considérée, une variable explicative donnée a la même influence sur \\(\\mathbb{P}(Y\\leq u_m|{\\bf x})\\). On dit qu’il y a égalité des pentes. Pour estimer les \\(M-1+p\\) paramètres, on cherche à maximiser la vraisemblance. library(VGAM) levels(SunRain[,&quot;Quality&quot;])&lt;-c(&quot;1&quot;,&quot;2&quot;,&quot;3&quot;) SunRain[,&quot;Quality&quot;]&lt;-as.numeric(SunRain[,&quot;Quality&quot;]) modelecumulsimpl &lt;- vglm(Quality ~ Sun + Rain, data=SunRain, family = cumulative(parallel=T,reverse=F)) modelecumulsimpl Call: vglm(formula = Quality ~ Sun + Rain, family = cumulative(parallel = T, reverse = F), data = SunRain) Coefficients: (Intercept):1 (Intercept):2 Sun Rain -1.420824 2.317790 -3.265814 1.588428 Degrees of Freedom: 68 Total; 64 Residual Residual deviance: 34.50584 Log-likelihood: -17.25292 On peut généraliser ce modèle en supposant que le rôle des variables dépend du niveau de la réponse, en posant \\[ \\textrm{logit}\\left[ \\mathbb{P}(Y \\leq u_m | {\\bf x}) \\right] = \\theta_0^{(m)} + \\theta_1^{(m)} x^{(1)} + \\ldots + \\theta_p^{(m)} x^{(p)} = {\\bf x} \\theta^{(m)} \\] avec \\(\\theta^{(m)}=(\\theta_0^{(m)} ,\\theta_1^{(m)} ,\\ldots, \\theta_p^{(m)})\\). On se retrouve donc avec un modèle à \\((M-1)(p+1)\\) paramètres (estimés par maximum de vraisemblance). modelecumul &lt;- vglm(Quality ~ Sun + Rain, data = SunRain, family = cumulative(parallel=F,reverse=F)) modelecumul Call: vglm(formula = Quality ~ Sun + Rain, family = cumulative(parallel = F, reverse = F), data = SunRain) Coefficients: (Intercept):1 (Intercept):2 Sun:1 Sun:2 Rain:1 -1.739086 2.005517 -4.020702 -2.814860 1.795255 Rain:2 1.326761 Degrees of Freedom: 68 Total; 62 Residual Residual deviance: 34.02694 Log-likelihood: -17.01347 Pour la suite, on va exploiter les résultats de la modélisation simplifiée pour les illustrations. Avec modelecumulsimpl@predictors, on récupère les valeurs des \\(\\textrm{logit}\\left[ \\mathbb{P}(Y \\leq u_m | {\\bf x}) \\right]\\). On peut facilement ensuite calculer les probabilités suivantes : \\[ \\left\\{ \\begin{array}{l} \\displaystyle \\mathbb{P}(Y \\leq u_m | {\\bf x}) = \\frac{\\displaystyle\\exp[{\\bf x} \\theta^{(m)}]}{\\displaystyle1 + \\exp[{\\bf x} \\theta^{(m)}]}\\\\ \\displaystyle \\mathbb{P}(Y = u_m | {\\bf x}) = \\mathbb{P}(Y \\leq u_m | {\\bf x}) - \\mathbb{P}(Y \\leq u_{m-1} | {\\bf x}) \\\\ \\displaystyle \\mathbb{P}(Y \\leq u_M | {\\bf x}) =1. \\end{array} \\right. \\] probacumul &lt;- exp(modelecumulsimpl@predictors)/(1+exp(modelecumulsimpl@predictors)) head(probacumul) logitlink(P[Y&lt;=1]) logitlink(P[Y&lt;=2]) 1 0.9608757 0.9990324 2 0.9994916 0.9999879 3 0.9989072 0.9999740 4 0.9088956 0.9976213 5 0.9995916 0.9999903 6 0.4872894 0.9755831 proba &lt;- cbind(probacumul[,1],probacumul[,2]-probacumul[,1],1-probacumul[,2]) head(proba) [,1] [,2] [,3] 1 0.9608757 0.0381566510 9.676064e-04 2 0.9994916 0.0004963458 1.210043e-05 3 0.9989072 0.0010668179 2.602321e-05 4 0.9088956 0.0887257897 2.378656e-03 5 0.9995916 0.0003986827 9.718527e-06 6 0.4872894 0.4882937635 2.441687e-02 Dans le cas où \\(Y\\) est une variable qualitative ordinale, on définit l’odds d’un individu \\({\\bf x}\\) relativement à \\(Y\\leq u_m\\) par \\[ \\textrm{odds}({\\bf x} | u_m) = \\frac{\\mathbb{P}(Y\\leq u_m | {\\bf x})}{1-\\mathbb{P}(Y\\leq u_m | {\\bf x})} = \\exp[{\\bf x} \\theta^{(m)}]. \\] L’odds ratio entre deux individus \\({\\bf x}\\) et \\(\\tilde{\\bf x}\\) relativement à \\(Y\\leq u_m\\) s’écrit alors \\[ \\textrm{OR}({\\bf x},\\tilde{\\bf x}| u_m) = \\frac{\\textrm{odds}({\\bf x}| u_m) }{\\textrm{odds}(\\tilde{\\bf x}| u_m) } = \\exp\\left[\\sum_{j=1}^p \\theta_j^{(m)} (x^{(j)} - \\tilde{x}^{(j)})\\right] . \\] On peut remarquer que cet odds ratio ne dépend pas de \\(\\theta_0^{(m)}\\). Aussi dans le cas de la modélisation avec pentes parallèles, cet odds ratio ne dépend pas de la modalité \\(u_m\\). En particulier, si \\({\\bf x}\\) et \\(\\tilde{\\bf x}\\) ne diffèrent que d’une unité pour seulement une variable \\(j\\), alors \\(\\textrm{OR}({\\bf x},\\tilde{\\bf x}| u_m) = \\exp[\\theta_j^{(m)}]\\). Dans notre exemple, lorsque l’on observe la valeur moyenne des variables explicatives (\\({\\bf x}=(1,0,0)\\) car les données sont centrées), on obtient dans notre exemple que \\(\\textrm{odds}({\\bf x} | &quot;{\\tt bad}&quot;) = e^{-1.420} = 0.24\\) : on a 0.24 fois plus de chance d’avoir un vin de qualité “bad” que d’une qualité meilleure. De même, \\(\\textrm{odds}({\\bf x} | &quot;{\\tt medium}&quot;) = e^{2.317} = 10.15\\) : on a 10.15 fois plus de chance d’avoir un vin de qualité inférieure à “medium” que “good”. Lorsque les jours d’ensoleillement augmentent de 1, on a \\(e^{-3.265814} = 0.04\\) fois plus de chances d’avoir un vin moins bon qu’il ne l’est. 10.7.2.2 Modélisation par les logits adjacents Il est également possible de définir la modélisation à partir des logits adjacents : \\[ \\left\\{ \\begin{array}{l} L_{M-1} = \\ln\\left[\\frac{\\pi_{M}({\\bf x})}{\\pi_{M-1}({\\bf x})}\\right] = \\theta_0^{(M-1)} + \\theta_1^{(M-1)} x^{(1)} + \\ldots + \\theta_p^{(M-1)} x^{(p)}\\\\ \\ldots\\\\ L_2 = \\ln\\left[\\frac{\\pi_3({\\bf x})}{\\pi_2({\\bf x})}\\right] = \\theta_0^{(2)} + \\theta_1^{(2)} x^{(1)} + \\ldots + \\theta_p^{(2)} x^{(p)}\\\\ \\\\ L_1 = \\ln\\left[\\frac{\\pi_2({\\bf x})}{\\pi_1({\\bf x})}\\right] = \\theta_0^{(1)} + \\theta_1^{(1)} x^{(1)} + \\ldots + \\theta_p^{(1)} x^{(p)}\\\\ \\end{array} \\right. \\] C’est la même idée que pour la régression multinomiale mais la catégorie de référence change à chaque étape. On peut relier les deux en remarquant que \\[ \\left\\{ \\begin{array}{l} \\ln\\left[\\frac{\\pi_2({\\bf x})}{\\pi_1({\\bf x})}\\right] = L_1 \\\\ \\\\ \\ln\\left[\\frac{\\pi_3({\\bf x})}{\\pi_1({\\bf x})}\\right] = L_2 + L_1\\\\ \\ldots\\\\ \\ln\\left[\\frac{\\pi_M ({\\bf x})}{\\pi_1({\\bf x})}\\right] = L_{M_1} + \\ldots + L_2 + L_1 \\end{array} \\right. \\] Comme précédemment, on peut considérer une modélisation simplifiée pour limiter le nombre de paramètres : \\[ \\ln\\left[\\frac{\\pi_{m+1}({\\bf x})}{\\pi_{m}({\\bf x})}\\right] = \\theta_0^{(m)} + \\theta_1 x^{(1)} + \\ldots + \\theta_p x^{(p)}. \\] Pour notre exemple, on ajuste le modèle “complet” et le modèle “simplifié” : modeleadj &lt;- vglm(Quality ~ Sun + Rain, data = SunRain, family = acat(parallel=F,reverse=F)) summary(modeleadj) Call: vglm(formula = Quality ~ Sun + Rain, family = acat(parallel = F, reverse = F), data = SunRain) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept):1 1.649 1.055 1.564 0.1179 (Intercept):2 -1.700 0.894 -1.901 0.0572 . Sun:1 4.110 2.010 NA NA Sun:2 2.504 1.141 2.195 0.0281 * Rain:1 -1.727 1.052 -1.641 0.1008 Rain:2 -1.185 1.036 -1.144 0.2526 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Names of linear predictors: loglink(P[Y=2]/P[Y=1]), loglink(P[Y=3]/P[Y=2]) Residual deviance: 33.6889 on 62 degrees of freedom Log-likelihood: -16.8445 on 62 degrees of freedom Number of Fisher scoring iterations: 7 Warning: Hauck-Donner effect detected in the following estimate(s): &#39;Sun:1&#39; modeleadjsimpl &lt;- vglm(Quality ~ Sun + Rain, data = SunRain, family = acat(parallel=T,reverse=F)) summary(modeleadjsimpl) Call: vglm(formula = Quality ~ Sun + Rain, family = acat(parallel = T, reverse = F), data = SunRain) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept):1 1.2810 0.7439 1.722 0.08506 . (Intercept):2 -2.0369 0.8481 -2.402 0.01632 * Sun 3.0711 0.9924 3.095 0.00197 ** Rain -1.4709 0.7037 -2.090 0.03658 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Names of linear predictors: loglink(P[Y=2]/P[Y=1]), loglink(P[Y=3]/P[Y=2]) Residual deviance: 34.3157 on 64 degrees of freedom Log-likelihood: -17.1578 on 64 degrees of freedom Number of Fisher scoring iterations: 7 No Hauck-Donner effect found in any of the estimates On définit l’odds d’une modalité \\(u_{m+1}\\) par rapport à \\(u_m\\) relativement à un individu \\({\\bf x}\\) par \\[ \\textrm{odds}(Y=u_{m+1} \\mbox{vs }Y=u_m ; {\\bf x}) = \\frac{\\mathbb{P}(Y=u_{m+1} | {\\bf x})}{\\mathbb{P}(Y=u_{m} | {\\bf x})} = \\frac{\\pi_{m+1}({\\bf x})}{\\pi_m({\\bf x})} = \\exp\\left[ {\\bf x} \\theta^{(m)} \\right]. \\] Pour deux individus \\({\\bf x}\\) et \\(\\tilde{\\bf x}\\), on définit alors l’odds ratio par \\[\\begin{eqnarray*} \\textrm{OR}(Y=u_{m+1} \\mbox{vs }Y=u_{m} ; {\\bf x},\\tilde{\\bf x}) &amp;=&amp; \\frac{\\textrm{odds}(Y=u_{m+1} \\mbox{vs }Y=u_{m} ; {\\bf x})}{\\textrm{odds}(Y=u_{m+1} \\mbox{vs }Y=u_{m} ; \\tilde{\\bf x})} \\\\ &amp;=&amp; \\exp\\left[ \\sum_{j=1}^p (x^{(j)} - \\tilde{x}^{(j)} ) \\theta_j^{(m)}\\right]. \\end{eqnarray*}\\] Ainsi si les deux individus \\({\\bf x}\\) et \\(\\tilde{\\bf x}\\) ne diffèrent que d’une unité pour la variable \\(j\\), on a \\[ \\textrm{OR}(Y=u_{m+1} \\mbox{vs }Y=u_{m} ; {\\bf x},\\tilde{\\bf x}) = \\exp[\\theta_j^{(m)}]. \\] Dans notre exemple, en prenant les résultats de la modélisation complète, on peut par exemple remarquer que si l’ensoleillement augmente d’une unité, on a \\(e^{4.11}= 60.9\\) fois plus de change que le vin soit “medium” que “bad”; et \\(e^{2.504}= 12.2\\) fois plus de chance que le vin soir “good” que “medium”. Si l’on prend la modélisation simplifiée, on a \\(e^{3.0711}= 21.5\\) fois plus de chance de passer dans la catégorie supérieure pour la qualité du vin (que l’on ait un vin “bad” ou “medium” présentement). Lorsque l’on observe la valeur moyenne des variables explicatives (\\(x=(0,0)\\)), on a \\(e^{1.281}=3.6\\) fois plus de chance d’avoir un vin “médium” que “bad” et \\(e^{-2.0369}=0.13\\) fois plus de chance d’avoir un vin “good” que un vin “medium”. Dans modeleadj@predictors, on récupère l’ensemble des valeurs des prédicteurs linéaires \\(\\ln\\left[\\hat\\pi_{m+1}({\\bf x}_i) / \\hat \\pi_{m}({\\bf x}_i)\\right]\\). A partir de ces valeurs, on peut retrouver les \\(\\pi_m({\\bf x}_i)\\) (disponibles dans modeleadj@fitted.values) par la formule \\[ \\left\\{\\begin{array}{l} \\hat \\pi_{m+1}({\\bf x}) = \\frac{\\displaystyle \\prod_{v=1}^m e^{{\\bf x} \\hat \\theta^{(v)}}}{\\displaystyle 1 + \\sum_{m&#39;=1}^{M-1}\\prod_{v=1}^{m&#39;} e^{{\\bf x} \\hat \\theta^{(v)}}}, \\quad \\forall m\\in\\{1,\\ldots,M-1\\} \\\\ \\\\ \\hat \\pi_{1}({\\bf x}) = \\frac{\\displaystyle 1}{\\displaystyle 1 + \\sum_{m=1}^{M-1} \\prod_{v=1}^{m} e^{{\\bf x} \\hat\\theta^{(v)}}}. \\end{array}\\right. \\] On définit alors les prédictions pour nos \\(n\\) individus par \\[ \\hat Y_i = u_{\\hat m} \\quad\\textrm{où}\\quad \\hat m\\in\\underset{m=1,\\ldots,M}{\\mbox{argmax}}\\ \\hat{\\pi}_m({\\bf x}_i). \\] Dans notre exemple, on compare ainsi les prédiction avec les valeurs observées de la réponse : hatpi&lt;-modeleadj@fitted.values hatY&lt;-apply(hatpi,1,which.max) table(Quality,hatY) hatY Quality 1 2 3 bad 11 1 0 medium 1 8 2 good 0 3 8 "],["RegLogLin.html", "Chapitre 11 Régression de Poisson / régression loglinéaire 11.1 Modèle de régression loglinéaire 11.2 Exemple de régression loglinéaire avec R 11.3 Sur-dispersion et modèle binomial négatif 11.4 Quelques codes avec python", " Chapitre 11 Régression de Poisson / régression loglinéaire Les slides associés à la régression loglinéaire sont disponibles ici (partie II) Le jeu de données utilisé dans ce chapitre est ShipAccident issu de la librairie AER. Dans ce chapitre, on s’intéresse au cas où la variable réponse \\(Y\\) compte le nombre de fois qu’un certain évènement a lieu dans une période de temps donnée (e.g. nombre d’accidents de la route sur une année, nombre d’enfants par famille, le nombre de grèves d’une compagnie sur une période de trois ans, etc). Nous allons illustrer les différents points abordés dans ce chapitre avec l’exemple suivant. Example 11.1 Nombre d’incidents maritimes On s’intéresse à la variable incidents comptant le nombre d’incidents par mois de mise en service d’un bateau. On considère ici un échantillon de 40 bateaux et l’on souhaite expliquer la variable incidents à l’aide des 4 variables suivantes : type : il y a 5 types de bateaux, désignés par A-B-C-D-E. C’est une variable qualitative nominale, codée sous forme de facteur, construction : période de construction du bateau, à savoir entre 1960 et 1979 par périodes de 5 ans, operation : période de mise en service (entre 1960 et 1974 ou entre 1975 et 1979), service : nombre total de mois de mise en service du bateau. Comme dans le chapitre précédent, on considère des variables explicatives quantitatives et qualitatives. Le comportement de ces variables est résumé sur la Figure 11.1, sauf la variable type pour laquelle chacune des modalités apparaît exactement 8 fois. data(&quot;ShipAccidents&quot;) summary(ShipAccidents) type construction operation service incidents A:8 1960-64: 9 1960-74:19 Min. : 0.0 Min. : 0.0 B:8 1965-69:10 1975-79:21 1st Qu.: 175.8 1st Qu.: 0.0 C:8 1970-74:10 Median : 782.0 Median : 2.0 D:8 1975-79:11 Mean : 4089.3 Mean : 8.9 E:8 3rd Qu.: 2078.5 3rd Qu.:11.0 Max. :44882.0 Max. :58.0 Figure 11.1: Résumé des différentes variables de l’exemple d’incidents maritimes. À première vue, il semblerait d’après l’histogramme que la variable réponse incidents suive une loi de Poisson de petit paramètre. En particulier, la probabilité d’observer peu d’incidents est très élevée, alors que la probabilité d’observer plusieurs incidents décroit exponentiellement. Notons toutefois que la distribution de Poisson est la loi la plus simple permettant de modéliser des données de comptage, mais ce n’est pas la seule. 11.1 Modèle de régression loglinéaire 11.1.1 Pourquoi un modèle particulier ? Dans la suite, on note \\(Y=(Y_1,\\ldots,Y_n)&#39;\\ \\) le vecteur des réponses, et \\(\\textbf{x}_i\\) le vecteur ligne des variables explicatives considérées pour l’individu \\(i\\) pour chaque \\(i\\) dans \\(\\{1\\ldots,n\\}\\). Les variables réponses à expliquer \\(Y_i | \\textbf{x}_i \\sim \\mathcal P(\\lambda(\\textbf{x}_i))\\) vérifient \\[ \\mathbb{E}[Y_i|\\textbf{x}_i] = \\lambda(\\textbf{x}_i), \\] avec \\(\\lambda(\\textbf{x}_i)&gt;0\\). L’objectif est de construire un modèle pour reconstituer \\(\\lambda(\\textbf{x}_i)\\) en fonction des variables explicatives. Si on utilise le modèle de régression usuel \\(Y_i = \\textbf{x}_i\\theta +\\varepsilon_i\\) pour expliquer le nombre d’incidents en fonction du nombre de mois de mise en service, on s’aperçoit d’une part que l’hypothèse de normalité des résidus n’est clairement pas réaliste (voir la figure ??). Figure 11.2: Ajustement d’un modèle linéaire (à gauche) et graphique QQ-plot des résidus correspondants (à droite) D’autre part, les variables \\(\\varepsilon_i\\) étant supposées centrées, \\[\\lambda(\\textbf{x}_i) = \\mathbb{E}[Y_i|\\textbf{x}_i] = \\textbf{x}_i \\theta.\\] Or rien n’indique que \\(\\textbf{x}_i \\theta &gt;0\\). Il est donc nécessaire de définir une fonction lien reliant \\(\\lambda(\\textbf{x}_i)\\) au prédicteur linéaire \\(\\eta_i= \\textbf{x}_i \\theta\\). Pour garantir que l’espérance conditionnelle \\(\\lambda(\\textbf{x}_i)=\\mathbb{E}[Y_i|\\textbf{x}_i]\\) est bien strictement positive, on définit le modèle par \\[ \\lambda(\\textbf{x}_i)=\\lambda_\\theta ({\\bf x}_i) = \\exp({\\bf x}_i \\theta). \\] Cela revient à poser \\(\\ln(\\lambda_\\theta ({\\bf x}_i))={\\bf x}_i \\theta\\). On retrouve la fonction lien logarithmique, qui est le lien canonique associé à la loi de Poisson, d’où le terme générique de régression loglinéaire. Une fois le modèle posé, il reste à estimer le paramètre du modèle \\(\\theta\\) inconnu. 11.1.2 Estimation des paramètres Comme dans le cas binaire, il faut bien faire attention à la nature des variables explicatives, et définir pour les variables qualitatives des modalités de référence. On se place ici dans un cadre très général. Le paramètre \\(\\theta\\) est estimé par la méthode du maximum de vraisemblance. La vraisemblance des données \\(\\underline{Y}=(Y_1,\\ldots, Y_n)&#39;\\) est définie par : \\[ L(\\underline{Y}; \\theta) = \\prod_{i=1}^n \\left[\\frac{\\lambda_\\theta ({\\bf x}_i)^{Y_i}}{Y_i !} \\exp(-\\lambda_\\theta ({\\bf x}_i))\\right], \\] et la log-vraisemblance par : \\[\\begin{eqnarray*} l(\\underline{Y}; \\theta) &amp;=&amp; \\sum_{i=1}^n \\left[Y_i \\ln(\\lambda_\\theta ({\\bf x}_i)) - \\lambda_\\theta ({\\bf x}_i) - \\ln(Y_i !) \\right]\\\\ &amp;=&amp; \\sum_{i=1}^n \\left[Y_i {\\bf x}_i \\theta - e^{{\\bf x}_i \\theta} - \\ln(Y_i !) \\right]. %&amp;=&amp; \\sum_{i=1}^n Y_i - F(\\theta_0 + \\theta_1 x_i). \\end{eqnarray*}\\] Comme dans les chapitres précédents, on cherche alors à annuler les dérivées partielles : \\[\\frac{\\partial l(\\underline{Y}; \\theta) }{\\partial \\theta_j} = \\sum_{i=1}^n \\left[x_i^{(j)} (Y_i - e^{{\\bf x}_i \\theta})\\right].\\] Encore une fois, le système obtenu n’admet généralement pas de solution calculable analytiquement. Un algorithme de type Newton-Raphson ou de Fisher-scoring est alors mis en place, nécessitant l’évaluation la matrice hessienne ou la matrice d’information de Fisher. Les dérivées d’ordre secondes sont obtenues de la manière suivante : \\[\\frac{\\partial ^2 l(\\underline{Y};\\theta)}{\\partial \\theta_j \\partial\\theta_k} = -\\sum_{i=1}^n x_i^{(j)} x_i^{(k)} e^{{\\bf x}_i \\theta}, \\] d’où l’écriture matricielle de l’information de Fisher \\[\\mathcal I_n(\\theta) = X&#39; W X, \\quad \\mbox{avec} \\quad W = \\mbox{diag}[e^{{\\bf x}_1 \\theta},\\ldots,e^{{\\bf x}_n \\theta}],\\] et \\(X\\) la matrice de design dont les lignes sont composées des vecteurs \\({\\bf x}_i\\). Remarquons encore une fois que cette matrice dépend du paramètre inconnu \\(\\theta\\) d’où la nécessité de mettre en place un algorithme itératif. Par ailleurs, notons que les dérivées secondes ne dépendant pas des variables \\(Y_i\\), les algorithmes de Newton-Raphson et de Fisher-scoring sont exactement les mêmes. 11.1.3 Ajustement et prédiction Une fois le modèle ajusté, nous obtenons une estimation pour chaque prédicteur linéaire \\(\\eta_i={\\bf x}_i\\theta\\) par \\(\\hat \\eta_i={\\bf x}_i\\hat\\theta\\) et pour chaque paramètre \\[\\hat \\lambda(\\textbf{x}_i) = \\lambda_{\\hat\\theta}({\\bf x}_i) = \\exp(\\textbf{x}_i \\hat\\theta).\\] Les valeurs ajustées \\(\\widehat Y_i\\) pour les \\(Y_i\\) sont alors définies suivant la règle \\[ \\widehat Y_i \\ \\in\\ \\underset{k\\in\\mathbb{N}}{\\mbox{argmax}} \\ \\left\\{\\frac{(\\hat\\lambda(\\textbf{x}_i))^k}{k!}e^{-\\hat\\lambda(\\textbf{x}_i)}\\right\\}. \\] \\(\\widehat Y_i\\) correspond donc à l’entier le plus probable pour la loi de Poisson de paramètre \\(\\hat\\lambda(\\textbf{x}_i)\\). Si l’on se donne maintenant un nouvel individu décrit par \\(\\textbf{x}_0\\) alors le modèle ajusté permet de prédire son nombre moyen de “succès”, donné par \\(\\hat\\lambda(\\textbf{x}_0) = \\exp(\\textbf{x}_0 \\hat\\theta)\\), et sa réponse prédite définie par \\[ \\widehat Y_0 \\ \\in\\ \\underset{k\\in\\mathbb{N}}{\\mbox{argmax}} \\ \\left{\\frac{[\\hat\\lambda(\\textbf{x}_0)]^k}{k!}e^{-\\hat\\lambda(\\textbf{x}_0)}\\right\\}. \\] 11.2 Exemple de régression loglinéaire avec R L’étude inférentielle du modèle de régression de Poisson est similaire au cas logistique, et découle directement des résultats asymptotiques étudiés dans le chapitre 9. Ils ne sont donc pas détaillés ici, mais illustrés dans cette partie sur l’exemple du nombre d’incidents maritimes. 11.2.1 Régression loglinéaire simple Dans cette section, on modélise la variable réponse incidents à l’aide d’une seule variable explicative. On va distinguer selon la nature de la variable explicative. 11.2.1.1 Variable explicative quantitative Commençons par modéliser la variable réponse incidents à l’aide de la variable \\(x=\\) service : \\[\\ln[\\lambda_\\theta(x)] = \\theta_0 + \\theta_1 x.\\] fit.service &lt;- glm(incidents ~ service, data=ShipAccidents, family=poisson) summary(fit.service) Call: glm(formula = incidents ~ service, family = poisson, data = ShipAccidents) Deviance Residuals: Min 1Q Median 3Q Max -6.0040 -3.1674 -2.0055 0.9155 7.2372 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 1.613e+00 7.150e-02 22.55 &lt;2e-16 *** service 6.417e-05 2.870e-06 22.36 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for poisson family taken to be 1) Null deviance: 730.25 on 39 degrees of freedom Residual deviance: 374.55 on 38 degrees of freedom AIC: 476.41 Number of Fisher Scoring iterations: 6 L’estimation du coefficient de la variable service vaut \\(6.417 \\times 10^{-5}\\), et est donc très proche de \\(0\\). Cependant, la \\(p\\)-valeur du \\(Z\\)-test (basé sur l’approximation Gaussienne) étant \\(&lt;2\\times10^{-16}\\), nous rejetons la nullité de ce coefficient. Cela est probablement dû à la très grande variance de cette variable. En particulier, la variable service semble avoir une influence significative sur la variable incidents. 11.2.1.2 Variable explicative qualitative À présent, modélisons la variable réponse incidents à l’aide de la seule variable qualitative type à \\(5\\) modalités. Comme dans le cas binaire, pour rendre le modèle identifiable, il faut choisir une modalité de référence (ici, la modalité choisie par défaut est type=A). Le modèle s’écrit donc \\[\\ln[\\lambda_\\theta(x)] = \\theta_0 + \\theta_1 \\mathbb{1}_{{\\tt type=B}} + \\theta_2 \\mathbb{1}_{{\\tt type=C}} + \\theta_3 \\mathbb{1}_{{\\tt type=D}} + \\theta_4 \\mathbb{1}_{{\\tt type=E}}.\\] fit.type &lt;- glm(incidents ~ type, data=ShipAccidents, family=poisson) summary(fit.type) Call: glm(formula = incidents ~ type, family = poisson, data = ShipAccidents) Deviance Residuals: Min 1Q Median 3Q Max -7.9530 -2.0616 -0.4541 1.2873 4.3425 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 1.6582 0.1543 10.747 &lt; 2e-16 *** typeB 1.7957 0.1666 10.777 &lt; 2e-16 *** typeC -1.2528 0.3273 -3.827 0.00013 *** typeD -0.9045 0.2875 -3.146 0.00165 ** typeE -0.2719 0.2346 -1.159 0.24650 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for poisson family taken to be 1) Null deviance: 730.25 on 39 degrees of freedom Residual deviance: 275.65 on 35 degrees of freedom AIC: 383.52 Number of Fisher Scoring iterations: 6 L’interprétation des coefficients n’étant pas si simple, il est possible de tester la significativité de la variable par un test de sous-modèle : anova(glm(incidents ~ 1, data=ShipAccidents, family=poisson), fit.type, test=&quot;Chisq&quot;) Analysis of Deviance Table Model 1: incidents ~ 1 Model 2: incidents ~ type Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) 1 39 730.25 2 35 275.65 4 454.6 &lt; 2.2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Ici, le test de rapport de vraisemblance (basé sur la déviance des modèles étudiés) rejette le sous-modèle nul. La variable type a une influence significative sur le nombre d’incidents, ce qui est en accord avec les boîtes à moustaches (boxplots) représentées en Figure 11.3. On comprend aussi pourquoi le paramètre estimé associé à type=B est très différent de 0 car différence de comportement de incident entre la modalité A de référence et B. Au contraire, on accepte la nullité du paramètre associé à type=E ce qui est en accord avec le comportement similaire de incident entre les modalités A et E de type (cf Figure 11.3). Figure 11.3: Nombres d’accidents par mois de services en fonction du type de bateau. 11.2.2 Régression loglinéaire multiple 11.2.2.1 Ajustement du modèle additif Dans cette section, on modélise la variable réponse incidents à l’aide de toutes les variables explicatives disponibles. Comme pour le cas binaire, nous pourrions considérer toutes les interactions d’ordre 2 entre les variables explicatives. Cependant, cela mènerait à estimer 37 coefficients, ce qui semble peu raisonnable pour un échantillon de 40 bateaux. Nous ajustons donc le modèle additif suivant \\[Y_i\\sim\\mathcal{P}(\\lambda(\\bf x)_i)\\] avec \\[ \\begin{array}{l l} \\ln[\\lambda(\\textbf{x}_i)] = &amp; \\theta_0 + \\alpha_1 \\mathbb{1}_{{\\tt type_i=B}} + \\alpha_2 \\mathbb{1}_{{\\tt type_i=C}} + \\alpha_3 \\mathbb{1}_{{\\tt type_i=D}} +\\alpha_4 \\mathbb{1}_{{\\tt type_i=E}}\\\\ &amp; + \\beta_1 \\mathbb{1}_{const_i = &quot;65-69&quot;} + \\beta_2 \\mathbb{1}_{const_i = &quot;70-74&quot;} + \\beta_3 \\mathbb{1}_{const_i =&quot;75-79&quot;} \\\\ &amp; + \\gamma_1\\mathbb{1}_{op_i = &quot;75-79&quot;} + \\theta_1 service_i \\\\ \\end{array} \\] Les modalités de référence des deux variables qualitatives construction et operation choisies sont respectivement 1960-64 et 1960-74. fit.add &lt;- glm(incidents ~ . , data=ShipAccidents, family=poisson) summary(fit.add) Call: glm(formula = incidents ~ ., family = poisson, data = ShipAccidents) Deviance Residuals: Min 1Q Median 3Q Max -2.5810 -1.4773 -0.8972 0.5952 3.2154 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 5.492e-04 2.787e-01 0.002 0.998427 typeB 5.933e-01 2.163e-01 2.743 0.006092 ** typeC -1.190e+00 3.275e-01 -3.635 0.000278 *** typeD -8.210e-01 2.877e-01 -2.854 0.004321 ** typeE -2.900e-01 2.351e-01 -1.233 0.217466 construction1965-69 1.148e+00 1.793e-01 6.403 1.53e-10 *** construction1970-74 1.596e+00 2.242e-01 7.122 1.06e-12 *** construction1975-79 5.670e-01 2.809e-01 2.018 0.043557 * operation1975-79 8.619e-01 1.317e-01 6.546 5.92e-11 *** service 7.270e-05 8.488e-06 8.565 &lt; 2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for poisson family taken to be 1) Null deviance: 730.253 on 39 degrees of freedom Residual deviance: 99.793 on 30 degrees of freedom AIC: 217.66 Number of Fisher Scoring iterations: 5 11.2.2.2 Sélection de variables et sous-modèles Il est possible de faire de la sélection de variables grâce à la commande step(fit.add). Une procédure de sélection de variables descendante sur critère AIC est alors appliquée sur le modèle additif. Dans notre cas, elle renvoie exactement le même modèle. step(fit.add,trace=1) Start: AIC=217.66 incidents ~ type + construction + operation + service Df Deviance AIC &lt;none&gt; 99.793 217.66 - type 4 148.053 257.92 - operation 1 147.687 263.55 - service 1 182.605 298.47 - construction 3 191.419 303.29 Call: glm(formula = incidents ~ type + construction + operation + service, family = poisson, data = ShipAccidents) Coefficients: (Intercept) typeB typeC 0.0005492 0.5932730 -1.1903189 typeD typeE construction1965-69 -0.8210370 -0.2899922 1.1478796 construction1970-74 construction1975-79 operation1975-79 1.5964752 0.5669790 0.8618750 service 0.0000727 Degrees of Freedom: 39 Total (i.e. Null); 30 Residual Null Deviance: 730.3 Residual Deviance: 99.79 AIC: 217.7 On pourrait également par exemple tester la nullité simultanée des coefficients des variables contructions et operation, ce qui revient à faire un test de sous-modèle, en considérant seulement les variables type et service. fit.ssmod &lt;- glm(incidents ~ type + service, data=ShipAccidents, family=poisson) anova(fit.ssmod, fit.add, test=&quot;Chisq&quot;) Analysis of Deviance Table Model 1: incidents ~ type + service Model 2: incidents ~ type + construction + operation + service Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) 1 34 230.832 2 30 99.793 4 131.04 &lt; 2.2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Le test de rapport de vraisemblance rejette le sous-modèle avec une \\(p\\)-valeur \\(&lt;2\\times 10^{-16}\\). 11.2.2.3 Prédiction On souhaite prédire le nombre moyen d’incidents pour un bateau de type A, construit entre 1965 et 1969, et mis en service entre 1960 et 1975 au bout de 1000 mois de services. \\[ \\hat\\lambda_0 = e^{X_0 \\hat\\theta_{ML}} \\textrm{ with } X_0=(1,\\underbrace{0,0,0,0}_{type},\\underbrace{1,0,0}_{construction},0,1000) \\] new.data = data.frame(type=factor(&quot;A&quot;), construction=factor(&quot;1965-69&quot;), operation=factor(&quot;1960-74&quot;), service = 1000) lambda_hat = exp(predict(fit.add,new.data)) lambda_hat 1 3.391016 Utilisant la loi de Poisson \\(A\\sim\\mathcal{P}(\\hat\\lambda_0)\\), on peut alors prédire la probabilité de certains évenements comme par exemple probabilité que ce type de bateau n’ait aucun incident : \\(\\mathbb{P}(A=0) = e^{- \\hat\\lambda_0}\\) probabilité que ce type de bateau ait au plus un incident : \\(\\mathbb{P}(A\\leq 1) = (1+\\hat\\lambda_0)e^{- \\hat\\lambda_0}\\) # probabilité d&#39;aucun incident : exp(-lambda_hat) 1 0.03367446 # probabilité d&#39;au plus un incident : (1+lambda_hat) * exp(-lambda_hat) 1 0.1478651 11.3 Sur-dispersion et modèle binomial négatif Dans le cas du modèle de régression de Poisson, on a \\[\\mathbb{E}[Y_i|{\\bf x}_i] = \\text{Var}(Y_i|{\\bf x}_i),\\] ce qui est une hypothèse très restrictive. Si \\(\\mathbb{E}[Y_i|{\\bf x}_i] &gt; \\text{Var}(Y_i|{\\bf x}_i)\\) (respectivement \\(\\mathbb{E}[Y_i|{\\bf x}_i] &lt; \\text{Var}(Y_i|{\\bf x}_i)\\)), nous parlons alors de sur-dispersion (respectivement de sous-dispersion). Ces deux propriétés n’étant pas autorisées par le modèle de Poisson, nous définissons une classe plus riche de modèles basée sur la loi binomiale négative. Rappelons que la loi binomiale négative de paramètres \\(n\\) et \\(p\\) permet de modéliser le nombre d’échecs nécessaires avant l’obtention de \\(n\\) succès lors de la répétition de “tirage” indépendants de probabilité de succès \\(p\\). Elle peut être généralisée à \\(n=r\\) non-entier. Le modèle binomial négatif suppose que la loi de \\(Y_i\\) vérifie pour tout \\(k\\) \\[\\mathbb{P}(Y_i = k) = \\frac{\\Gamma(r+k)}{k! \\Gamma(r)} \\left(\\frac{\\lambda(\\textbf{x}_i)}{\\lambda(\\textbf{x}_i)+r}\\right)^k \\left(1-\\frac{\\lambda(\\textbf{x}_i)}{\\lambda(\\textbf{x}_i)+r}\\right)^r.\\] Nous pouvons alors démontrer que \\[\\mathbb{E}[Y_i] = \\lambda(\\textbf{x}_i) \\quad \\mbox{et}\\quad \\text{Var}(Y_i) = \\lambda(\\textbf{x}_i) (1 + \\nu^2\\lambda(\\textbf{x}_i)),\\] où \\(\\nu = 1/r\\) mesure le degré de sur-dispersion. Remarquons que le cas limite \\(\\nu=0\\) correspond à la loi de Poisson. Comme dans le cas Poisson, l’espérance conditionnelle est modélisée par \\[\\mathbb{E}[Y_i|{\\bf x}_i] = \\lambda_\\theta({\\bf x}_i) = \\exp({\\bf x}_i \\theta).\\] En particulier, ce modèle appartient également à la famille des modèles de regression loglinéaire. Les paramètres inconnus \\(\\theta\\) et \\(\\nu\\) sont estimés par maximum de vraisemblance. Dans R, le modèle binomial négatif est implémenté dans la fonction glm() pour la famille de lois family = quasipoisson(link = \"log\"). 11.4 Quelques codes avec python import pandas as pd import numpy as np import statsmodels.api as sm from statsmodels.formula.api import glm Accidpy=r.ShipAccidents fitservicepy=glm(&#39;incidents~service&#39;,data=Accidpy,family=sm.families.Poisson()).fit() print(fitservicepy.summary()) Generalized Linear Model Regression Results ============================================================================== Dep. Variable: incidents No. Observations: 40 Model: GLM Df Residuals: 38 Model Family: Poisson Df Model: 1 Link Function: log Scale: 1.0000 Method: IRLS Log-Likelihood: -236.21 Date: Jeu, 28 oct 2021 Deviance: 374.55 Time: 12:20:55 Pearson chi2: 368. No. Iterations: 6 Covariance Type: nonrobust ============================================================================== coef std err z P&gt;|z| [0.025 0.975] ------------------------------------------------------------------------------ Intercept 1.6127 0.072 22.555 0.000 1.473 1.753 service 6.417e-05 2.87e-06 22.356 0.000 5.85e-05 6.98e-05 ============================================================================== fittypepy=glm(&#39;incidents~C(type)&#39;,data=Accidpy,family=sm.families.Poisson()).fit() print(fittypepy.summary()) Generalized Linear Model Regression Results ============================================================================== Dep. Variable: incidents No. Observations: 40 Model: GLM Df Residuals: 35 Model Family: Poisson Df Model: 4 Link Function: log Scale: 1.0000 Method: IRLS Log-Likelihood: -186.76 Date: Jeu, 28 oct 2021 Deviance: 275.65 Time: 12:20:56 Pearson chi2: 249. No. Iterations: 5 Covariance Type: nonrobust ================================================================================ coef std err z P&gt;|z| [0.025 0.975] -------------------------------------------------------------------------------- Intercept 1.6582 0.154 10.747 0.000 1.356 1.961 C(type)[T.B] 1.7957 0.167 10.777 0.000 1.469 2.122 C(type)[T.C] -1.2528 0.327 -3.827 0.000 -1.894 -0.611 C(type)[T.D] -0.9045 0.287 -3.146 0.002 -1.468 -0.341 C(type)[T.E] -0.2719 0.235 -1.159 0.246 -0.732 0.188 ================================================================================ from scipy.stats import chi2 LR_stat=(-2)*(fittypepy.llnull - fittypepy.llf); pvalue=1-chi2(4).cdf(LR_stat); print(LR_stat) 454.6025535903679 print(pvalue) 0.0 fitaddpy =glm(&#39;incidents~C(type)+C(construction)+C(operation)+service&#39;, data=Accidpy,family=sm.families.Poisson()).fit() print(fitaddpy.summary()) Generalized Linear Model Regression Results ============================================================================== Dep. Variable: incidents No. Observations: 40 Model: GLM Df Residuals: 30 Model Family: Poisson Df Model: 9 Link Function: log Scale: 1.0000 Method: IRLS Log-Likelihood: -98.830 Date: Jeu, 28 oct 2021 Deviance: 99.793 Time: 12:20:58 Pearson chi2: 90.0 No. Iterations: 6 Covariance Type: nonrobust ============================================================================================== coef std err z P&gt;|z| [0.025 0.975] ---------------------------------------------------------------------------------------------- Intercept 0.0005 0.279 0.002 0.998 -0.546 0.547 C(type)[T.B] 0.5933 0.216 2.743 0.006 0.169 1.017 C(type)[T.C] -1.1903 0.327 -3.635 0.000 -1.832 -0.548 C(type)[T.D] -0.8210 0.288 -2.854 0.004 -1.385 -0.257 C(type)[T.E] -0.2900 0.235 -1.233 0.217 -0.751 0.171 C(construction)[T.1965-69] 1.1479 0.179 6.403 0.000 0.796 1.499 C(construction)[T.1970-74] 1.5965 0.224 7.122 0.000 1.157 2.036 C(construction)[T.1975-79] 0.5670 0.281 2.018 0.044 0.016 1.118 C(operation)[T.1975-79] 0.8619 0.132 6.546 0.000 0.604 1.120 service 7.27e-05 8.49e-06 8.565 0.000 5.61e-05 8.93e-05 ============================================================================================== fitssmodpy = glm(&#39;incidents~C(type)+service&#39;,data=Accidpy,family=sm.families.Poisson()).fit() LR_stat=(fitssmodpy.deviance - fitaddpy.deviance) print(LR_stat) 131.03874238496047 print(1-chi2(4).cdf(LR_stat)) 0.0 "],["rappels-de-probabilités-statistiques-et-doptimisation.html", "A Rappels de probabilités, statistiques et d’optimisation A.1 Rappels sur les échantillons gaussiens A.2 Estimation sans biais de variance minimale A.3 La méthode de Newton-Raphson A.4 Théorème central limite: condition de Lindeberg", " A Rappels de probabilités, statistiques et d’optimisation A.1 Rappels sur les échantillons gaussiens A.1.1 La loi normale Definition A.1 On dit que la variable aléatoire \\(X\\) suit une loi normale de paramètres \\((m, \\sigma^2)\\), notée \\(\\mathcal{N}(m, \\sigma^2)\\), si la loi de \\(X\\) a pour densité \\[ f(x)=\\frac{1}{\\sigma \\sqrt{2 \\pi}}\\exp \\left[-\\frac{1}{2\\sigma^2}(x-m)^2\\right]. \\] Proposition A.1 Propriétés de la loi gaussienne Si \\(X\\) suit une loi \\(\\mathcal{N}(m, \\sigma^2)\\) alors \\(\\mathbb{E}[X]=m\\), \\(\\mbox{Var}(X)=\\sigma^2\\) et \\((X-m) / \\sigma \\mbox{ suit la loi } \\mathcal{N}(0,1).\\) De plus, la fonction caractéristique de la loi de \\(X\\) est définie par \\[ \\forall t\\in\\mathbb{R},\\ \\Phi_X(t)= \\mathbb{E}\\left[e^{itX}\\right]=\\exp\\left( itm-\\frac{\\sigma^2 t^2}{2}\\right). \\] Si \\(X_1,\\ldots,X_n\\) sont des variables aléatoires gaussiennes indépendantes, telles que, pour \\(i=1,\\cdots,n,\\ X_i \\mbox{ suit la loi } \\mathcal{N}(m_i, \\sigma_i^2)\\), alors pour tout \\((\\alpha_1, \\ldots, \\alpha_n) \\in \\mathbb{R}^n,\\) \\[\\begin{equation} \\tag{A.1} \\alpha_1 X_1 + \\ldots +\\alpha_n X_n \\mbox{ suit la loi } \\mathcal{N}(\\alpha_1 m_1 + \\ldots +\\alpha_n m_n,\\alpha_1^2 \\sigma_1^2 + \\ldots +\\alpha_n^2 \\sigma_n^2). \\end{equation}\\] A.1.2 Vecteurs gaussiens Definition A.2 Un vecteur aléatoire \\(X\\) à valeurs dans \\(\\mathbb{R}^d\\) est dit gaussien si toute combinaison linéaire de ses composantes est une variable aléatoire gaussienne. Si \\(X=(X_1,\\cdots,X_d)&#39;\\) est un vecteur gaussien, on définit son vecteur moyenne \\(\\mathbb{E}[X]\\) par : \\[\\mathbb{E}[X]=(\\mathbb{E}[X_1],\\cdots,\\mathbb{E}[X_d])&#39;\\] et sa matrice de variance-covariance \\(\\mbox{Var}(X)\\) par \\[\\begin{eqnarray*} \\mbox{Var}(X)&amp;=&amp;\\mathbb{E}\\left[\\left(X-\\mathbb{E}(X)\\right)\\left(X-\\mathbb{E}(X)\\right)&#39;\\right]\\\\ &amp;=&amp;\\left(\\begin{array}{cccc} \\mbox{Var}(X_1) &amp; \\mbox{Cov}(X_1,X_2) &amp; \\cdots &amp; \\mbox{Cov}(X_1,X_d)\\\\ \\mbox{Cov}(X_2,X_1) &amp; \\mbox{Var}(X_2) &amp; \\cdots &amp; \\mbox{Cov}(X_2,X_d)\\\\ \\vdots &amp; \\ddots &amp; \\cdots &amp; \\vdots\\\\ \\mbox{Cov}(X_d,X_1) &amp; \\mbox{Cov}(X_d,X_2) &amp; \\cdots &amp; \\mbox{Var}(X_d) \\end{array}\\right). \\end{eqnarray*}\\] Remark. On peut noter que La matrice \\(\\mbox{Var}(X)\\) est symétrique puisque l’on a pour tout \\(i \\neq j\\) : \\[\\mbox{Var}(X)_{i,j}=\\mbox{Cov}(X_i,X_j)=\\mbox{Cov}(X_j,X_i)=\\mbox{Var}(X)_{j,i}.\\] Si \\((X_1,\\cdots,X_n)\\) est un n-échantillon de loi gaussienne, i.e. \\(X_1,\\cdots, X_n\\) sont \\(n\\) variables aléatoires indépendantes et identiquement distribuées selon une loi \\(\\mathcal{N}(\\mu,\\sigma^2)\\), alors on a évidemment que \\(X=(X_1,\\cdots,X_n)&#39;\\) est un vecteur gaussien de vecteur moyenne \\(\\mathbb{E}[X]=(\\mu,\\cdots,\\mu)&#39;\\) et de matrice de variance-covariance \\(\\mbox{Var}(X)=\\sigma^2I_n\\) où \\(I_n\\) désigne la matrice identité. On va s’intéresser à la fonction caractéristique d’un vecteur gaussien et aux conséquences importantes qui en découlent. Theorem A.1 Soit \\(X=(X_1,\\cdots,X_d)&#39;\\) un vecteur gaussien. On note \\(m=\\mathbb{E}[X] \\in \\mathbb{R}^d\\) et \\(\\Sigma=\\mbox{Var}(X) \\in \\mathcal{M}_d(\\mathbb{R})\\). On a que \\(X\\) admet pour fonction caractéristique la fonction \\[\\forall u \\in \\mathbb{R}^d, \\, \\Phi_X(u)=\\mathbb{E}\\left[\\mbox{exp}(iu&#39;X)\\right]=\\mbox{exp}\\left(iu&#39;m-\\frac{1}{2}u&#39;\\Sigma u\\right).\\] La loi de \\(X\\) est entièrement déterminée par la donnée de \\(m\\) et de \\(\\Sigma\\). On note \\(X \\sim \\mathcal{N}_d(m,\\Sigma)\\). Corollary A.1 (Propriété de linéarité) Soit \\(X=(X_1,\\cdots,X_d)&#39;\\sim \\mathcal{N}_d(m,\\Sigma)\\). On a pour toute matrice \\(A\\) de \\(\\mathcal{M}_{pd}(\\mathbb{R})\\) et pour tout vecteur \\(b\\) de \\(\\mathbb{R}^p\\) \\[AX+b \\sim \\mathcal{N}_p(Am+b,A\\Sigma A&#39;).\\] Remark. Soient \\((X_i)_{i=1,\\cdots,n}\\) des variables aléatoires indépendantes de loi \\(\\mathcal{N}(m_i,\\sigma^2_i)\\). Alors on a \\[X =(X_1,\\cdots,X_n)&#39;\\sim \\mathcal{N}_n(m,\\Sigma) \\mbox{ avec } m=(m_1,\\cdots,m_n)&#39; \\mbox{ et } \\Sigma=\\left(\\begin{array}{cccc} \\sigma^2_1 &amp; 0 &amp; \\cdots &amp; 0\\\\ 0 &amp; \\sigma^2_2 &amp; 0 &amp; \\vdots \\\\ \\vdots &amp; \\ddots &amp; \\ddots &amp; 0 \\\\ 0 &amp; \\cdots &amp; 0 &amp; \\sigma^2_n \\end{array} \\right).\\] En prenant \\(A=(\\alpha_1,\\cdots,\\alpha_n)\\) et \\(b=0_{n}\\), on retrouve la Proposition A.1, équation (A.1). En effet, on a \\(\\displaystyle Am+b=\\sum_{i=1}^n \\alpha_i m_i\\) et \\(\\displaystyle A\\Sigma A&#39;=\\sum_{i=1}^n \\alpha_i^2 \\sigma^2_i\\). Corollary A.2 (Propriété d'indépendance) Soit \\(X=(X_1,\\cdots,X_d)&#39;\\) un vecteur gaussien. Alors les trois propriétés suivantes sont équivalentes : Les composantes \\(X_1,\\cdots, X_d\\) sont mutuellement indépendantes. Les composantes \\(X_1,\\cdots, X_d\\) sont deux à deux indépendantes. La matrice de variance-covariance \\(\\Sigma\\) est diagonale, i.e. \\(\\forall i\\neq j, \\, \\mbox{Cov}(X_i,X_j)=0.\\) Remark. Les composantes d’un vecteur gaussien sont des variables aléatoires gaussiennes mais la réciproque est fausse. En effet, on considère \\(X\\) et \\(Y\\) deux variables indépendantes telles que \\(X \\sim \\mathcal{N}(0,1)\\) et \\(Y \\sim \\mathcal{B}(0.5)\\). Alors \\(X_1=X\\) et \\(X_2=(2Y-1)X\\) sont des variables gaussiennes mais \\((X_1,X_2)&#39;\\) n’est pas un vecteur gaussien. On note que dans cet exemple, \\(\\mbox{Cov}(X_1,X_2)=0\\) mais que \\(X_1\\) et \\(X_2\\) ne sont pas indépendantes. A.1.3 Loi du khi-deux, loi de Student, loi de Fisher Definition A.3 Soient \\(Y_1, \\ldots,Y_n\\) des variables aléatoires indépendantes et de même loi \\(\\mathcal{N}(0,1).\\) La loi de \\(Y_1^2+ \\ldots + Y_n^2\\) est appelée loi du khi-deux à \\(n\\) degrés de liberté, et notée \\(\\chi^2(n).\\) Proposition A.2 Propriétés de la loi du khi-deux : Si \\(V \\sim \\chi^2(n)\\) alors \\(\\mathbb{E}[V]=n\\) et \\(\\mbox{Var}(V)=2n.\\) Si \\(V_1 \\sim \\chi^2(n_1)\\), si \\(V_2 \\sim \\chi^2(n_2)\\) et si \\(V_1\\) et \\(V_2\\) sont des variables aléatoires indépendantes, alors \\(V_1+V_2 \\sim \\chi^2(n_1+n_2)\\). Definition A.4 Soient \\(U\\) et \\(V\\) deux variables aléatoires telles que \\(U\\sim \\mathcal{N}(0,1)\\), \\(V \\sim \\chi^2(n)\\) et, \\(U\\) et \\(V\\) sont indépendantes. Alors la loi de \\[ \\frac{U}{\\sqrt{V/n}}=\\sqrt{n}\\frac{U}{\\sqrt{V}}\\] est appelée loi de Student à \\(n\\) degrés de liberté, notée \\(\\mathcal{T}(n)\\). Definition A.5 Soient \\(V_1\\) et \\(V_2\\) deux variables aléatoires indépendantes, respectivement de loi \\(\\chi^2(n_1)\\) et \\(\\chi^2(n_2)\\). La loi de \\[ \\frac{V_1/n_1}{V_2/n_2}\\] est appelée loi de Fisher de paramètres \\((n_1, n_2)\\). Elle est notée \\(\\mathcal{F}(n_1,n_2)\\). A.1.4 Estimation de la moyenne et de la variance d’un échantillon gaussien Soient \\(X_1, \\ldots, X_n\\) \\(n\\) variables aléatoires indépendantes et de même loi (i.i.d.), de loi \\(\\mathcal{N}(m, \\sigma^2)\\). À partir de l’observation d’une réalisation de l’échantillon \\((X_1, \\ldots, X_n)\\), on souhaite estimer les paramètres inconnus \\(m\\) et \\(\\sigma^2\\). Estimateur de \\(m\\) : La moyenne empirique \\[\\bar{X}_n=\\frac{1}{n} \\sum_{i=1}^n X_i\\] est un estimateur de \\(m\\). \\(\\bar{X}_n\\) est un estimateur sans biais de \\(m\\) : \\(\\mathbb{E}[\\bar{X}_n]= \\frac{1}{n} \\underset{i=1}{\\stackrel{n}{\\sum}} \\mathbb{E}[X_i]=m\\). \\(\\mbox{Var}(\\bar{X}_n)=\\frac{1}{n^2} \\underset{i=1}{\\stackrel{n}{\\sum}} \\mbox{Var}(X_i)=\\frac{\\sigma^2}{n}\\underset{n \\rightarrow \\infty}{\\longrightarrow} 0.\\) D’après l’inégalité de Bienaymé-Tchebytchev, \\(\\bar{X}_n\\) converge en probabilité quand \\(n\\) tend vers \\(+\\infty\\) vers \\(m\\), i.e. \\[ \\bar{X}_n \\underset{n\\rightarrow \\infty}{\\stackrel{\\mathbb{P}}{\\longrightarrow}}m. \\] D’après la Proposition A.1, équation (A.1), \\(\\bar{X}_n \\sim \\mathcal{N}\\left(m, \\frac{\\sigma^2}{n}\\right)\\). Il en résulte que la variable aléatoire \\[\\sqrt{n} \\frac{( \\bar{X}_n -m)}{\\sigma} \\sim \\mathcal{N}(0,1).\\] Estimateur de \\(\\sigma^2\\) \\[ S^2=\\frac{1}{n-1} \\sum_{i=1}^n (X_i-\\bar{X}_n)^2=\\frac{1}{n-1} \\left\\{ \\sum_{i=1}^nX_i^2- n \\bar{X}_n^2\\right\\}\\] est un estimateur sans biais de \\(\\sigma^2\\). De plus par la loi des grands nombres, on peut démontrer que \\(S^2\\) converge en probabilité quand \\(n\\) tend vers \\(+\\infty\\) vers \\(\\sigma ^2\\), c’est-à-dire \\[\\begin{equation} \\tag{A.2} S^2 \\underset{n\\rightarrow\\infty}{\\stackrel{\\mathbb{P}}{\\longrightarrow}}\\sigma^2. \\end{equation}\\] Theorem A.2 (Théorème de Cochran) Soient \\(X_1, \\ldots, X_n\\) i.i.d. de loi \\(\\mathcal{N}(0,\\sigma^2)\\). On note \\(X\\) le vecteur \\((X_1, \\ldots, X_n) \\in \\mathbb{R}^n\\). Soit \\(E_1 \\oplus E_2 \\oplus \\ldots \\oplus E_p\\) une décomposition de \\(\\mathbb{R}^n\\) en \\(p\\) sous-espaces orthogonaux de dimensions respectives \\(r_1, \\ldots, r_p\\). On note \\(X_{E_i}\\) la projection orthogonale de \\(X\\) sur \\(E_i\\). Alors les vecteurs \\(X_{E_1},X_{E_2},\\ldots,X_{E_p}\\) sont indépendants, de plus, pour tout \\(i\\), la variable \\(\\|X_{E_i}\\|^2\\) a pour loi \\(\\sigma^2\\chi^2(r_i)\\). Proposition A.3 Soient \\(X_1,\\ldots,X_n\\) i.i.d. de loi \\(\\mathcal{N}(m,\\sigma^2)\\). Les variables aléatoires \\[\\begin{equation} \\tag{A.3} \\bar{X}_n=\\frac{1}{n}\\sum_{i=1}^n X_i {\\mbox{ et }} S^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar{X}_n)^2 \\end{equation}\\] sont indépendantes. \\(\\bar{X}_n\\sim \\mathcal{N}(m,\\sigma^2/n),\\) \\(S^2\\sim \\frac{\\sigma^2}{n-1}\\chi^2(n-1).\\) Il en résulte que la variable aléatoire \\[\\begin{equation} \\tag{A.4} \\sqrt{n}\\ \\frac{\\bar{X}_n-m}{S} \\sim \\mathcal{T}(n-1). \\end{equation}\\] A.1.5 Construction d’intervalles de confiance Notons \\(t_{1-\\alpha/2}\\) le \\((1-\\alpha/2)\\)-quantile de la loi de Student à \\(n-1\\) degrés de liberté. Il résulte de la Proposition A.3, équation (A.4), que \\[ \\mathbb{P}\\left( - t_{1-\\alpha/2} \\leq \\frac{\\sqrt{n} (\\bar{X}_n-m)}{S} \\leq t_{1-\\alpha/2}\\right) =1-\\alpha. \\] Ceci fournit l’intervalle de confiance pour \\(m\\) avec coefficient de sécurité \\(1-\\alpha\\) : \\[ \\left[\\bar{X}_n - t_{1-\\alpha/2} \\frac{S}{\\sqrt{n}} \\,; \\, \\bar{X}_n + t_{1-\\alpha/2} \\frac{S}{\\sqrt{n}}\\right]. \\] Afin de construire un intervalle de confiance pour \\(\\sigma^2\\), nous introduisons les \\(\\alpha/2\\) et \\(1-\\alpha/2\\) quantiles de la loi du khi-deux à \\(n-1\\) degrés de liberté, notés respectivement \\(u_{\\alpha/2}\\) et \\(u_{1-\\alpha/2}\\). On obtient l’intervalle de confiance pour \\(\\sigma^2\\) avec coefficient de sécurité \\(1-\\alpha\\) : \\[ \\left[\\frac{(n-1)S^2}{u_{1-\\alpha/2}} \\, ; \\, \\frac{(n-1)S^2}{u_{\\alpha/2}}\\right] . \\] A.2 Estimation sans biais de variance minimale Tous les résultats de cette section sont admis et nous renvoyons pour les détails à des ouvrages plus théoriques comme Saporta (2006) ou Castelle and Duflo (1994). Definition A.6 Soit \\(U\\) une statistique fonction de \\(X_1,\\cdots,X_n\\) de loi \\(g(u,\\beta)\\) (densité dans le cas continu ou \\(\\mathbb{P}(U=u)\\) dans le cas discret). \\(U\\) est dite exhaustive si l’on a \\(L(X,\\beta)=g(u,\\beta)h(X)\\) (principe de factorisation). Cela signifie que la loi conditionnelle de l’échantillon est indépendante du paramètre. Par conséquent, une fois connue \\(U\\), aucune valeur de l’échantillon, ni aucune autre statistique ne nous apportera de renseignements supplémentaires sur \\(\\beta\\). Theorem A.3 S’il existe un estimateur de \\(\\beta\\) sans biais, de variance minimale, alors il est unique presque sûrement. Theorem A.4 (Rao-Blackwell) Soit \\(T\\) un estimateur quelconque sans biais de \\(\\beta\\) et soit \\(U\\) une statistique exhaustive pour \\(\\beta\\). Alors \\(T^*=\\mathbb{E}[T/\\beta]\\) est un estimateur sans biais de \\(\\beta\\) au moins aussi bon que \\(T\\). Theorem A.5 S’il existe une statistique exhaustive \\(U\\), alors l’estimateur \\(T\\) sans biais de \\(\\beta\\) de variance minimale (unique d’après le théorème A.3) ne dépend que de \\(U\\). Definition A.7 On dit qu’une statistique \\(U\\) est complète pour une famille de lois de probabilités \\(f(x,\\beta)\\) si \\(\\mathbb{E}\\left[h(U)\\right]=0, \\forall \\beta \\Rightarrow h=0\\ \\mbox{ps.}\\) Nous pouvons montrer que la statistique exhaustive des familles exponentielles est complète. Theorem A.6 (Lehmann-Scheffé) Si \\(T^*\\) est un estimateur sans biais de \\(\\beta\\) dépendant d’une statistique exhaustive complète \\(U\\) alors \\(T^*\\) est l’unique estimateur sans biais de variance minimale de \\(\\beta\\). En particulier si l’on dispose déjà de \\(T\\), estimateur sans biais de \\(\\beta\\), alors \\(T^*=\\mathbb{E}\\left[T/U\\right]\\). En conclusion, si on dispose d’un estimateur sans biais fonction d’une statistique exhaustive complète alors c’est le meilleur estimateur possible. A.3 La méthode de Newton-Raphson Soit \\(t:\\mathbb{R}\\rightarrow \\mathbb{R}\\) une fonction \\(\\mathcal{C}^1\\) donnée. La problématique consiste à trouver \\(Z^{\\star}\\) tel que \\(t(Z^{\\star})=0\\). Par définition de la dérivée, on a \\[ t&#39;(Z^{\\star}) = \\lim_{h\\rightarrow 0} \\frac{t(Z^{\\star}+h)-t(Z^{\\star})}{h}.\\] La méthode de Newton est basée sur l’heuristique suivante. Si \\(x\\) est suffisamment ‘proche’ de \\(Z^{\\star}\\), alors moralement \\[ t&#39;(x) \\simeq \\frac{t(x) - t(Z^{\\star})}{x-Z^{\\star}} \\ \\Leftrightarrow \\ x- Z^{\\star} \\simeq \\frac{t(x)}{t&#39;(x)}, \\] par définition de \\(Z^{\\star}\\). On va utiliser cette méthode de manière itérative en initialisant un \\(x_0\\) puis en posant, pour tout \\(n\\in\\mathbb{N}\\), \\[ x_n = x_{n-1} - \\frac{t(x_{n-1})}{t&#39;(x_{n-1})}.\\] Sous des hypothèses assez souples (fonction \\(t\\) deux fois différentiable au voisinage de \\(Z^{\\star}\\) par exemple), on peut démontrer que \\(x_n \\rightarrow Z^{\\star}\\) quand \\(n\\rightarrow +\\infty\\). A.4 Théorème central limite: condition de Lindeberg Le théorème suivant généralise le Théorème central limite à des suites de variables indépendantes mais non identiquement distribuées. Ce type de résultat est particulièrement intéressant pour le modèle linéaire généralisé. Theorem A.7 Soient \\(X_1,\\dots,X_n\\) des variables aléatoires indépendantes d’espérances et de variances respectives \\(m_i\\) et \\(\\sigma_i^2\\). Soient \\(S_n^2 = \\sum_{i=1}^n \\sigma_i^2\\) et pour tout \\(i\\in \\lbrace 1,\\dots,n \\rbrace\\), \\(F_i\\) la fonction de répartition des variables \\(X_i-m_i\\). Si \\[\\begin{equation} \\forall \\varepsilon&gt;0, \\ \\lim_{n\\to +\\infty} \\left[ \\frac{1}{S_n^2} \\sum_{i=1}^n \\int _{|x|&gt;\\varepsilon S_n} x^2dF_i(x) \\right] = 0, \\tag{A.5} \\end{equation}\\] alors, \\[\\frac{ \\sum_{i=1}^n (X_i - m_i)}{\\sqrt{S_n^2}} \\stackrel{\\mathcal{L}}{\\longrightarrow} \\mathcal{N}(0,1) \\ \\mathrm{quand} \\ n\\rightarrow + \\infty.\\] References "],["preuves-de-quelques-résultats-du-cours.html", "B Preuves de quelques résultats du cours B.1 Preuve pour le test de Fisher B.2 Preuve de la proposition 7.5 B.3 Preuve de la proposition 6.2 B.4 Preuve de la proposition 6.3 B.5 Critère du \\(C_p\\) de Mallows B.6 Preuve de la proposition ??", " B Preuves de quelques résultats du cours B.1 Preuve pour le test de Fisher On reprend les notations du chapitre 4. Rappelons la nature de chaque objet : \\(\\theta\\in\\mathbb{R}^k\\), \\(C\\in\\mathcal{M}_{qk}(\\mathbb{R})\\), \\(X_0\\in\\mathcal{M}_{nk_0}(\\mathbb{R})\\) et \\(X\\in\\mathcal{M}_{nk}(\\mathbb{R})\\) avec \\(Im(X_0)\\subset Im(X)\\). Proposition B.1 On veut tester \\[ \\mathcal{H}_0 : Y = X_0\\beta + \\varepsilon \\hspace{0.3cm}(M_0) \\textrm{ contre } \\mathcal{H}_1 : Y=X\\theta +\\varepsilon \\hspace{0.3cm}(M_1) \\] i.e \\[ \\mathcal{H}_0 : C\\theta = 0 \\textrm{ contre } \\mathcal{H}_1 : C\\theta \\neq 0. \\] Test 1 : La statistique de test \\[ F = \\frac{SCR_0 - SCR / (k-k_0)}{SCR/(n-k)} = \\frac{\\|X \\widehat{\\theta} - X_0 \\widehat{\\beta}\\|^2 / (k-k_0)}{\\|Y - X \\widehat{\\theta}\\|^2 / (n-k)}\\underset{\\mathcal{H}_0}{\\sim} \\mathcal{F}(k-k_0,n-k) \\] et la zone de rejet est donnée par \\[ \\mathcal{R} =\\{F \\geq f_{1-\\alpha,k-k_0,n-k}\\}. \\] Test 2 : La statistique de test \\[ \\tilde{F} = \\frac{[C\\widehat{\\theta}]&#39; [C (X&#39;X)^{-1} C&#39;]^{-1} [C\\widehat{\\theta}] /q}{SCR/(n-k)} \\underset{\\mathcal{H}_0}{\\sim} \\mathcal{F}(q,n-k) \\] et la zone de rejet est donnée par \\[ \\mathcal{R} =\\{\\tilde F \\geq f_{1-\\alpha,q,n-k}\\}. \\] Ces deux tests sont identiques. Proof. Nous allons prouver le résultat du test de Fisher, en particulier que ces deux tests sont équivalents. Montrons que \\(\\tilde{F} \\underset{\\mathcal{H}_0}{\\sim} \\mathcal{F}(q,n-k)\\) L’application \\(C : \\mathbb{R}^k \\to \\mathbb{R}^q\\) est surjective car \\(rg(C)=q\\) par hypothèse. On a \\(C\\widehat{\\theta} \\sim \\mathcal{N}_q(C\\theta,\\sigma^2 \\Delta)\\) avec \\(\\Delta = C(X&#39;X)^{-1}C&#39;\\). La matrice \\((X&#39;X)^{-1}\\) étant inversible, elle peut s’écrire sous la forme \\(AA&#39;\\) où \\(A\\in\\mathcal{M}_k(\\mathbb{R})\\) inversible. De plus, \\(rg(\\Delta)=rg(CAA&#39;C&#39;)=rg(A&#39;C&#39;) = q - dim(Ker(A&#39;C&#39;))\\). Or \\(A&#39;C&#39;x=0_k \\Leftrightarrow C&#39;x=0_k \\Rightarrow x=0_q\\) car \\(A\\) inversible et \\(C&#39;\\) injective. Ainsi \\(rg(\\Delta)=q\\) et \\(\\Delta = (CA)(CA)&#39;\\in\\mathcal{M}_q(\\mathbb{R})\\). \\(\\Delta\\) étant inversible, elle se décompose en \\(\\Delta=BB&#39;\\) avec \\(B\\in\\mathcal{M}_q(\\mathbb{R})\\) inversible. Sous \\(\\mathcal{H}_0\\), \\(C\\widehat{\\theta}\\sim \\mathcal{N}_q(0_q,\\sigma^2 \\Delta)\\) donc \\(B^{-1} C\\widehat{\\theta} \\sim\\mathcal{N}_q(0_q,\\sigma^2 I_q)\\). On en déduit donc que \\([B^{-1} C\\widehat{\\theta}]&#39; [B^{-1} C\\widehat{\\theta}] / \\sigma^2 \\sim \\chi^2(q).\\) De plus \\(SCR= (n-k) \\widehat{\\sigma^2}\\sim \\sigma^2 \\chi^2(n-k)\\) et \\(\\widehat{\\theta}\\) et \\(\\widehat{\\sigma^2}\\) sont indépendants. On en conclut que \\[ \\frac{[B^{-1} C\\widehat{\\theta}]&#39; [B^{-1} C\\widehat{\\theta} ] /q}{SCR/(n-k)} = \\frac{[C\\widehat{\\theta}]&#39; [C(X&#39;X)^{-1}C&#39;]^{-1} [C\\widehat{\\theta} ] /q}{SCR/(n-k)} \\underset{\\mathcal{H}_0}{\\sim} \\mathcal{F}(q,n-k). \\] Montrons que \\(F=\\tilde F\\) Tout d’abord, \\[\\begin{eqnarray*} \\|Y - X_0 \\widehat{\\beta}\\|^2 &amp;=&amp; \\underset{\\beta\\in\\mathbb{R}^q}{\\min} \\|Y - X_0 \\beta\\|^2\\\\ &amp;=&amp; \\underset{u\\in Im(X_0)}{\\min} \\|Y - u\\|^2\\\\ &amp;=&amp; \\underset{u\\in X (Ker(C)) }{\\min} \\|Y - u\\|^2\\\\ &amp;=&amp; \\underset{\\theta\\in Ker(C) }{\\min} \\|Y - X\\theta\\|^2\\\\ &amp;=&amp;\\|Y - X \\tilde \\theta\\|^2. \\end{eqnarray*}\\] Le vecteur \\(\\tilde \\theta\\) minimise \\(\\|Y - X\\theta\\|^2\\) sous la contrainte \\(\\theta\\in Ker(C)\\). Soit \\(\\lambda \\in\\mathbb{R}^q\\). Pour déterminer \\(\\tilde \\theta\\) on résout, \\[\\begin{eqnarray*} &amp;&amp; \\frac{\\partial }{\\partial \\theta} [(Y-X\\theta)&#39; (Y-X\\theta) + \\lambda&#39; C\\theta ] = 0_k\\\\ \\Leftrightarrow&amp;&amp; \\frac{\\partial }{\\partial \\theta} [Y&#39;Y - \\theta&#39;X&#39;Y - Y&#39;X\\theta + \\theta&#39; X&#39; X \\theta + \\lambda&#39; C\\theta ] = 0_k\\\\ \\Leftrightarrow&amp; &amp;-2X&#39;Y + 2 X&#39; X \\theta + C&#39;\\lambda =0_k \\end{eqnarray*}\\] donc \\(\\tilde \\theta = (X&#39;X)^{-1}X&#39;Y - \\frac 1 2 (X&#39;X)^{-1} C&#39;\\lambda\\). En utilisant la contrainte \\(C \\tilde \\theta=0_q\\), on obtient que \\(\\frac 1 2 \\lambda = \\Delta^{-1} C(X&#39;X)^{-1} X&#39;Y\\) car \\(\\Delta\\) est inversible. Finalement, \\(\\tilde \\theta = (X&#39;X)^{-1}X&#39;Y - (X&#39;X)^{-1} C&#39; \\Delta^{-1} C (X&#39;X)^{-1} X&#39;Y = \\widehat{\\theta} - (X&#39;X)^{-1} C&#39; \\Delta^{-1} C\\widehat{\\theta}\\). Ainsi \\[\\begin{eqnarray*} \\|X \\widehat{\\theta} - X_0 \\widehat{\\beta}\\|^2 &amp;=&amp; \\|X \\widehat{\\theta} - X \\tilde \\theta\\|^2\\\\ &amp;=&amp; \\|X(X&#39;X)^{-1} C&#39; \\Delta^{-1}C \\widehat{\\theta}\\|^2\\\\ &amp;=&amp; (C \\widehat{\\theta})&#39; \\Delta^{-1} C(X&#39;X)^{-1} X&#39;X (X&#39;X)^{-1} C&#39; \\Delta^{-1} (C \\widehat{\\theta})\\\\ &amp;=&amp; (C \\widehat{\\theta})&#39; \\Delta^{-1}(C \\widehat{\\theta}). \\end{eqnarray*}\\] Montrons que \\(q=k-k_0\\) Soit \\((e_1,\\ldots,e_{k-q})\\) une base de \\(Ker(C)\\) donc \\((Xe_1,\\ldots,Xe_{k-q})\\) est une famille génératrice de \\(X(Ker(C))\\). On montre ensuite facilement que c’est une famille libre car \\(X\\) est injective. Ainsi \\(dim(X(Ker(C))) = dim(Im(X_0))=k-q=k_0\\). B.2 Preuve de la proposition 7.5 Proof. On considère un modèle d’ANOVA à deux facteurs de la forme générale \\[ Y = X \\theta + \\varepsilon = (\\mathbb{1}_n, X_{(\\alpha)}, X_{(\\beta)}, X_{(\\gamma)}) \\theta + \\varepsilon \\] avec \\(\\theta=(\\mu,\\alpha,\\beta,\\gamma)&#39;\\), \\(\\alpha=(\\alpha_1,\\ldots,\\alpha_I)\\), \\(\\beta=(\\beta_1,\\ldots,\\beta_J)\\) et \\(\\gamma=(\\gamma_{11}, \\ldots, \\gamma_{IJ})\\). On considère les sous-espaces vectoriels suivants de \\(\\mathbb{R}^n\\) : \\[ \\begin{array}{l} E_\\mu = Vect(\\mathbb{1}_n) \\\\ E_\\alpha=\\{X_{(\\alpha)} \\alpha; \\sum_{i=1}^I n_{i+}\\alpha_i =0\\} \\\\ E_\\beta=\\{X_{(\\beta)} \\beta; \\sum_{j=1}^J n_{+j}\\beta_j =0\\} \\\\ E_\\gamma=\\{X_{(\\gamma)} \\gamma; \\sum_{i=1}^I n_{ij}\\gamma_{ij} =\\sum_{j=1}^J n_{ij}\\gamma_{ij}=0\\} \\end{array} \\] On introduit les ensembles \\(A_{(\\alpha)}=\\{\\alpha;\\ \\sum_{i=1}^I n_{i+} \\alpha_i =0\\}\\) et \\(A_{(\\beta)}=\\{\\beta;\\ \\sum_{j=1}^J n_{+j} \\beta_j =0\\}\\) Commençons par caractériser que \\(E_\\mu\\), \\(E_\\alpha\\), \\(E_\\beta\\) et \\(E_\\gamma\\) sont orthogonaux : \\ soit \\(v^{(\\mu)}\\in E_\\mu\\), \\(v^{(\\alpha)}\\in E_\\alpha\\), \\(v^{(\\beta)}\\in E_\\beta\\) et \\(v^{(\\gamma)}\\in E_\\gamma\\). On a donc \\[ \\begin{array}{l} &lt;v^{(\\mu)},v^{(\\alpha)} &gt; =\\sum_{i,j,\\ell} \\mu \\alpha_i = \\mu \\sum_{i=1}^I n_{i+} \\alpha_i = 0 \\\\ &lt;v^{(\\mu)},v^{(\\beta)} &gt; =\\sum_{i,j,\\ell} \\mu \\beta_j = \\mu \\sum_{j=1}^J n_{+j} \\beta_j = 0 \\\\ &lt;v^{(\\mu)},v^{(\\gamma)} &gt; =\\sum_{i,j,\\ell} \\mu \\gamma_{ij} = \\mu \\sum_{i=1}^I \\sum_{j=1}^J n_{ij} \\gamma_{ij} = 0 \\\\ &lt;v^{(\\alpha)},v^{(\\gamma)} &gt; =\\sum_{i,j,\\ell} \\alpha_i \\gamma_{ij} = \\sum_{i=1}^I \\alpha_i (\\sum_{j=1}^J n_{ij} \\gamma_{ij}) = 0 \\\\ &lt;v^{(\\beta)},v^{(\\gamma)} &gt; =\\sum_{i,j,\\ell} \\beta_j \\gamma_{ij} = \\sum_{j=1}^J \\beta_j (\\sum_{i=1}^In_{ij} \\gamma_{ij}) = 0 \\\\ &lt;v^{(\\alpha)},v^{(\\beta)} &gt; =\\sum_{i,j,\\ell} \\alpha_i \\beta_{j} = \\sum_{i=1}^I \\sum_{j=1}^J n_{ij} \\alpha_i \\beta_{j} \\end{array} \\] On remarque que si \\(n_{ij}= n_{i+} n_{+j} / n\\), alors \\[ &lt;v^{(\\alpha)},v^{(\\beta)} &gt; = \\sum_{i=1}^I \\frac{n_{i+}}{n} \\alpha_i \\left(\\sum_{j=1}^J n_{+j} \\beta_{j} \\right) =0. \\] Réciproquement, supposons que \\(E_\\alpha\\) et \\(E_\\beta\\) sont orthogonaux : \\[\\begin{equation} \\tag{B.1} \\sum_{i=1}^I \\sum_{j=1}^J n_{ij} \\alpha_i \\beta_{j} = 0,\\ \\forall \\alpha\\in A_{(\\alpha)},\\ \\forall \\beta\\in A_{(\\beta)}. \\end{equation}\\] Fixons \\(\\alpha\\). est vrai pour tout \\(\\beta\\in A_{(\\beta)}\\) et \\(\\sum_{j=1}^J n_{+j} \\beta_j=0\\) donc \\[ \\sum_{j=1}^J \\left(\\sum_{i=1}^I n_{ij} \\alpha_i \\right) \\beta_{j} = 0 = \\sum_{j=1}^J n_{+j} \\beta_j. \\] Ainsi, \\(\\sum_{i=1}^I n_{ij} \\alpha_i = c_j n_{+j}\\) où \\(c_j\\) constante pour \\(j=1,\\ldots,J\\). En sommant sur \\(j\\), \\[ \\sum_{j=1}^J \\left(\\sum_{i=1}^I n_{ij} \\alpha_i \\right) = \\sum_{j=1}^J c_j n_{+j} = \\sum_{i=1}^I n_{i+} \\alpha_i =0 \\] d’où \\(c_j=0\\) pour tout \\(j\\) donc \\(\\sum_{i=1}^I n_{ij} \\alpha_i =0\\). A nouveau, pour tout \\(\\alpha\\in A_{(\\alpha)}\\) et pour tout \\(j\\), \\[ \\sum_{i=1}^I n_{ij} \\alpha_i = 0 = \\sum_{i=1}^I n_{i+} \\alpha_i \\] donc \\(n_{ij}\\) et \\(n_{i+}\\) sont proportionnels pour tout \\(i\\) : \\[n_{ij} = d_j n_{i+} \\textrm{ avec } d_j \\textrm{ constante}.\\] Ainsi ${i=1}^I n{ij} = {i=1}^I d_j n{i+} = c_j n = n_{+j} $ d’où \\(d_j = n_{+j}/n\\) et \\(n_{ij} = (n_{+j}/n ) n_{i+}\\). B.3 Preuve de la proposition 6.2 Proof. \\[\\begin{eqnarray*} \\mathcal{R}(m,m^\\star) &amp;=&amp; \\mathbb{E}\\left[\\|X_{(m)} \\hat\\theta_{(m)} - \\mu^\\star\\|^2\\right] \\\\ &amp;=&amp; \\mathbb{E}\\left[\\|X_{(m)} \\hat\\theta_{(m)} - \\mu^\\star_{(m)} + \\mu^\\star_{(m)} - \\mu^\\star\\|^2\\right] \\hspace*{0.5cm} \\textrm{ avec } \\mu^\\star_{(m)}=P_{[X_{(m)}]}\\mu^\\star\\\\ &amp;=&amp;\\mathbb{E}\\left[\\|X_{(m)} \\hat\\theta_{(m)} - \\mu^\\star_{(m)}\\|^2\\right] + \\mathbb{E}\\left[\\|\\mu^\\star_{(m)} - \\mu^\\star\\|^2\\right] \\hspace*{0.5cm} \\textrm{ par Pythagore }\\\\ &amp;=&amp;\\mathbb{E}\\left[\\|X_{(m)} \\hat\\theta_{(m)} - \\mu^\\star_{(m)}\\|^2\\right] + \\|\\mu^\\star_{(m)} - \\mu^\\star\\|^2. \\end{eqnarray*}\\] Or \\[ X_{(m)} \\hat\\theta_{(m)} = P_{[X_{(m)}]} Y = P_{[X_{(m)}]} \\left(X_{(m^\\star)} \\theta_{(m^\\star)} + \\varepsilon_{(m^\\star)}\\right) = \\mu^\\star_{(m)} + P_{[X_{(m)}]} \\varepsilon_{(m^\\star)}, \\] donc \\[ \\|X_{(m)} \\hat\\theta_{(m)} - \\mu^\\star_{(m)}\\|^2 = \\|P_{[X_{(m)}]} \\varepsilon_{(m^\\star)}\\|^2 \\sim (\\sigma{^\\star})^2 \\chi^2(|m|+1) \\] d’après le théorème de Cochran. Ainsi \\(\\mathbb{E}\\left[\\|X_{(m)} \\hat\\theta_{(m)} - \\mu^\\star_{(m)}\\|^2\\right] = (\\sigma{^\\star})^2 (|m|+1)\\). B.4 Preuve de la proposition 6.3 Proof. Sous le modèle \\(m^\\star\\), la densité de \\(Y=(Y_1,\\ldots,Y_n)&#39;\\) vaut \\[ f^\\star(Y) = (2\\pi\\sigma^{\\star\\,2})^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^{\\star\\,2}} \\|Y - \\mu^\\star\\|^2\\right). \\] Sous le modèle \\(m\\), la densité de \\(Y\\) vaut \\[ f_{(m)}(Y) = (2\\pi\\sigma_{(m)}^{2})^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma_{(m)}^{2}} \\|Y - \\mu_{(m)}\\|^2\\right). \\] Ainsi \\[ \\ln\\left(\\frac{f^\\star(Y)}{f_{(m)}(Y)}\\right) = \\frac n 2 \\ln\\left(\\frac{\\sigma_{(m)}^2}{\\sigma^{\\star\\,2}}\\right) + \\frac{\\|Y - \\mu_{(m)}\\|^2}{2 \\sigma_{(m)}^2} - \\frac{\\|Y - \\mu^\\star\\|^2}{2 \\sigma^{\\star\\,2}}. \\] D’où \\[\\begin{eqnarray*} KL(m^\\star,m) &amp;=&amp; \\mathbb{E}_{f^\\star}\\left[\\ln\\left(\\frac{f^\\star(Y)}{f_{(m)}(Y)}\\right)\\right]\\\\ &amp;=&amp; \\frac n 2 \\ln\\left(\\frac{\\sigma_{(m)}^2}{\\sigma^{\\star\\,2}}\\right) + \\frac{1}{2 \\sigma_{(m)}^2} \\mathbb{E}_{f^\\star}\\left[\\|Y - \\mu_{(m)}\\|^2\\right] - \\frac{1}{2 \\sigma^{\\star\\,2}} \\mathbb{E}_{f^\\star}\\left[ \\|Y - \\mu^\\star\\|^2\\right]. \\end{eqnarray*}\\] Or \\(\\mathbb{E}_{f^\\star}\\left[ \\|Y - \\mu^\\star\\|^2\\right] = \\mathbb{E}_{f^\\star}\\left[ \\|\\varepsilon^\\star\\|^2\\right] = n \\sigma^{\\star\\,2}\\) et \\[\\begin{eqnarray*} \\mathbb{E}_{f^\\star}\\left[\\|Y - \\mu_{(m)}\\|^2\\right] &amp;=&amp; \\mathbb{E}_{f^\\star}\\left[\\|Y - \\mu^\\star + \\mu^\\star - \\mu_{(m)}\\|^2\\right] \\\\ &amp;=&amp; \\mathbb{E}_{f^\\star}\\left[\\|Y - \\mu^\\star \\|^2 \\right] + \\|\\mu^\\star - \\mu_{(m)}\\|^2 + 2 \\mathbb{E}_{f^\\star}\\left[(\\mu^\\star - \\mu_{(m)})&#39; (Y - \\mu^\\star)\\right]\\\\ &amp;=&amp; n\\sigma^{\\star\\,2} + \\|\\mu^\\star - \\mu_{(m)}\\|^2 \\end{eqnarray*}\\] car \\(\\mathbb{E}_{f^\\star}\\left[Y\\right] = \\mu^\\star\\). Finalement, on obtient que \\[\\begin{eqnarray*} KL(m^\\star,m) &amp;=&amp; \\frac n 2 \\ln\\left(\\frac{\\sigma_{(m)}^2}{\\sigma^{\\star\\,2}}\\right) + \\frac{n\\sigma^{\\star\\,2} + \\|\\mu^\\star - \\mu_{(m)}\\|^2 }{2 \\sigma_{(m)}^2} - \\frac{n\\sigma^{\\star\\,2} }{2 \\sigma^{\\star\\,2}} \\\\ &amp;=&amp; \\frac n 2 \\left[ \\ln\\left(\\frac{\\sigma_{(m)}^2}{\\sigma^{\\star\\,2}}\\right) + \\frac{\\sigma^{\\star\\,2} }{ \\sigma_{(m)}^2} -1 \\right] + \\frac{ \\|\\mu^\\star - \\mu_{(m)}\\|^2 }{2 \\sigma_{(m)}^2}. \\end{eqnarray*}\\] B.5 Critère du \\(C_p\\) de Mallows Soit \\(m\\in \\mathcal{M}\\) fixé. On rappelle que d’après la proposition 6.2, le risque quadratique entre \\(m\\) et \\(m^\\star\\) vaut : \\[\\mathcal{R}(m,m^\\star)= \\|\\mu^\\star_{(m)} - \\mu^\\star\\|^2 + (\\sigma{^\\star})^2 (|m|+1).\\] Commençons par essayer d’estimer le terme de biais. D’après le théorème de Pythagore et le théorème de Cochran, on a : \\[\\begin{eqnarray*} \\mathbb{E}\\left[ \\| Y - \\widehat{Y}_{(m)} \\|^2\\right] &amp; = &amp; \\mathbb{E}\\left[\\| Y - \\mu_{(m)}^{\\star} \\|^2\\right] - \\mathbb{E}\\left[ \\| \\widehat{Y}_{(m)} - \\mu_{(m)}^{\\star}\\|^2\\right], \\\\ &amp; = &amp; \\mathbb{E}\\left[\\|Y-\\mu^{\\star}+\\mu^{\\star}-\\mu_{(m)}^\\star\\|^2\\right]- \\mathbb{E}\\left[ \\| \\widehat{Y}_{(m)} - \\mu_{(m)}^{\\star} \\|^2\\right], \\\\ &amp; = &amp; \\mathbb{E}\\left[\\| Y - \\mu^{\\star} \\|^2\\right] +\\| \\mu^{\\star} - \\mu_{(m)}^{\\star} \\|^2 - (|m|+1) \\sigma^{\\star\\,2},\\\\ &amp; = &amp; \\| \\mu^\\star - \\mu_{(m)}^\\star \\|^2 + n\\sigma^{\\star\\,2} - (|m|+1) \\sigma^{\\star\\,2}, \\end{eqnarray*}\\] ou encore \\[\\begin{equation} \\| \\mu^\\star- \\mu_{(m)}^\\star \\|^2 = \\mathbb{E}\\left[\\left\\| Y - \\widehat{Y}_{(m)} \\right\\|^2\\right] + (|m|+1) \\sigma^{\\star\\,2} - n\\sigma^{\\star\\,2}. \\tag{B.2} \\end{equation}\\] D’après (B.2), le terme de biais \\(\\| \\mu^\\star - \\mu_{(m)}^\\star \\|^2\\) peut donc ^etre estimé par \\(\\| Y - \\widehat{Y}_{(m)} \\|^2 + (|m|+1) \\sigma^{\\star\\,2}\\) (on néglige le terme en \\(n\\sigma^{\\star\\,2}\\) puisque ce dernier ne dépend pas de \\(m\\) et n’interviendra donc pas dans la minimisation). Si la variance est connue, on obtient alors le critère : \\[ C_p(m)= \\| Y - \\widehat{Y}_{(m)} \\|^2 + 2|m| \\sigma^{\\star\\,2}.\\] On retiendra alors le modèle \\(\\hat m_{CP}\\) vérifiant : \\[ \\hat m_{CP} = \\mathrm{arg} \\min_{m\\in \\mathcal{M}} C_p(m).\\] Dans le cas où la variance est inconnue, on utilisera l’estimateur \\(\\widehat{\\sigma^2} = \\widehat{\\sigma^2}_{(m_p)}\\) où \\(m_p=\\lbrace 1,\\dots, p \\rbrace\\) est le modèle prenant en compte tous les régresseurs. B.6 Preuve de la proposition ?? Dans le cas d’une famille exponentielle, \\[ l(\\underline{Y};\\theta) = \\underset{i=1}{\\stackrel{n}{\\sum}} \\left\\{ \\frac{Y_i \\omega_i - b(\\omega_i)}{\\gamma(\\phi)} + c(Y_i,\\phi) \\right\\} = \\underset{i=1}{\\stackrel{n}{\\sum}} \\ell_i \\] avec \\(\\mu_i=b&#39;(\\omega_i)\\), \\(\\eta_i=g(\\mu_i)=\\textbf{x}_i \\theta\\), \\(Var(Y_i)=b&#39;&#39;(\\omega_i) \\gamma(\\phi)\\). Calculons \\[ \\frac{\\partial\\ell_i}{\\partial\\theta_j}=\\frac{\\partial\\ell_i}{\\partial\\omega_i}\\frac{\\partial\\omega_i}{\\partial\\mu_i}\\frac{\\partial\\mu_i}{\\partial\\eta_i}\\frac{\\partial\\eta_i}{\\partial\\theta_j} : \\] Comme \\[\\begin{eqnarray*} \\frac{\\partial\\ell_i}{\\partial\\omega_i}&amp;=&amp;[Y_i-b&#39;(\\omega_i)]/\\gamma(\\phi)=(Y_i-\\mu_i)/\\gamma(\\phi),\\\\ \\frac{\\partial\\omega_i}{\\partial\\mu_i}&amp;=&amp;1/b&#39;&#39;(\\omega_i)=\\gamma(\\phi) / \\text{Var}(Y_i),\\\\ \\frac{\\partial\\eta_i}{\\partial\\theta_j}&amp;=&amp;x_{i}^{(j)} \\quad \\text{car}\\quad \\eta_i=\\mathbf{x}_i\\bs{\\theta},\\\\ \\frac{\\partial\\mu_i}{\\partial\\eta_i}&amp;\\quad&amp; \\text{dépend de la fonction lien}\\quad\\eta_i=g(\\mu_i), \\end{eqnarray*}\\] on obtient que \\[ S_j = \\frac{\\partial l(\\underline{Y}; \\theta)}{\\partial \\theta_j} = \\sum_{i=1}^n\\frac{(Y_i-\\mu_i)x_{i}^{(j)}}{\\text{Var}(Y_i)}\\ \\frac{\\partial\\mu_i}{\\partial\\eta_i}\\quad \\forall j=0,\\ldots,p. \\] "],["references.html", "References", " References Exercise 2.1 Soit \\(Y_i = \\theta_0 + \\theta_1 x^{(1)}_i+\\cdots + \\theta_p x^{(p)}_i+\\varepsilon_i,\\ \\forall i=1,\\ldots,n\\) avec \\(\\varepsilon_1, \\cdots, \\varepsilon_n\\) i.i.d de loi \\(\\mathcal{N}(0,\\sigma^2)\\). Quelle est la loi de \\(Y_i\\) ? Quelle est la loi de \\(Y\\) ? Correction de #exoloiY On a \\(Y_i=\\theta_0 + \\theta_1 x^{(1)}_i+\\cdots + \\theta_p x^{(p)}_i+\\varepsilon_i=X_i \\theta + \\varepsilon_i\\) et \\(\\varepsilon_i\\sim\\mathcal{N}(0,\\sigma^2)\\) donc \\(Y_i\\sim \\mathcal{N}(X_i\\theta,\\sigma^2)\\). \\(\\varepsilon_1, \\cdots, \\varepsilon_n\\) i.i.d de loi \\(\\mathcal{N}(0,\\sigma^2)\\) donc \\(\\varepsilon=(\\varepsilon_1, \\cdots, \\varepsilon_n)\\sim\\mathcal{N}_n(0_n,\\sigma^2 I_n)\\). Ainsi \\(Y=X\\theta+\\varepsilon \\sim\\mathcal{N}_n(X\\theta,\\sigma^2 I_n)\\). Exercise 2.2 Afin d’écrire sous forme matricielle ce modèle, les observations sont rangées par modalité du facteur \\[Y=(Y_{11}, \\cdots, Y_{1,n_1}, Y_{2,1},\\cdots ,Y_{2n_2}, \\cdots,Y_{I1},\\cdots, Y_{In_I})&#39;.\\] Soit \\(\\displaystyle n=\\sum_{i=1}^In_i\\). Ecrivez le modèle (2.2) sous la forme \\(Y = X\\theta + \\varepsilon\\) en précisant la matrice de design \\(X\\in\\mathcal{M}_{nI}(\\mathbb{R})\\) et \\(\\theta\\in\\mathbb{R}^I\\). Quelle est la loi de \\(Y_{ij}\\), de \\(Y_i=(Y_{i1},\\ldots,Y_{in_i})&#39;\\) et de \\(Y\\) ? Exercise 3.1 L’exercice suivant vous guide pour déontrer les points clés du théorème 3.2. Montrez que \\(\\mathbb{E}\\left [\\widehat{\\theta}\\right]= \\theta\\) (rappel : \\(\\mathbb{E}\\left [Y\\right]= X \\theta\\)) Montrez que \\(\\mathrm{Var}\\left(\\widehat{\\theta}\\right)= \\sigma^2 (X&#39;X)^{-1}\\) (rappel : \\(\\mathrm{Var}(AY) = A \\mathrm{Var}(Y) A&#39;\\)) Pourquoi \\(\\widehat\\theta\\) est un vecteur gaussien ? "]]
