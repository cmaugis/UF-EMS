<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapitre 5 Modèles singuliers, orthogonalité et importance des hypothèses sur les erreurs | Modèle linéaire général et modèle linéaire généralisé</title>
  <meta name="description" content="Chapitre 5 Modèles singuliers, orthogonalité et importance des hypothèses sur les erreurs | Modèle linéaire général et modèle linéaire généralisé" />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapitre 5 Modèles singuliers, orthogonalité et importance des hypothèses sur les erreurs | Modèle linéaire général et modèle linéaire généralisé" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapitre 5 Modèles singuliers, orthogonalité et importance des hypothèses sur les erreurs | Modèle linéaire général et modèle linéaire généralisé" />
  
  
  

<meta name="author" content="Cathy Maugis-Rabusseau (INSA Toulouse / IMT)" />


<meta name="date" content="2021-10-28" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="Test.html"/>
<link rel="next" href="regression.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<link href="libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet" />
<script src="libs/pagedtable-1.1/js/pagedtable.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">UF Elements de modélisation statistique</a></li>
<li>      <img src="image/LogoInsaToulouse.jpg" height="20px" align="right"/>      </li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Préface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#modélisation-dune-réponse-quantitative"><i class="fa fa-check"></i><b>1.1</b> Modélisation d’une réponse quantitative</a><ul>
<li class="chapter" data-level="1.1.1" data-path="intro.html"><a href="intro.html#jeu-de-données-illustratif"><i class="fa fa-check"></i><b>1.1.1</b> Jeu de données illustratif</a></li>
<li class="chapter" data-level="1.1.2" data-path="intro.html"><a href="intro.html#régression-linéaire"><i class="fa fa-check"></i><b>1.1.2</b> Régression linéaire</a></li>
<li class="chapter" data-level="1.1.3" data-path="intro.html"><a href="intro.html#analyse-de-la-variance-anova"><i class="fa fa-check"></i><b>1.1.3</b> Analyse de la variance (ANOVA)</a></li>
<li class="chapter" data-level="1.1.4" data-path="intro.html"><a href="intro.html#analyse-de-covariance-ancova"><i class="fa fa-check"></i><b>1.1.4</b> Analyse de covariance (ANCOVA)</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#modélisation-dune-variable-binaire-de-comptage"><i class="fa fa-check"></i><b>1.2</b> Modélisation d’une variable binaire, de comptage, …</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#objectifs-du-cours"><i class="fa fa-check"></i><b>1.3</b> Objectifs du cours</a></li>
</ul></li>
<li class="part"><span><b>I Le modèle linéaire général</b></span></li>
<li class="chapter" data-level="2" data-path="DefML.html"><a href="DefML.html"><i class="fa fa-check"></i><b>2</b> Définitions générales</a><ul>
<li class="chapter" data-level="2.1" data-path="DefML.html"><a href="DefML.html#modlinreg"><i class="fa fa-check"></i><b>2.1</b> Modèle linéaire régulier</a></li>
<li class="chapter" data-level="2.2" data-path="DefML.html"><a href="DefML.html#exemples-de-modèle-linéaire-gaussien"><i class="fa fa-check"></i><b>2.2</b> Exemples de modèle linéaire gaussien</a><ul>
<li class="chapter" data-level="2.2.1" data-path="DefML.html"><a href="DefML.html#le-modèle-de-régression-linéaire"><i class="fa fa-check"></i><b>2.2.1</b> Le modèle de régression linéaire</a></li>
<li class="chapter" data-level="2.2.2" data-path="DefML.html"><a href="DefML.html#le-modèle-danalyse-de-la-variance"><i class="fa fa-check"></i><b>2.2.2</b> Le modèle d’analyse de la variance</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="DefML.html"><a href="DefML.html#en-résumé"><i class="fa fa-check"></i><b>2.3</b> En résumé</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="EstML.html"><a href="EstML.html"><i class="fa fa-check"></i><b>3</b> Estimation des paramètres</a><ul>
<li class="chapter" data-level="3.1" data-path="EstML.html"><a href="EstML.html#estimation-de-theta"><i class="fa fa-check"></i><b>3.1</b> Estimation de <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="3.2" data-path="EstML.html"><a href="EstML.html#valeurs-ajustées-et-résidus"><i class="fa fa-check"></i><b>3.2</b> Valeurs ajustées et résidus</a></li>
<li class="chapter" data-level="3.3" data-path="EstML.html"><a href="EstML.html#estimation-de-sigma2"><i class="fa fa-check"></i><b>3.3</b> Estimation de <span class="math inline">\(\sigma^2\)</span></a></li>
<li class="chapter" data-level="3.4" data-path="EstML.html"><a href="EstML.html#erreurs-standards"><i class="fa fa-check"></i><b>3.4</b> Erreurs standards</a></li>
<li class="chapter" data-level="3.5" data-path="EstML.html"><a href="EstML.html#intervalle-de-confiance-de-theta_j-de-xtheta_i-et-de-x_0theta"><i class="fa fa-check"></i><b>3.5</b> Intervalle de confiance de <span class="math inline">\(\theta_j\)</span>, de <span class="math inline">\((X\theta)_i\)</span> et de <span class="math inline">\(X_0\theta\)</span></a><ul>
<li class="chapter" data-level="3.5.1" data-path="EstML.html"><a href="EstML.html#ICthetaj"><i class="fa fa-check"></i><b>3.5.1</b> Intervalle de confiance de <span class="math inline">\(\theta_j\)</span></a></li>
<li class="chapter" data-level="3.5.2" data-path="EstML.html"><a href="EstML.html#ICXthetai"><i class="fa fa-check"></i><b>3.5.2</b> Intervalle de confiance de <span class="math inline">\((X\theta)_i\)</span></a></li>
<li class="chapter" data-level="3.5.3" data-path="EstML.html"><a href="EstML.html#ICX0theta"><i class="fa fa-check"></i><b>3.5.3</b> Intervalle de confiance de <span class="math inline">\(X_0\theta\)</span></a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="EstML.html"><a href="EstML.html#ICpredit"><i class="fa fa-check"></i><b>3.6</b> Intervalles de prédiction</a></li>
<li class="chapter" data-level="3.7" data-path="EstML.html"><a href="EstML.html#qualité-dajustement"><i class="fa fa-check"></i><b>3.7</b> Qualité d’ajustement</a></li>
<li class="chapter" data-level="3.8" data-path="EstML.html"><a href="EstML.html#en-résumé-1"><i class="fa fa-check"></i><b>3.8</b> En résumé</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="Test.html"><a href="Test.html"><i class="fa fa-check"></i><b>4</b> Test de Fisher-Snedecor</a><ul>
<li class="chapter" data-level="4.1" data-path="Test.html"><a href="Test.html#hypothèses-testées"><i class="fa fa-check"></i><b>4.1</b> Hypothèses testées</a><ul>
<li class="chapter" data-level="4.1.1" data-path="Test.html"><a href="Test.html#première-écriture"><i class="fa fa-check"></i><b>4.1.1</b> Première écriture</a></li>
<li class="chapter" data-level="4.1.2" data-path="Test.html"><a href="Test.html#seconde-écriture"><i class="fa fa-check"></i><b>4.1.2</b> Seconde écriture</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="Test.html"><a href="Test.html#le-test-de-fisher-snedecor"><i class="fa fa-check"></i><b>4.2</b> Le test de Fisher-Snedecor</a><ul>
<li class="chapter" data-level="4.2.1" data-path="Test.html"><a href="Test.html#principe"><i class="fa fa-check"></i><b>4.2.1</b> Principe</a></li>
<li class="chapter" data-level="4.2.2" data-path="Test.html"><a href="Test.html#comblinconjointes"><i class="fa fa-check"></i><b>4.2.2</b> La statistique de test</a></li>
<li class="chapter" data-level="4.2.3" data-path="Test.html"><a href="Test.html#règle-de-décision"><i class="fa fa-check"></i><b>4.2.3</b> Règle de décision</a></li>
<li class="chapter" data-level="4.2.4" data-path="Test.html"><a href="Test.html#comblin"><i class="fa fa-check"></i><b>4.2.4</b> Cas particulier où <span class="math inline">\(q=1\)</span> : Test de Student</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="Test.html"><a href="Test.html#intervalle-région-de-confiance-pour-ctheta"><i class="fa fa-check"></i><b>4.3</b> Intervalle (région) de confiance pour <span class="math inline">\(C\theta\)</span></a><ul>
<li class="chapter" data-level="4.3.1" data-path="Test.html"><a href="Test.html#ic-pour-ctheta-in-mathbbr"><i class="fa fa-check"></i><b>4.3.1</b> IC pour <span class="math inline">\(C\theta \in \mathbb{R}\)</span></a></li>
<li class="chapter" data-level="4.3.2" data-path="Test.html"><a href="Test.html#région-de-confiance-pour-ctheta-in-mathbbrq"><i class="fa fa-check"></i><b>4.3.2</b> Région de confiance pour <span class="math inline">\(C\theta \in \mathbb{R}^q\)</span></a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="Test.html"><a href="Test.html#en-résumé-2"><i class="fa fa-check"></i><b>4.4</b> En résumé</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="singulier.html"><a href="singulier.html"><i class="fa fa-check"></i><b>5</b> Modèles singuliers, orthogonalité et importance des hypothèses sur les erreurs</a><ul>
<li class="chapter" data-level="5.1" data-path="singulier.html"><a href="singulier.html#quand-h1-h4-ne-sont-pas-respectées"><i class="fa fa-check"></i><b>5.1</b> Quand H1-H4 ne sont pas respectées…</a><ul>
<li class="chapter" data-level="5.1.1" data-path="singulier.html"><a href="singulier.html#propriétés-de-lestimateur-des-moindres-carrés-widehattheta"><i class="fa fa-check"></i><b>5.1.1</b> Propriétés de l’estimateur des moindres carrés <span class="math inline">\(\widehat{\theta}\)</span></a></li>
<li class="chapter" data-level="5.1.2" data-path="singulier.html"><a href="singulier.html#propriétés-de-lestimateur-des-moindres-carrés-widehatsigma2"><i class="fa fa-check"></i><b>5.1.2</b> Propriétés de l’estimateur des moindres carrés <span class="math inline">\(\widehat{\sigma}^2\)</span></a></li>
<li class="chapter" data-level="5.1.3" data-path="singulier.html"><a href="singulier.html#modèles-avec-corrélations"><i class="fa fa-check"></i><b>5.1.3</b> Modèles avec corrélations</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="singulier.html"><a href="singulier.html#ModSingulier"><i class="fa fa-check"></i><b>5.2</b> Modèles singuliers</a><ul>
<li class="chapter" data-level="5.2.1" data-path="singulier.html"><a href="singulier.html#contraintes-didentifiabilité"><i class="fa fa-check"></i><b>5.2.1</b> Contraintes d’identifiabilité</a></li>
<li class="chapter" data-level="5.2.2" data-path="singulier.html"><a href="singulier.html#fonctions-estimables-et-contrastes"><i class="fa fa-check"></i><b>5.2.2</b> Fonctions estimables et contrastes</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="singulier.html"><a href="singulier.html#orthogonalité"><i class="fa fa-check"></i><b>5.3</b> Orthogonalité</a><ul>
<li class="chapter" data-level="5.3.1" data-path="singulier.html"><a href="singulier.html#orthogonalité-pour-les-modèles-réguliers"><i class="fa fa-check"></i><b>5.3.1</b> Orthogonalité pour les modèles réguliers</a></li>
<li class="chapter" data-level="5.3.2" data-path="singulier.html"><a href="singulier.html#orthogonalité-pour-les-modèles-non-réguliers"><i class="fa fa-check"></i><b>5.3.2</b> Orthogonalité pour les modèles non-réguliers</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="singulier.html"><a href="singulier.html#en-résumé-3"><i class="fa fa-check"></i><b>5.4</b> En résumé</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>6</b> La régression linéaire</a><ul>
<li class="chapter" data-level="6.1" data-path="regression.html"><a href="regression.html#introduction"><i class="fa fa-check"></i><b>6.1</b> Introduction</a><ul>
<li class="chapter" data-level="6.1.1" data-path="regression.html"><a href="regression.html#exemple-illustratif"><i class="fa fa-check"></i><b>6.1.1</b> Exemple illustratif</a></li>
<li class="chapter" data-level="6.1.2" data-path="regression.html"><a href="regression.html#problématique"><i class="fa fa-check"></i><b>6.1.2</b> Problématique</a></li>
<li class="chapter" data-level="6.1.3" data-path="regression.html"><a href="regression.html#le-modèle-de-régression-linéaire-simple"><i class="fa fa-check"></i><b>6.1.3</b> Le modèle de régression linéaire simple</a></li>
<li class="chapter" data-level="6.1.4" data-path="regression.html"><a href="regression.html#le-modèle-de-régression-linéaire-multiple"><i class="fa fa-check"></i><b>6.1.4</b> Le modèle de régression linéaire multiple</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="regression.html"><a href="regression.html#estimation"><i class="fa fa-check"></i><b>6.2</b> Estimation</a><ul>
<li class="chapter" data-level="6.2.1" data-path="regression.html"><a href="regression.html#résultats-généraux"><i class="fa fa-check"></i><b>6.2.1</b> Résultats généraux</a></li>
<li class="chapter" data-level="6.2.2" data-path="regression.html"><a href="regression.html#propriétés-en-régression-linéaire-simple"><i class="fa fa-check"></i><b>6.2.2</b> Propriétés en régression linéaire simple</a></li>
<li class="chapter" data-level="6.2.3" data-path="regression.html"><a href="regression.html#le-coefficient-r2"><i class="fa fa-check"></i><b>6.2.3</b> Le coefficient <span class="math inline">\(R^2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="regression.html"><a href="regression.html#tests-et-intervalles-de-confiance"><i class="fa fa-check"></i><b>6.3</b> Tests et intervalles de confiance</a><ul>
<li class="chapter" data-level="6.3.1" data-path="regression.html"><a href="regression.html#test-de-nullité-dun-paramètre-du-modèle"><i class="fa fa-check"></i><b>6.3.1</b> Test de nullité d’un paramètre du modèle</a></li>
<li class="chapter" data-level="6.3.2" data-path="regression.html"><a href="regression.html#test-de-nullité-de-quelques-paramètres-du-modèle"><i class="fa fa-check"></i><b>6.3.2</b> Test de nullité de quelques paramètres du modèle</a></li>
<li class="chapter" data-level="6.3.3" data-path="regression.html"><a href="regression.html#test-de-nullité-de-tous-les-paramètres-du-modèle"><i class="fa fa-check"></i><b>6.3.3</b> Test de nullité de tous les paramètres du modèle</a></li>
<li class="chapter" data-level="6.3.4" data-path="regression.html"><a href="regression.html#intervalle-de-confiance-de-theta_j-de-xtheta_i-et-de-x_0theta-1"><i class="fa fa-check"></i><b>6.3.4</b> Intervalle de confiance de <span class="math inline">\(\theta_j\)</span>, de <span class="math inline">\((X\theta)_i\)</span> et de <span class="math inline">\(X_0\theta\)</span></a></li>
<li class="chapter" data-level="6.3.5" data-path="regression.html"><a href="regression.html#intervalle-de-prédiction"><i class="fa fa-check"></i><b>6.3.5</b> Intervalle de prédiction</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="regression.html"><a href="regression.html#sélection-des-variables-explicatives"><i class="fa fa-check"></i><b>6.4</b> Sélection des variables explicatives</a><ul>
<li class="chapter" data-level="6.4.1" data-path="regression.html"><a href="regression.html#cadre-général-de-sélection-de-modèles"><i class="fa fa-check"></i><b>6.4.1</b> Cadre général de sélection de modèles</a></li>
<li class="chapter" data-level="6.4.2" data-path="regression.html"><a href="regression.html#quelques-critères-pour-sélectionner-un-modèle"><i class="fa fa-check"></i><b>6.4.2</b> Quelques critères pour sélectionner un modèle</a></li>
<li class="chapter" data-level="6.4.3" data-path="regression.html"><a href="regression.html#algorithmes-de-sélection-de-variables"><i class="fa fa-check"></i><b>6.4.3</b> Algorithmes de sélection de variables</a></li>
<li class="chapter" data-level="6.4.4" data-path="regression.html"><a href="regression.html#illustration-sur-lexemple"><i class="fa fa-check"></i><b>6.4.4</b> Illustration sur l’exemple</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="regression.html"><a href="regression.html#régression-linéaire-régularisée"><i class="fa fa-check"></i><b>6.5</b> Régression linéaire régularisée</a><ul>
<li class="chapter" data-level="6.5.1" data-path="regression.html"><a href="regression.html#régression-ridge"><i class="fa fa-check"></i><b>6.5.1</b> Régression ridge</a></li>
<li class="chapter" data-level="6.5.2" data-path="regression.html"><a href="regression.html#régression-lasso"><i class="fa fa-check"></i><b>6.5.2</b> Régression Lasso</a></li>
<li class="chapter" data-level="6.5.3" data-path="regression.html"><a href="regression.html#régression-elastic-net"><i class="fa fa-check"></i><b>6.5.3</b> Régression Elastic-Net</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="regression.html"><a href="regression.html#ValidationMod"><i class="fa fa-check"></i><b>6.6</b> Validation du modèle</a><ul>
<li class="chapter" data-level="6.6.1" data-path="regression.html"><a href="regression.html#contrôle-graphique-a-posteriori"><i class="fa fa-check"></i><b>6.6.1</b> Contrôle graphique a posteriori</a></li>
<li class="chapter" data-level="6.6.2" data-path="regression.html"><a href="regression.html#pour-vérifier-les-hypothèses-h1-et-h2-adéquation-et-homoscédasticité"><i class="fa fa-check"></i><b>6.6.2</b> Pour vérifier les hypothèses H1 et H2 : adéquation et homoscédasticité</a></li>
<li class="chapter" data-level="6.6.3" data-path="regression.html"><a href="regression.html#pour-vérifier-lhypothèse-h3-indépendance"><i class="fa fa-check"></i><b>6.6.3</b> Pour vérifier l’hypothèse H3 : indépendance</a></li>
<li class="chapter" data-level="6.6.4" data-path="regression.html"><a href="regression.html#pour-vérifier-lhypothèse-h4-gaussianité"><i class="fa fa-check"></i><b>6.6.4</b> Pour vérifier l’hypothèse H4 : gaussianité</a></li>
<li class="chapter" data-level="6.6.5" data-path="regression.html"><a href="regression.html#détection-de-données-aberrantes"><i class="fa fa-check"></i><b>6.6.5</b> Détection de données aberrantes</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="regression.html"><a href="regression.html#en-résumé-4"><i class="fa fa-check"></i><b>6.7</b> En résumé</a></li>
<li class="chapter" data-level="6.8" data-path="regression.html"><a href="regression.html#quelques-codes-python"><i class="fa fa-check"></i><b>6.8</b> Quelques codes python</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ANOVA.html"><a href="ANOVA.html"><i class="fa fa-check"></i><b>7</b> Analyse de variance (ANOVA)</a><ul>
<li class="chapter" data-level="7.1" data-path="ANOVA.html"><a href="ANOVA.html#vocabulaire"><i class="fa fa-check"></i><b>7.1</b> Vocabulaire</a></li>
<li class="chapter" data-level="7.2" data-path="ANOVA.html"><a href="ANOVA.html#analyse-de-variance-à-un-facteur"><i class="fa fa-check"></i><b>7.2</b> Analyse de variance à un facteur</a><ul>
<li class="chapter" data-level="7.2.1" data-path="ANOVA.html"><a href="ANOVA.html#exemple-et-notations"><i class="fa fa-check"></i><b>7.2.1</b> Exemple et notations</a></li>
<li class="chapter" data-level="7.2.2" data-path="ANOVA.html"><a href="ANOVA.html#modèle-régulier"><i class="fa fa-check"></i><b>7.2.2</b> Modèle régulier</a></li>
<li class="chapter" data-level="7.2.3" data-path="ANOVA.html"><a href="ANOVA.html#modèle-singulier"><i class="fa fa-check"></i><b>7.2.3</b> Modèle singulier</a></li>
<li class="chapter" data-level="7.2.4" data-path="ANOVA.html"><a href="ANOVA.html#prédictions-résidus-et-variance"><i class="fa fa-check"></i><b>7.2.4</b> Prédictions, résidus et variance</a></li>
<li class="chapter" data-level="7.2.5" data-path="ANOVA.html"><a href="ANOVA.html#intervalle-de-confiance-et-test-sur-leffet-facteur"><i class="fa fa-check"></i><b>7.2.5</b> Intervalle de confiance et test sur l’effet facteur</a></li>
<li class="chapter" data-level="7.2.6" data-path="ANOVA.html"><a href="ANOVA.html#test-deffet-du-facteur"><i class="fa fa-check"></i><b>7.2.6</b> Test d’effet du facteur</a></li>
<li class="chapter" data-level="7.2.7" data-path="ANOVA.html"><a href="ANOVA.html#tableau-danalyse-de-la-variance-à-un-facteur"><i class="fa fa-check"></i><b>7.2.7</b> Tableau d’analyse de la variance à un facteur</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="ANOVA.html"><a href="ANOVA.html#analyse-de-variance-à-deux-facteurs"><i class="fa fa-check"></i><b>7.3</b> Analyse de variance à deux facteurs</a><ul>
<li class="chapter" data-level="7.3.1" data-path="ANOVA.html"><a href="ANOVA.html#notations-et-exemple"><i class="fa fa-check"></i><b>7.3.1</b> Notations et exemple</a></li>
<li class="chapter" data-level="7.3.2" data-path="ANOVA.html"><a href="ANOVA.html#modélisation"><i class="fa fa-check"></i><b>7.3.2</b> Modélisation</a></li>
<li class="chapter" data-level="7.3.3" data-path="ANOVA.html"><a href="ANOVA.html#estimation-des-paramètres"><i class="fa fa-check"></i><b>7.3.3</b> Estimation des paramètres</a></li>
<li class="chapter" data-level="7.3.4" data-path="ANOVA.html"><a href="ANOVA.html#prédiction-résidus-et-variance"><i class="fa fa-check"></i><b>7.3.4</b> Prédiction, résidus et variance</a></li>
<li class="chapter" data-level="7.3.5" data-path="ANOVA.html"><a href="ANOVA.html#décomposition-de-la-variabilité"><i class="fa fa-check"></i><b>7.3.5</b> Décomposition de la variabilité</a></li>
<li class="chapter" data-level="7.3.6" data-path="ANOVA.html"><a href="ANOVA.html#le-diagramme-dinteractions"><i class="fa fa-check"></i><b>7.3.6</b> Le diagramme d’interactions</a></li>
<li class="chapter" data-level="7.3.7" data-path="ANOVA.html"><a href="ANOVA.html#tests-dhypothèses"><i class="fa fa-check"></i><b>7.3.7</b> Tests d’hypothèses</a></li>
<li class="chapter" data-level="7.3.8" data-path="ANOVA.html"><a href="ANOVA.html#test-dabsence-deffet-du-facteur-b"><i class="fa fa-check"></i><b>7.3.8</b> Test d’absence d’effet du facteur <span class="math inline">\(B\)</span></a></li>
<li class="chapter" data-level="7.3.9" data-path="ANOVA.html"><a href="ANOVA.html#tableau-danalyse-de-variance-à-deux-facteurs-croisés-dans-le-cas-dun-plan-orthogonal"><i class="fa fa-check"></i><b>7.3.9</b> Tableau d’analyse de variance à deux facteurs croisés dans le cas d’un plan orthogonal</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="ANOVA.html"><a href="ANOVA.html#en-résumé-5"><i class="fa fa-check"></i><b>7.4</b> En résumé</a></li>
<li class="chapter" data-level="7.5" data-path="ANOVA.html"><a href="ANOVA.html#quelques-codes-en-python"><i class="fa fa-check"></i><b>7.5</b> Quelques codes en python</a><ul>
<li class="chapter" data-level="7.5.1" data-path="ANOVA.html"><a href="ANOVA.html#exemple-danova-à-un-facteur"><i class="fa fa-check"></i><b>7.5.1</b> Exemple d’ANOVA à un facteur</a></li>
<li class="chapter" data-level="7.5.2" data-path="ANOVA.html"><a href="ANOVA.html#exemple-danova-à-deux-facteurs"><i class="fa fa-check"></i><b>7.5.2</b> Exemple d’ANOVA à deux facteurs</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ANCOVA.html"><a href="ANCOVA.html"><i class="fa fa-check"></i><b>8</b> Analyse de covariance (ANCOVA)</a><ul>
<li class="chapter" data-level="8.1" data-path="ANCOVA.html"><a href="ANCOVA.html#les-données"><i class="fa fa-check"></i><b>8.1</b> Les données</a></li>
<li class="chapter" data-level="8.2" data-path="ANCOVA.html"><a href="ANCOVA.html#modélisation-1"><i class="fa fa-check"></i><b>8.2</b> Modélisation</a><ul>
<li class="chapter" data-level="8.2.1" data-path="ANCOVA.html"><a href="ANCOVA.html#modélisation-régulière"><i class="fa fa-check"></i><b>8.2.1</b> Modélisation régulière</a></li>
<li class="chapter" data-level="8.2.2" data-path="ANCOVA.html"><a href="ANCOVA.html#modélisation-singulière"><i class="fa fa-check"></i><b>8.2.2</b> Modélisation singulière</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="ANCOVA.html"><a href="ANCOVA.html#estimation-des-paramètres-1"><i class="fa fa-check"></i><b>8.3</b> Estimation des paramètres</a></li>
<li class="chapter" data-level="8.4" data-path="ANCOVA.html"><a href="ANCOVA.html#tests-dhypothèses-1"><i class="fa fa-check"></i><b>8.4</b> Tests d’hypothèses</a><ul>
<li class="chapter" data-level="8.4.1" data-path="ANCOVA.html"><a href="ANCOVA.html#absence-de-tout-effet"><i class="fa fa-check"></i><b>8.4.1</b> Absence de tout effet</a></li>
<li class="chapter" data-level="8.4.2" data-path="ANCOVA.html"><a href="ANCOVA.html#test-dabsence-dinteraction"><i class="fa fa-check"></i><b>8.4.2</b> Test d’absence d’interaction</a></li>
<li class="chapter" data-level="8.4.3" data-path="ANCOVA.html"><a href="ANCOVA.html#test-dabsence-de-leffet-de-la-covariable-z"><i class="fa fa-check"></i><b>8.4.3</b> Test d’absence de l’effet de la covariable z</a></li>
<li class="chapter" data-level="8.4.4" data-path="ANCOVA.html"><a href="ANCOVA.html#test-dabsence-de-leffet-facteur-t"><i class="fa fa-check"></i><b>8.4.4</b> Test d’absence de l’effet facteur T</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="ANCOVA.html"><a href="ANCOVA.html#en-résumé-6"><i class="fa fa-check"></i><b>8.5</b> En résumé</a></li>
<li class="chapter" data-level="8.6" data-path="ANCOVA.html"><a href="ANCOVA.html#quelques-codes-en-python-1"><i class="fa fa-check"></i><b>8.6</b> Quelques codes en python</a></li>
</ul></li>
<li class="part"><span><b>II Le modèle linéaire généralisé</b></span></li>
<li class="chapter" data-level="9" data-path="GLM.html"><a href="GLM.html"><i class="fa fa-check"></i><b>9</b> Principe du modèle linéaire généralisé</a><ul>
<li class="chapter" data-level="9.1" data-path="GLM.html"><a href="GLM.html#introduction-1"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="GLM.html"><a href="GLM.html#caractérisation-dun-modèle-linéaire-généralisé"><i class="fa fa-check"></i><b>9.2</b> Caractérisation d’un modèle linéaire généralisé</a><ul>
<li class="chapter" data-level="9.2.1" data-path="GLM.html"><a href="GLM.html#loi-de-la-variable-réponse-y"><i class="fa fa-check"></i><b>9.2.1</b> Loi de la variable réponse <span class="math inline">\(Y\)</span></a></li>
<li class="chapter" data-level="9.2.2" data-path="GLM.html"><a href="GLM.html#prédicteur-linéaire"><i class="fa fa-check"></i><b>9.2.2</b> Prédicteur linéaire</a></li>
<li class="chapter" data-level="9.2.3" data-path="GLM.html"><a href="GLM.html#fonction-de-lien"><i class="fa fa-check"></i><b>9.2.3</b> Fonction de lien</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="GLM.html"><a href="GLM.html#EstimMLG"><i class="fa fa-check"></i><b>9.3</b> Estimation</a><ul>
<li class="chapter" data-level="9.3.1" data-path="GLM.html"><a href="GLM.html#estimation-par-maximum-de-vraisemblance"><i class="fa fa-check"></i><b>9.3.1</b> Estimation par maximum de vraisemblance</a></li>
<li class="chapter" data-level="9.3.2" data-path="GLM.html"><a href="GLM.html#algorithmes-de-newton-raphson-et-fisher-scoring"><i class="fa fa-check"></i><b>9.3.2</b> Algorithmes de Newton-Raphson et Fisher-scoring</a></li>
<li class="chapter" data-level="9.3.3" data-path="GLM.html"><a href="GLM.html#equations-de-vraisemblance"><i class="fa fa-check"></i><b>9.3.3</b> Equations de vraisemblance</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="GLM.html"><a href="GLM.html#NormalitéAsymptotique"><i class="fa fa-check"></i><b>9.4</b> Loi asymptotique de l’EMV et inférence</a></li>
<li class="chapter" data-level="9.5" data-path="GLM.html"><a href="GLM.html#tests-dhypothèses-2"><i class="fa fa-check"></i><b>9.5</b> Tests d’hypothèses</a><ul>
<li class="chapter" data-level="9.5.1" data-path="GLM.html"><a href="GLM.html#test-de-modèles-emboîtés"><i class="fa fa-check"></i><b>9.5.1</b> Test de modèles emboîtés</a></li>
<li class="chapter" data-level="9.5.2" data-path="GLM.html"><a href="GLM.html#TestParamMLG"><i class="fa fa-check"></i><b>9.5.2</b> Test d’un paramètre <span class="math inline">\(\theta_j\)</span></a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="GLM.html"><a href="GLM.html#MLGIC"><i class="fa fa-check"></i><b>9.6</b> Intervalle de confiance pour <span class="math inline">\(\theta_j\)</span></a><ul>
<li class="chapter" data-level="9.6.1" data-path="GLM.html"><a href="GLM.html#par-wald"><i class="fa fa-check"></i><b>9.6.1</b> Par Wald</a></li>
<li class="chapter" data-level="9.6.2" data-path="GLM.html"><a href="GLM.html#fondé-sur-le-rapport-de-vraisemblances"><i class="fa fa-check"></i><b>9.6.2</b> Fondé sur le rapport de vraisemblances</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="GLM.html"><a href="GLM.html#qualité-dajustement-1"><i class="fa fa-check"></i><b>9.7</b> Qualité d’ajustement</a><ul>
<li class="chapter" data-level="9.7.1" data-path="GLM.html"><a href="GLM.html#le-pseudo-r2"><i class="fa fa-check"></i><b>9.7.1</b> Le pseudo <span class="math inline">\(R^2\)</span></a></li>
<li class="chapter" data-level="9.7.2" data-path="GLM.html"><a href="GLM.html#le-chi2-de-pearson-généralisé"><i class="fa fa-check"></i><b>9.7.2</b> Le <span class="math inline">\(\chi^2\)</span> de Pearson généralisé</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="GLM.html"><a href="GLM.html#ResidusGLM"><i class="fa fa-check"></i><b>9.8</b> Diagnostic, résidus</a></li>
<li class="chapter" data-level="9.9" data-path="GLM.html"><a href="GLM.html#en-résumé-7"><i class="fa fa-check"></i><b>9.9</b> En résumé</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="RegLogistique.html"><a href="RegLogistique.html"><i class="fa fa-check"></i><b>10</b> Régression logistique</a><ul>
<li class="chapter" data-level="10.1" data-path="RegLogistique.html"><a href="RegLogistique.html#introduction-2"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="RegLogistique.html"><a href="RegLogistique.html#pourquoi-des-modèles-particuliers"><i class="fa fa-check"></i><b>10.2</b> Pourquoi des modèles particuliers ?</a></li>
<li class="chapter" data-level="10.3" data-path="RegLogistique.html"><a href="RegLogistique.html#odds-et-odds-ratio"><i class="fa fa-check"></i><b>10.3</b> Odds et odds ratio</a></li>
<li class="chapter" data-level="10.4" data-path="RegLogistique.html"><a href="RegLogistique.html#régression-logistique-simple"><i class="fa fa-check"></i><b>10.4</b> Régression logistique simple</a><ul>
<li class="chapter" data-level="10.4.1" data-path="RegLogistique.html"><a href="RegLogistique.html#subquanti"><i class="fa fa-check"></i><b>10.4.1</b> Avec une variable explicative quantitative</a></li>
<li class="chapter" data-level="10.4.2" data-path="RegLogistique.html"><a href="RegLogistique.html#sect1expquali"><i class="fa fa-check"></i><b>10.4.2</b> Avec une variable explicative qualitative</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="RegLogistique.html"><a href="RegLogistique.html#régression-logistique-multiple"><i class="fa fa-check"></i><b>10.5</b> Régression logistique multiple</a><ul>
<li class="chapter" data-level="10.5.1" data-path="RegLogistique.html"><a href="RegLogistique.html#modèle-sans-interaction"><i class="fa fa-check"></i><b>10.5.1</b> Modèle sans interaction</a></li>
<li class="chapter" data-level="10.5.2" data-path="RegLogistique.html"><a href="RegLogistique.html#modèle-avec-interactions"><i class="fa fa-check"></i><b>10.5.2</b> Modèle avec interactions</a></li>
<li class="chapter" data-level="10.5.3" data-path="RegLogistique.html"><a href="RegLogistique.html#etude-complémentaire-du-modèle-retenu"><i class="fa fa-check"></i><b>10.5.3</b> Etude complémentaire du modèle retenu</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="RegLogistique.html"><a href="RegLogistique.html#quelques-codes-avec-python"><i class="fa fa-check"></i><b>10.6</b> Quelques codes avec python</a></li>
<li class="chapter" data-level="10.7" data-path="RegLogistique.html"><a href="RegLogistique.html#régression-polytomique"><i class="fa fa-check"></i><b>10.7</b> Régression polytomique</a><ul>
<li class="chapter" data-level="10.7.1" data-path="RegLogistique.html"><a href="RegLogistique.html#régression-multinomiale-ou-polytomique-non-ordonnée"><i class="fa fa-check"></i><b>10.7.1</b> Régression multinomiale ou polytomique non-ordonnée</a></li>
<li class="chapter" data-level="10.7.2" data-path="RegLogistique.html"><a href="RegLogistique.html#régression-polytomique-ordonnée"><i class="fa fa-check"></i><b>10.7.2</b> Régression polytomique ordonnée</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="RegLogLin.html"><a href="RegLogLin.html"><i class="fa fa-check"></i><b>11</b> Régression de Poisson / régression loglinéaire</a><ul>
<li class="chapter" data-level="11.1" data-path="RegLogLin.html"><a href="RegLogLin.html#modèle-de-régression-loglinéaire"><i class="fa fa-check"></i><b>11.1</b> Modèle de régression loglinéaire</a><ul>
<li class="chapter" data-level="11.1.1" data-path="RegLogLin.html"><a href="RegLogLin.html#pourquoi-un-modèle-particulier"><i class="fa fa-check"></i><b>11.1.1</b> Pourquoi un modèle particulier ?</a></li>
<li class="chapter" data-level="11.1.2" data-path="RegLogLin.html"><a href="RegLogLin.html#estimation-des-paramètres-3"><i class="fa fa-check"></i><b>11.1.2</b> Estimation des paramètres</a></li>
<li class="chapter" data-level="11.1.3" data-path="RegLogLin.html"><a href="RegLogLin.html#ajustement-et-prédiction"><i class="fa fa-check"></i><b>11.1.3</b> Ajustement et prédiction</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="RegLogLin.html"><a href="RegLogLin.html#exemple-de-régression-loglinéaire-avec-r"><i class="fa fa-check"></i><b>11.2</b> Exemple de régression loglinéaire avec R</a><ul>
<li class="chapter" data-level="11.2.1" data-path="RegLogLin.html"><a href="RegLogLin.html#régression-loglinéaire-simple"><i class="fa fa-check"></i><b>11.2.1</b> Régression loglinéaire simple</a></li>
<li class="chapter" data-level="11.2.2" data-path="RegLogLin.html"><a href="RegLogLin.html#régression-loglinéaire-multiple"><i class="fa fa-check"></i><b>11.2.2</b> Régression loglinéaire multiple</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="RegLogLin.html"><a href="RegLogLin.html#sur-dispersion-et-modèle-binomial-négatif"><i class="fa fa-check"></i><b>11.3</b> Sur-dispersion et modèle binomial négatif</a></li>
<li class="chapter" data-level="11.4" data-path="RegLogLin.html"><a href="RegLogLin.html#quelques-codes-avec-python-1"><i class="fa fa-check"></i><b>11.4</b> Quelques codes avec python</a></li>
</ul></li>
<li class="appendix"><span><b>Annexes</b></span></li>
<li class="chapter" data-level="A" data-path="rappels-de-probabilités-statistiques-et-doptimisation.html"><a href="rappels-de-probabilités-statistiques-et-doptimisation.html"><i class="fa fa-check"></i><b>A</b> Rappels de probabilités, statistiques et d’optimisation</a><ul>
<li class="chapter" data-level="A.1" data-path="rappels-de-probabilités-statistiques-et-doptimisation.html"><a href="rappels-de-probabilités-statistiques-et-doptimisation.html#rappels-sur-les-échantillons-gaussiens"><i class="fa fa-check"></i><b>A.1</b> Rappels sur les échantillons gaussiens</a><ul>
<li class="chapter" data-level="A.1.1" data-path="rappels-de-probabilités-statistiques-et-doptimisation.html"><a href="rappels-de-probabilités-statistiques-et-doptimisation.html#la-loi-normale"><i class="fa fa-check"></i><b>A.1.1</b> La loi normale</a></li>
<li class="chapter" data-level="A.1.2" data-path="rappels-de-probabilités-statistiques-et-doptimisation.html"><a href="rappels-de-probabilités-statistiques-et-doptimisation.html#vecteurs-gaussiens"><i class="fa fa-check"></i><b>A.1.2</b> Vecteurs gaussiens</a></li>
<li class="chapter" data-level="A.1.3" data-path="rappels-de-probabilités-statistiques-et-doptimisation.html"><a href="rappels-de-probabilités-statistiques-et-doptimisation.html#loi-du-khi-deux-loi-de-student-loi-de-fisher"><i class="fa fa-check"></i><b>A.1.3</b> Loi du khi-deux, loi de Student, loi de Fisher</a></li>
<li class="chapter" data-level="A.1.4" data-path="rappels-de-probabilités-statistiques-et-doptimisation.html"><a href="rappels-de-probabilités-statistiques-et-doptimisation.html#estimation-de-la-moyenne-et-de-la-variance-dun-échantillon-gaussien"><i class="fa fa-check"></i><b>A.1.4</b> Estimation de la moyenne et de la variance d’un échantillon gaussien</a></li>
<li class="chapter" data-level="A.1.5" data-path="rappels-de-probabilités-statistiques-et-doptimisation.html"><a href="rappels-de-probabilités-statistiques-et-doptimisation.html#construction-dintervalles-de-confiance"><i class="fa fa-check"></i><b>A.1.5</b> Construction d’intervalles de confiance</a></li>
</ul></li>
<li class="chapter" data-level="A.2" data-path="rappels-de-probabilités-statistiques-et-doptimisation.html"><a href="rappels-de-probabilités-statistiques-et-doptimisation.html#estimation-sans-biais-de-variance-minimale"><i class="fa fa-check"></i><b>A.2</b> Estimation sans biais de variance minimale</a></li>
<li class="chapter" data-level="A.3" data-path="rappels-de-probabilités-statistiques-et-doptimisation.html"><a href="rappels-de-probabilités-statistiques-et-doptimisation.html#Newton-Raphson"><i class="fa fa-check"></i><b>A.3</b> La méthode de Newton-Raphson</a></li>
<li class="chapter" data-level="A.4" data-path="rappels-de-probabilités-statistiques-et-doptimisation.html"><a href="rappels-de-probabilités-statistiques-et-doptimisation.html#théorème-central-limite-condition-de-lindeberg"><i class="fa fa-check"></i><b>A.4</b> Théorème central limite: condition de Lindeberg</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="preuves-de-quelques-résultats-du-cours.html"><a href="preuves-de-quelques-résultats-du-cours.html"><i class="fa fa-check"></i><b>B</b> Preuves de quelques résultats du cours</a><ul>
<li class="chapter" data-level="B.1" data-path="preuves-de-quelques-résultats-du-cours.html"><a href="preuves-de-quelques-résultats-du-cours.html#ProofFisher"><i class="fa fa-check"></i><b>B.1</b> Preuve pour le test de Fisher</a></li>
<li class="chapter" data-level="B.2" data-path="preuves-de-quelques-résultats-du-cours.html"><a href="preuves-de-quelques-résultats-du-cours.html#annexe:ortho"><i class="fa fa-check"></i><b>B.2</b> Preuve de la proposition @ref(prp:Proportho)</a></li>
<li class="chapter" data-level="B.3" data-path="preuves-de-quelques-résultats-du-cours.html"><a href="preuves-de-quelques-résultats-du-cours.html#annexe:risque"><i class="fa fa-check"></i><b>B.3</b> Preuve de la proposition @ref(prp:risque)</a></li>
<li class="chapter" data-level="B.4" data-path="preuves-de-quelques-résultats-du-cours.html"><a href="preuves-de-quelques-résultats-du-cours.html#annexe:KL"><i class="fa fa-check"></i><b>B.4</b> Preuve de la proposition @ref(prp:KL)</a></li>
<li class="chapter" data-level="B.5" data-path="preuves-de-quelques-résultats-du-cours.html"><a href="preuves-de-quelques-résultats-du-cours.html#annexe:Mallows"><i class="fa fa-check"></i><b>B.5</b> Critère du <span class="math inline">\(C_p\)</span> de Mallows</a></li>
<li class="chapter" data-level="B.6" data-path="preuves-de-quelques-résultats-du-cours.html"><a href="preuves-de-quelques-résultats-du-cours.html#annexe:Sj"><i class="fa fa-check"></i><b>B.6</b> Preuve de la proposition @ref(prp:eqSj)</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li>Cathy Maugis-Rabusseau</li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Modèle linéaire général et modèle linéaire généralisé</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="singulier" class="section level1">
<h1><span class="header-section-number">Chapitre 5</span> Modèles singuliers, orthogonalité et importance des hypothèses sur les erreurs</h1>
<div id="quand-h1-h4-ne-sont-pas-respectées" class="section level2">
<h2><span class="header-section-number">5.1</span> Quand H1-H4 ne sont pas respectées…</h2>
<p>L’hypothèse de gaussianité des erreurs est la plus difficile à vérifier en pratique. Les tests classiques de normalité (test de Kolmogorov-Smirnov, Cramer-Von Mises, Anderson-Darling ou de Shapiro-Wilks) demanderaient l’observation des erreurs <span class="math inline">\(\varepsilon_i\)</span> elles-mêmes ; ils perdent beaucoup de leur puissance quand ils sont appliqués sur les résidus <span class="math inline">\(\widehat{\varepsilon_i}=Y_i-\widehat{Y_i}\)</span>, notamment en raison du fait que ces résidus ne sont pas indépendants. Nous pouvons cependant toujours faire des droites de Henry ou des QQ-plots pour mettre en évidence des écarts évidents. Il n’en reste pas moins que l’hypothèse de gaussianité sera le plus souvent un credo que nous ne pourrons pas vraiment vérifier expérimentalement. Fort heureusement, il existe une théorie asymptotique (donc de grands échantillons) du modèle linéaire qui n’a pas besoin de cette hypothèse. Comme il est dit dans la section <a href="DefML.html#modlinreg">2.1</a>, c’est dans cette optique là qu’il faut réellement penser le modèle linéaire.</p>
<div id="propriétés-de-lestimateur-des-moindres-carrés-widehattheta" class="section level3">
<h3><span class="header-section-number">5.1.1</span> Propriétés de l’estimateur des moindres carrés <span class="math inline">\(\widehat{\theta}\)</span></h3>
<div class="proposition">
<p><span id="prp:unlabeled-div-18" class="proposition"><strong>Proposition 5.1  </strong></span>Soit <span class="math inline">\(\widehat{\theta} =(X&#39;X)^{-1}X&#39;Y.\)</span></p>
<ul>
<li><span class="math inline">\(\widehat{\theta}\)</span> reste sans biais, <span class="math inline">\(\mathbb{E}[\widehat{\theta}]=\theta\)</span>, sous l’hypothèse H1.</li>
<li>la matrice de variance-covariance de <span class="math inline">\(\widehat{\theta}\)</span> reste égale à <span class="math inline">\(\sigma^2 (X&#39;X)^{-1}\)</span> sous les hypothèses H2 et H3, mais si H1 n’est pas vraie cette propriété a peu d’intérêt.</li>
<li><span class="math inline">\(\widehat{\theta}\)</span> n’est plus un estimateur optimal parmi les estimateurs sans biais, mais il le reste parmi les estimateurs linéaires sans biais sous H1-H3.</li>
<li><span class="math inline">\(\widehat{\theta}\)</span> est gaussien sous H3 et H4. Si H4 n’est pas vraie, alors il tend à être gaussien pour de grands échantillons. On dit qu’il est asymptotiquement gaussien.</li>
</ul>
</div>
</div>
<div id="propriétés-de-lestimateur-des-moindres-carrés-widehatsigma2" class="section level3">
<h3><span class="header-section-number">5.1.2</span> Propriétés de l’estimateur des moindres carrés <span class="math inline">\(\widehat{\sigma}^2\)</span></h3>
<p>Cette étude n’a bien sûr d’intérêt que si <span class="math inline">\(\sigma^2\)</span> est bien définie ce qui nécessite l’hypothèse H2. Nous considérons
<span class="math display">\[\widehat{\sigma^2} = \frac{1}{n-k}\|Y-X\widehat{\theta}\|^2 \textrm{ avec } \widehat{\theta} =(X&#39;X)^{-1}X&#39;Y.\]</span></p>
<div class="proposition">
<ul>
<li></li>
<li>Sous les hypothèses H1-H3, <span class="math inline">\(\widehat{\sigma}^2\)</span> reste un estimateur sans biais de <span class="math inline">\(\sigma^2\)</span> même si l’hypothèse H4 n’est pas vérifiée : <span class="math inline">\(\mathbb{E}[\widehat{\sigma}^2]=\sigma^2.\)</span></li>
<li>Il est clair que <span class="math inline">\((n-k)\widehat{\sigma}^2\)</span> ne suit plus une loi <span class="math inline">\(\sigma^2 \chi^2(n-k)\)</span> dès que l’hypothèse H4 n’est pas vérifiée.</li>
<li>Nous montrons facilement que sous les hypothèses H1-H3, <span class="math inline">\(\widehat{\sigma}^2\)</span> converge en probabilité vers <span class="math inline">\(\sigma^2\)</span> quand le nombre d’observations devient grand, même si l’hypothèse H4 n’est pas vérifiée.</li>
<li>Enfin, sous les seules hypothèses H1-H3, dès que la loi de <span class="math inline">\(\varepsilon_i\)</span> admet un moment d’ordre 4, alors <span class="math inline">\(\widehat{\sigma}^2\)</span> converge à la vitesse <span class="math inline">\(\sqrt{n}\)</span> vers <span class="math inline">\(\sigma^2\)</span> mais sa vitesse exacte de convergence dépend du type de loi, plus précisément du coefficient de Kurtosis <span class="citation">(Azaïs and Bardet <a href="#ref-Azais" role="doc-biblioref">2005</a>)</span>.</li>
</ul>
</div>
<!--
### Propriétés des statistiques de test T et F
Dans cette partie, nous considérons simplement le cas du modèle linéaire non-gaussien, sans l'hypothèse H4. Le résultat général est la validité asymptotique (pour de grands effectifs) des tests évoqués (sous certaines conditions peu restrictives), voir @Azais pour les détails et les explications théoriques d'un tel résultat.

Pour illustrer cette validité dans un cas simple, prenons l'exemple de l'analyse de variance à un facteur. Nous avons vu que l'estimateur $\widehat{\mu_i}$ de la valeur moyenne d'une classe est une simple moyenne empirique : $\widehat{ \mu_i}=Y_{i.}=\frac 1 n \sum_{j=1}^{n_i} Y_{ij}$. Pour "mesurer" la vitesse de convergence de $\widehat{ \mu_i}$ vers $\mu_i$, nous utilisons le Théorème de la Limite Centrale. Pour revenir au problème précédent d'analyse de la variance à un facteur, une conséquence de ce théorème est que pour $n$ grand, $\widehat{ \mu_i}=Y_{i.}$ suit approximativement une loi gaussienne et une conséquence de la Loi des Grands Nombres est que $\widehat{ \sigma^2}$ est très proche de $\sigma^2$. Ceci implique par exemple que si nous voulons tester l'hypothèse $"\mu_i=p"$, pour un $i$ donné et avec un réel $p$ connu, alors comme précédemment nous considèrerons la statistique de test :
$$\widehat{T}=\frac{\widehat{ \mu_i} -p}{\sqrt{\widehat{\sigma^2}/n}}.$$
Pour $n$ grand, $\widehat{T}$ suit approximativement une loi gaussienne centrée réduite qui n'est autre que la limite d'une loi de Student $\mathcal T(n)$ dont le nombre de degrés de liberté tend vers l'infini, voir @Azais.

Ce résultat théorique peut être complété par une étude de simulation. Dans un mémoire, Bonnet et Lansiaux [@Bonnet] ont étudié le comportement du test de Fisher en analyse de variance à un facteur à 2, 5 ou 10 niveaux, avec des indices de répétition de 2, 4 ou 8. Nous avons donc de 4 à 80 données dans chaque expérience. La validité du test est appréciée par le niveau réel du test pour un niveau nominal de $10\%, \, 5\%$ ou $1\%$.

Divers types de loi non normales sont utilisées. Nous apercevons un écart au comportement nominal du test seulement dans le cas où les éléments suivants sont réunis :
dispositifs déséquilibrés,
 petits échantillons,
loi dissymétrique.

Dans les autres cas, tout se passe comme si les données étaient gaussiennes. Ainsi cette étude confirme, que sauf cas extrêmes, le test de Fisher n'a pas besoin de l'hypothèse de gaussiannité pour être approximativement exact.
-->
</div>
<div id="modèles-avec-corrélations" class="section level3">
<h3><span class="header-section-number">5.1.3</span> Modèles avec corrélations</h3>
<p>Il est possible de modéliser des corrélations entre erreurs, par exemple en supposant que ces erreurs sont issues d’un processus ARMA, ce qui permet de ne plus avoir besoin de l’hypothèse H3, voir <span class="citation">Guyon (<a href="#ref-Guyon" role="doc-biblioref">2001</a>)</span>.</p>
<p>Il est également possible de modéliser les liaisons par des modèles à effets aléatoires et poser un modèle mixte, voir Pinheiro et Bates <span class="citation">(Pinheiro and Bates <a href="#ref-Pinheiro" role="doc-biblioref">2006</a>)</span>. <!--%ou le cours de 5ème année de Magali San Cristobal.--></p>
</div>
</div>
<div id="ModSingulier" class="section level2">
<h2><span class="header-section-number">5.2</span> Modèles singuliers</h2>
<p>Nous nous sommes jusqu’à présent cantonnés à l’étude des modèles linéaires réguliers. Or certains modèles ne peuvent être paramétrés de façon régulière : ils sont naturellement sur-paramétrés. Un exemple simple est celui du modèle additif en analyse de la variance à 2 facteurs. Considérons le cas où les 2 facteurs ont chacun 2 niveaux et que les 4 combinaisons sont observées une fois et une seule. On a donc, avec les notations vues précédemment :
<span class="math display">\[
Y_{i,j} = \mu + a-i + b_j + \varepsilon_{i,j},\ i\in\{1,2\},\ j\in\{1,2\}.
\]</span></p>
<!--%\begin{eqnarray*}
%Y_{11}&=\mu+a_1+b_1+\varepsilon_{11}\\
%Y_{12}&=\mu+a_1+b_2+\varepsilon_{12}\\
%Y_{21}&=\mu+a_2+b_1+\varepsilon_{21}\\
%Y_{22}&=\mu+a_2+b_2+\varepsilon_{22}
%\end{eqnarray*}
-->
<p>Le vecteur <span class="math inline">\(\theta=(\mu,a_1,a_2,b_1,b_2)&#39;\)</span> et
la matrice <span class="math inline">\(X\)</span> du modèle vaut :
<span class="math display">\[
X=\left(
\begin{array}{ccccc}
1 &amp; 1 &amp; 0 &amp; 1 &amp; 0 \\
1 &amp; 1 &amp; 0 &amp; 0 &amp; 1 \\
1 &amp; 0 &amp; 1 &amp; 1 &amp; 0 \\
1 &amp; 0 &amp; 1 &amp; 0 &amp; 1
\end{array}
\right).
\]</span>
Nous remarquons que tout vecteur de la forme <span class="math inline">\((\alpha+\beta,-\alpha,-\alpha, -\beta,-\beta)\)</span> donne le vecteur nul lorsqu’il est multiplié par la matrice <span class="math inline">\(X\)</span>. Les valeurs <span class="math inline">\(\mu, \, a_i, \, b_i\)</span> pour <span class="math inline">\(i=1\)</span> ou 2 ne sont donc pas identifiables de manière unique. Le modèle est en fait <strong>sur-paramétré</strong> : nous avons 5 paramètres inconnus pour seulement 4 observations.</p>
<div class="definition">
<p><span id="def:unlabeled-div-20" class="definition"><strong>Definition 5.1  </strong></span>Le modèle est dit <strong>singulier</strong> quand la matrice <span class="math inline">\(X\)</span> est non injective, c’est-à-dire s’il existe <span class="math inline">\(\theta\neq 0_k\)</span> tel que <span class="math inline">\(X\theta =0_n\)</span>.</p>
</div>
<p>Nous rappelons que <span class="math inline">\(Ker(X)=\{u \in \mathbb{R}^k; \, Xu=0_{n}\}\)</span> désigne le noyau de <span class="math inline">\(X\)</span>. Nous pouvons faire deux remarques :</p>
<ul>
<li><span class="math inline">\(X\widehat{\theta}\)</span> reste unique, puisque c’est la projection de <span class="math inline">\(Y\)</span> sur <span class="math inline">\(Im(X)\)</span>.</li>
<li><span class="math inline">\(\widehat{\theta}\)</span> ne peut être unique puisque si <span class="math inline">\(\widehat{\theta}\)</span> est solution et si <span class="math inline">\(u \in Ker(X)\)</span> alors <span class="math inline">\(\widehat{\theta} + u\)</span> est encore solution.</li>
</ul>
<p>Si <span class="math inline">\(X\)</span> n’est pas régulière, alors la matrice <span class="math inline">\(X&#39;X\)</span> n’est pas inversible. Pour contourner ce problème, nous définissons alors un inverse généralisé de <span class="math inline">\((X&#39;X)\)</span>.</p>
<div class="definition">
<p><span id="def:unlabeled-div-21" class="definition"><strong>Definition 5.2  </strong></span>Soit <span class="math inline">\(M\)</span> une matrice. Alors la matrice <span class="math inline">\(M^-\)</span> est <strong>une matrice inverse généralisée</strong> de <span class="math inline">\(M\)</span> si <span class="math inline">\(MM^-M=M\)</span>.</p>
</div>
<p>Cette construction est toujours possible. En effet, <span class="math inline">\((X&#39;X)\)</span> définit une application bijective de <span class="math inline">\(Ker(X)^{\perp}\)</span> sur lui-même. Il suffit donc simplement de négliger la partie contenue dans le noyau : on prend l’inverse sur <span class="math inline">\(Ker(X)^{\perp}\)</span>, complété arbitrairement sur <span class="math inline">\(Ker(X)\)</span>. La définition de <span class="math inline">\((X&#39;X)^-\)</span> est donc loin d’être unique ! Il est alors possible de généraliser les résultats du cas régulier.</p>
<div class="proposition">
<p><span id="prp:thetasing" class="proposition"><strong>Proposition 5.2  </strong></span>Si <span class="math inline">\((X&#39;X)^-\)</span> est une matrice inverse généralisée de <span class="math inline">\(X&#39;X\)</span> alors <span class="math inline">\(\widehat{\theta}=(X&#39;X)^-X&#39;Y\)</span> est une solution des équations normales :
<span class="math display">\[(X&#39;X)\widehat{\theta} = X&#39;Y.\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-22" class="proof"><em>Proof</em>. </span>On commence par remarquer que
<span class="math display">\[\forall \omega\in\mathbb{R}^k,\, &lt;X\omega,P_{[X]^\perp} Y&gt; = &lt;\omega, X&#39;P_{[X]^\perp} Y&gt; = 0\]</span> donc
<span class="math display">\[
X&#39;Y = X&#39;P_{[X]}Y + X&#39; P_{[X]^{\perp}} Y = X&#39;P_{[X]}Y.
\]</span>
Ainsi, <span class="math inline">\(\exists u\in\mathbb{R}^k,\ X&#39;Y = X&#39;Xu\)</span>. Finalement,
<span class="math display">\[
(X&#39;X)\widehat\theta = (X&#39;X) (X&#39;X)^{-}X&#39;Y = (X&#39;X) (X&#39;X)^{-}X&#39;Xu = X&#39;Xu = X&#39;Y. 
\]</span></p>
</div>
<!--
%\begin{Exo}~
%\begin{enumerate}
%\item Montrez que $X'P_{[X]^\perp} Y = 0_k$. Pour cela, vous pouvez évaluer $<Xw,P_{[X]^\perp} Y>$ pour tout $w\in\R^k$.
%\item Déduisez-en que $X'Y=X' X u$ avec $u\in \R^k$. 
%\item Finalement, montrez le résultat de la proposition \ref{prop:thetasing}.
%\end{enumerate}
%\end{Exo}
-->
<div class="remark">
<p><span id="unlabeled-div-23" class="remark"><em>Remark</em>. </span>Cet estimateur n’est pas unique et dépend de la définition choisie pour <span class="math inline">\((X&#39;X)^-\)</span>. Par contre, le vecteur <span class="math inline">\(X\widehat{\theta}\)</span> reste unique, même si la matrice <span class="math inline">\(X\)</span> est singulière. Ce vecteur correspond en effet à la projection orthogonale de <span class="math inline">\(Y\)</span> sur <span class="math inline">\(Im(X)\)</span>.</p>
</div>
<p>En règle générale, nous préférons lever l’indétermination sur <span class="math inline">\(\hat\theta\)</span> en fixant des contraintes, souvent afin de donner un sens plus intuitif à <span class="math inline">\(\theta\)</span>.</p>
<div id="contraintes-didentifiabilité" class="section level3">
<h3><span class="header-section-number">5.2.1</span> Contraintes d’identifiabilité</h3>
<div class="proposition">
<p><span id="prp:ident" class="proposition"><strong>Proposition 5.3  </strong></span>Supposons la matrice <span class="math inline">\(X\)</span> singulière de rang <span class="math inline">\(r&lt;k\)</span> de sorte qu’il y ait <span class="math inline">\(k-r\)</span> paramètres redondants. Soit <span class="math inline">\(M\)</span> une matrice à <span class="math inline">\(k-r\)</span> lignes et <span class="math inline">\(k\)</span> colonnes, supposée de rang <span class="math inline">\(k-r\)</span> et telle que :
<span class="math display">\[Ker(M) \cap Ker(X)= \lbrace 0_{k} \rbrace.\]</span>
Alors,</p>
<ul>
<li>la matrice <span class="math inline">\((X&#39;X+M&#39;M)\)</span> est inversible et son inverse est une matrice inverse généralisée de <span class="math inline">\(X&#39;X\)</span></li>
<li>le vecteur <span class="math inline">\(\widehat{\theta} = (X&#39;X+M&#39;M)^{-1}X&#39;Y\)</span> est l’unique solution du système <span class="math inline">\(\displaystyle \left\lbrace  \begin{array}{c}  X&#39;X\alpha = X&#39;Y \\  M\alpha= 0_{k-r}.  \end{array}  \right.\)</span></li>
</ul>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-24" class="exercise"><strong>Exercise 5.1  </strong></span>L’objectif est de démontrer la proposition <a href="singulier.html#prp:ident">5.3</a>.</p>
<ol style="list-style-type: decimal">
<li><p>Pour montrer que <span class="math inline">\(X&#39;X+M&#39;M\)</span> est inversible : montrez que la matrice
<span class="math display">\[
A=\left(\begin{array}{c} X\\ M \end{array}\right)\in\mathcal M_{n+k-r,k}(\mathbb{R})
\]</span>
est injective et donc <span class="math inline">\(A&#39;A\)</span> est inversible.</p></li>
<li><p>Considérez le problème de minimisation suivant :
<span class="math display">\[
g:\alpha\mapsto \|Y-X\alpha\|^2 + \|M\alpha\|^2 .
\]</span>
Ecrivez <span class="math inline">\(g(\alpha)\)</span> sous la forme <span class="math inline">\(g(\alpha)=\|\tilde{Y} - A \alpha\|^2\)</span> avec <span class="math inline">\(\tilde {Y}\)</span> à préciser. Déduisez-en que <span class="math inline">\(\widehat{\theta}\)</span> est solution du système
<span class="math inline">\(\displaystyle \left\lbrace  \begin{array}{c}  X&#39;X\alpha = X&#39;Y \\  M\alpha= 0_{k-r}.  \end{array}  \right.\)</span></p></li>
<li><p>Montrez l’unicité de la solution</p></li>
</ol>
</div>
<p>Le choix de la contrainte n’est pas toujours évident. Par ailleurs, pour chaque contrainte <span class="math inline">\(M\)</span>, nous aurons un estimateur correspondant ce qui est parfois gênant.</p>
<div class="example">
<p><span id="exm:unlabeled-div-25" class="example"><strong>Example 5.1  </strong></span>Prenons l’exemple de l’analyse de variance à un facteur avec effet différentiel : nous supposons pour simplifier que <span class="math inline">\(I=4\)</span>. Le modèle s’écrit donc de la façon suivante :
<span class="math display">\[Y_{i,j}=\mu + \alpha_i +\varepsilon_{ij} \mbox{ pour } i=1,\cdots,4 \mbox{ et } j=1.\]</span>
La matrice <span class="math inline">\(X\)</span> associée au modèle est :
<span class="math display">\[ X= \left( 
      \begin{array}{ccccc}
        1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\
        1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\
        1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\
        1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 
      \end{array} \right).\]</span>
Il nous faut poser une contrainte (dite d’identifiabilité) sur le vecteur <span class="math inline">\(\theta\)</span> au travers du choix d’une matrice à <span class="math inline">\(1\)</span> ligne et <span class="math inline">\(k\)</span> colonnes.</p>
<ul>
<li><p>Si on considère <span class="math inline">\(M= (0 \ 1 \ 1 \ 1 \ 1)\)</span>, la contrainte correspondante est
<span class="math display">\[ M\theta = 0 \Leftrightarrow \alpha_1 + \alpha_2 + \alpha_3 + \alpha_4= 0.\]</span>
On impose donc que la somme des effets différentiels est nulle.</p></li>
<li><p>Si on considère <span class="math inline">\(M=(0,1,0,0,0)\)</span>, la contrainte correspondante est <span class="math inline">\(M\theta = \alpha_1=0\)</span>. On impose donc que la première modalité est la référence.</p></li>
</ul>
</div>
<!--
%\textsf{Exercice : construire l'estimateur $\hat\beta$ obtenu par la méthode des moindres carrés sous la contrainte $H\beta=0$.}\\
-->
</div>
<div id="fonctions-estimables-et-contrastes" class="section level3">
<h3><span class="header-section-number">5.2.2</span> Fonctions estimables et contrastes</h3>
<p>En présence d’une matrice singulière, il est donc toujours possible de construire un estimateur. Qu’en est-il des tests ? En particulier, ces contraintes sont-elles systématiquement nécessaires ?</p>
<p>La plupart des quantités que nous avons voulu tester sont des fonctions de <span class="math inline">\(\theta\)</span> qui ne dépendent pas de la solution particulière des équations normales, c’est-à-dire du type de contraintes d’identifiabilité choisi. Ces fonctions sont appelées estimables car elles sont intrinsèques.</p>
<div class="definition">
<p><span id="def:unlabeled-div-26" class="definition"><strong>Definition 5.3  </strong></span>Une combinaison linéaire <span class="math inline">\(C\theta\)</span> est dite <strong>fonction estimable</strong> (de paramètre <span class="math inline">\(\theta\)</span>) si elle ne dépend pas du choix particulier d’une solution des équations normales. On caractérise ces fonctions comme étant celles qui s’écrivent <span class="math inline">\(C\theta=DX\theta\)</span> où <span class="math inline">\(D\)</span> est une matrice de plein rang.</p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-27" class="definition"><strong>Definition 5.4  </strong></span>On appelle <strong>contraste</strong> une fonction estimable <span class="math inline">\(C\theta\)</span> telle que <span class="math inline">\(C \mathbb{1}=0\)</span>, où <span class="math inline">\(\un\)</span> désigne le vecteur unité.</p>
</div>
<p>En analyse de variance, la plupart des combinaisons linéaires que l’on teste sont en fait des contrastes (cf chapitre <a href="ANOVA.html#ANOVA">7</a>). Dans l’exemple précédent, <span class="math inline">\(\alpha_1 - \alpha_2\)</span> est un contraste.</p>
</div>
</div>
<div id="orthogonalité" class="section level2">
<h2><span class="header-section-number">5.3</span> Orthogonalité</h2>
<div id="orthogonalité-pour-les-modèles-réguliers" class="section level3">
<h3><span class="header-section-number">5.3.1</span> Orthogonalité pour les modèles réguliers</h3>
<p>L’orthogonalité est une notion qui peut notablement simplifier la résolution et la compréhension d’un modèle linéaire. Un modèle linéaire admet le plus souvent une décomposition naturelle des paramètres <span class="math inline">\(\theta\)</span> (cf exemple ci-dessous) et conséquemment une décomposition de la matrice <span class="math inline">\(X\)</span> associée au modèle. On va s’intéresser ici à l’orthogonalité éventuelle des différents espaces associés à cette décomposition (l’orthogonalité sera toujours comprise par la suite au sens d’orthogonalité liée au produit scalaire euclidien usuel). Le problème sera plus ou moins délicat suivant que le modèle est régulier ou non. En premier lieu, illustrons par deux exemples ce que l’on entend par décomposition des paramètres.</p>
<div class="example">
<p><span id="exm:unlabeled-div-28" class="example"><strong>Example 5.2  </strong></span>Soit le modèle de régression linéaire multiple sur trois variables <span class="math inline">\(x^{(1)}, \, x^{(2)}\)</span> et <span class="math inline">\(x^{(3)}\)</span> :
<span class="math display">\[Y_i=\mu + \theta_1x_i^{(1)}+\theta_2 x^{(2)}_i +\theta_3 x^{(3)}_i + \varepsilon_i, i=1, \cdots, n&gt;4.\]</span>
Le vecteur <span class="math inline">\(\theta\)</span> comprend 4 coordonnées : <span class="math inline">\(\mu, \, \theta_1, \, \theta_2, \theta_3\)</span> et la matrice <span class="math inline">\(X\)</span> quatre colonnes. Assez naturellement ici, on peut considérer la décomposition, plus précisément on parlera par la suite de partition en quatre éléments. La partition de la matrice revient alors à l’écrire comme concaténation de 4 vecteurs colonnes. L’orthogonalité de la partition correspondra alors strictement à l’orthogonalité des 4 droites vectorielles : <span class="math inline">\([\mathbb{1}], \, [x^{(1)}], \, [x^{(2)}]\)</span> et <span class="math inline">\([x^{(3)}]\)</span>.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-29" class="example"><strong>Example 5.3  </strong></span>Soit le modèle de régression quadratique sur <span class="math inline">\(x^{(1)}\)</span> et <span class="math inline">\(x^{(2)}\)</span> :
<span class="math display">\[Y_i=\mu + \theta_1x^{(1)}_i+\theta_2 x^{(2)}_i +\gamma_1 \left(x^{(1)}_i\right)^2 +\gamma_2\left(x^{(2)}_i\right)^2+ \delta x^{(1)}_i x^{(2)}_i + \varepsilon_i, i=1, \cdots, n&gt;6.\]</span>
Ici plutôt que de demander comme précédemment l’orthogonalité de chacun des régresseurs (ce qui serait beaucoup demander), on peut définir la partition naturelle correspondant à :</p>
<ul>
<li>la constante <span class="math inline">\(\mu\)</span> ;</li>
<li>les effets linéaires <span class="math inline">\(\theta_1, \, \theta_2\)</span> ;</li>
<li>les effets carrés <span class="math inline">\(\gamma_1, \, \gamma_2\)</span> ;</li>
<li>l’effet produit <span class="math inline">\(\delta\)</span>.</li>
</ul>
<p>L’orthogonalité de la partition est alors définie comme l’orthogonalité des sous-espaces vectoriels : <span class="math inline">\([\mathbb{1}], \, [(x^{(1)}, \, x^{(2)})], \, \left[\left((x^{(1)})^2, \, (x^{(2)})^2\right)\right]\)</span> et <span class="math inline">\([x^{(1)} x^{(2)}]\)</span>.</p>
</div>
<p>En conséquence, on voit bien, à partir de ces deux exemples, qu’il faudra parler de modèle avec partition orthogonale plutôt que de modèle orthogonal.</p>
<p>Formalisons ces exemples dans une définition.</p>
<div class="definition">
<p><span id="def:unlabeled-div-30" class="definition"><strong>Definition 5.5  </strong></span>Soit un modèle linéaire général régulier <span class="math inline">\(Y=X\theta + \varepsilon.\)</span>
Considérons une partition en <span class="math inline">\(m\)</span> termes de <span class="math inline">\(X\)</span> et de <span class="math inline">\(\theta\)</span>, soit
<span class="math display">\[Y= X_1\theta_1 + \cdots +X_m\theta_m +\varepsilon,\]</span>
où la matrice <span class="math inline">\(X_j\)</span> est une matrice de taille <span class="math inline">\((n,k_j)\)</span> et <span class="math inline">\(\theta_j \in \mathbb{R}^{k_j}\)</span> avec <span class="math inline">\(k_j \in \{1,\cdots,k\}\)</span> pour <span class="math inline">\(j=1,\cdots,m\)</span> et avec <span class="math inline">\(\sum_{j=1}^{m} k_j=k\)</span>.
On dit que <strong>cette partition est orthogonale</strong> si les sous-espaces vectoriels de <span class="math inline">\(\mathbb{R}^n\)</span>,
<span class="math inline">\([X_1], \cdots, [X_m],\)</span>
sont orthogonaux.</p>
</div>
<p>Une conséquence simple de l’orthogonalité d’un modèle linéaire est que la matrice d’information <span class="math inline">\(X&#39;X\)</span> a une structure bloc diagonale, chaque bloc étant associé à chaque élément de la partition.</p>
<p>Le plus souvent, la partition du vecteur de paramètres <span class="math inline">\(\theta\)</span> en différents effets vient</p>
<ul>
<li>en régression, des différentes variables ;</li>
<li>en analyse de la variance, des décompositions en interactions.</li>
</ul>
<p>L’orthogonalité donne aux modèles statistiques les deux propriétés suivantes :</p>
<div class="proposition">
<p><span id="prp:unlabeled-div-31" class="proposition"><strong>Proposition 5.4  </strong></span>Soit un modèle linéaire régulier muni d’une partition orthogonale :
<span class="math display">\[Y= X_1\theta_1 + \cdots +X_m\theta_m +\varepsilon.\]</span>
Alors</p>
<ul>
<li>les estimateurs des moindres carrés des différents effets <span class="math inline">\(\widehat{\theta}_1, \dots, \widehat\theta_m\)</span> sont non-corrélés et indépendants sous l’hypothèse gaussienne.</li>
<li>pour <span class="math inline">\(l=1,\cdots, m\)</span>, l’expression de l’estimateur <span class="math inline">\(\widehat{\theta}_l\)</span> ne dépend pas de la présence ou non des autres termes <span class="math inline">\(\theta_j\)</span> dans le modèle.</li>
</ul>
</div>
<!--
%\begin{demo}
%Présentée en amphi.
%\end{demo}
-->
<p>L’orthogonalité apporte une simplification des calculs : elle permet d’obtenir facilement une expression explicite des estimateurs. Par ailleurs, elle donne une indépendance approximative entre les tests des différents effets. Les tests portant sur des effets orthogonaux ne sont liés que par l’estimation du <span class="math inline">\(\sigma^2\)</span>.</p>
<!--
%Un exemple d'application de la seconde propriété est le suivant : soit un modèle de régression multiple et supposons que la partition la plus fine soit orthogonale. Alors l'expression de l'estimateur du coefficient $\theta_l$ de la variable $x^{(l)}$ vaut
%$$\theta_l=\sum_{i=1}^n \frac{x_i^{(l)}Y_i}{\left(x_i^{(l)}\right)}.$$
%Il suffit pour le vérifier de considérer le modèle de régression linéaire simple dans lequel on ne considère que la variable $x^{(l)}$.
-->
</div>
<div id="orthogonalité-pour-les-modèles-non-réguliers" class="section level3">
<h3><span class="header-section-number">5.3.2</span> Orthogonalité pour les modèles non-réguliers</h3>
<p>Lorsque le modèle est singulier, il est nécessaire de rajouter des contraintes. Il est alors raisonnable d’effectuer cette démarche en tenant compte de la partition, i.e. <span class="math inline">\(C_j \theta_j =0\)</span> où <span class="math inline">\(X_j|_{Ker(C_j)}\)</span> sont injectives.</p>
<div class="definition">
<p><span id="def:unlabeled-div-32" class="definition"><strong>Definition 5.6  </strong></span>Soit la partition suivante d’un modèle linéaire
<span class="math display">\[Y=X_1\theta_1+\cdots+X_m\theta_m +\varepsilon.\]</span>
Soit un système de contraintes <span class="math inline">\(C_1\theta_1=0, \cdots, C_m \theta_m=0\)</span> qui rendent le modèle identifiable.
On dit que ces contraintes rendent la partition orthogonale si les sous-espaces vectoriels
<span class="math display">\[V_j=\left\{X_j \theta_j; \, \theta_j \in Ker(C_j)\right\}, \, j=1, \cdots, m\]</span>
sont orthogonaux.</p>
</div>
<p>Cette notion est proche du cas régulier. Cependant, la notion d’orthogonalité dépend des contraintes choisies. L’idée sera en général de choisir des contraintes qui rendent le modèle orthogonal.
On verra que cette définition prend tout son sens avec l’exemple incontournable du modèle d’analyse de la variance à deux facteurs croisés (cf chapitre <a href="ANOVA.html#ANOVA">7</a>).</p>
<!--
%::: {.proposition}\label{Proportho}
%Soit le modèle d'analyse de variance à deux facteurs croisés :
%$$Y_{ijk}=\mu + \alpha_i + \beta_j + \gamma_{ij} + \varepsilon_{ijk} ; i=1, \cdots, I, \, j=1, \cdots, J, \, k=1, \cdots, n_{ij},$$
%avec $n_{ij}\geq 1, \, \sum_{ij}n_{ij}=n >IJ$.
%Alors il existe des contraintes qui rendent la partition $\mu, \, \alpha, \, \beta, \, \gamma$ orthogonale si et seulement si 
%\begin{equation}\label{EqOrtho}
%n_{ij}=\frac{n_{i +}n_{+ j}}{n}.
%\end{equation}
%où par exemple, $\displaystyle n_{i +}=\sum_j n_{ij}$ et $\displaystyle n=\sum_{ij}n_{ij}$. Dans ce cas, les contraintes sont 
%
%   \item[(i)] $\displaystyle \sum_{i=1}^In_{i +} \alpha_i =0 $ ;
%   \item[(ii)] $\displaystyle \sum_{j=1}^J n_{+ j} \beta_j=0 $ ;
%   \item[(iii)] $\displaystyle \forall i=1,\cdots, I, \, \sum_{j=1}^J n_{ij}\gamma_{ij}=0$ ;
%   \item[(iv)] $\displaystyle \forall j=1,\cdots, J, \, \sum_{i=1}^I n_{ij}\gamma_{ij}=0$.
%
%::: 
%%\begin{demo}
%%Présentée en amphi.
%%\end{demo}
%
%\underline{Remarques :} 
%\begin{enumerate}
%   \item Un énoncé très proche est possible pour le modèle additif.
%   \item Notez bien que pour le système de contraintes qui correspond à la décomposition de type III, cf paragraphe suivant :
%   $$\sum_i \alpha_i =0, \, \sum_j \beta_j=0, \, \forall i, \, \sum_j \gamma_{ij}=0 \mbox{ et } \forall j, \, \sum_j \gamma_{ij} =0$$
%   il n'y a possibilité d'orthogonalité, d'après la proposition \ref{Proportho}, que si le modèle est équi-répété, i.e. $n_{ij}=cte$.
%\end{enumerate}
%
%
-->
</div>
</div>
<div id="en-résumé-3" class="section level2">
<h2><span class="header-section-number">5.4</span> En résumé</h2>
<div class="summarybox">
<p>Dans ce chapitre, il est attendu que vous ayez compris</p>
<ul>
<li>la problématique de l’estimation des paramètres pour un modèle linéaire singulier</li>
<li>l’intérêt d’avoir l’orthogonalité dans un modèle linéaire</li>
</ul>
<p>Les résultats énoncés dans ce chapitre ne sont pas à connaitre. Il faudra savoir les mettre en application dans le cadre de l’ANOVA (voir Chapitre <a href="ANOVA.html#ANOVA">7</a>) et de l’ANCOVA (voir Chapitre <a href="ANCOVA.html#ANCOVA">8</a>).</p>
</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Azais">
<p>Azaïs, Jean-Marc, and Jean-Marc Bardet. 2005. <em>Le Modèle Linéaire Par L’exemple: Régression, Analyse de La Variance et Plans d’expériences Illustrés Avec R, Sas et Splus</em>. Dunod.</p>
</div>
<div id="ref-Guyon">
<p>Guyon, X. 2001. “Modele Linéaire et économétrie.” <em>Ellipse, Paris</em>.</p>
</div>
<div id="ref-Pinheiro">
<p>Pinheiro, José, and Douglas Bates. 2006. <em>Mixed-Effects Models in S and S-Plus</em>. Springer Science &amp; Business Media.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="Test.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Bookdown-poly.pdf", "Bookdown-poly.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
